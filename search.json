[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modern Polars",
    "section": "",
    "text": "Preface\nThis is a side-by-side comparison of the Polars and Pandas dataframe libraries, based on Modern Pandas by Tom Augsburger.\n(In case you haven’t heard, Polars is a very fast and elegant dataframe libary that does the same kinds of things Pandas does.)\nThe bulk of this book is structured examples of idiomatic Polars and Pandas code, with commentary on the API and performance of both.\nFor the most part, I argue that Polars is “better” than Pandas, though I do try and make it clear when Polars is lacking a Pandas feature or is otherwise disappointing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-for",
    "href": "index.html#who-is-this-for",
    "title": "Modern Polars",
    "section": "Who is this for?",
    "text": "Who is this for?\nThis is not a beginner’s introduction to data programming, though you certainly don’t need to be an expert to read it. If you have some familiarity with any dataframe library, most of the examples should make sense, but if you’re familiar with Pandas they’ll make even more sense because all the Polars code is accompanied by the equivalent Pandas code.\nYou don’t need to have read Modern Pandas, though I of course think it’s a great read.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "Modern Polars",
    "section": "Why?",
    "text": "Why?\nThere’s this weird phenomenon where people write data programming code as if they hate themselves. Many of them are academic or quant types who seem to have some complex about being “bad at coding”. Armchair psychology aside, lots of clever folk keep doing really dumb stuff with Pandas, and at some point you have to wonder if the Pandas API is too difficult for its users.\nAt the very least, articles like Minimally Sufficient Pandas make a compelling case for Pandas having too much going on.\nHaving used Pandas a lot, I think Polars is more intuitive and does a better job of having One Obvious Way to do stuff. It’s also much faster at most things, even when you do Pandas the right way.\nHopefully this work shows you how, why and when to prefer Polars.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "Modern Polars",
    "section": "Credit",
    "text": "Credit\nThe Pandas examples are mostly lifted from Tom’s articles, with some updates for data that’s no longer available, and some code changes to reflect how Pandas is written in 2023. This isn’t just me being lazy - I want to draw on Pandas examples that quite a lot of people are already familiar with.\nSo credit goes to Tom for the Pandas examples, for most of the data fetching code and for the general structure of the articles. Meanwhile the text content and the Polars examples are from me.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#running-the-code-yourself",
    "href": "index.html#running-the-code-yourself",
    "title": "Modern Polars",
    "section": "Running the code yourself",
    "text": "Running the code yourself\nYou can install the exact packages that the book uses with the env.yml file:\nmamba env create -f env.yml\nIf you’re not using mamba/conda you can install the following package versions and it should work:\npolars: 1.0.0\npyarrow: 10.0.1\npandas: 2.2.2\nnumpy: 1.26.4\nfsspec: 2024.6.1\nmatplotlib: 3.8.0\nseaborn: 0.13.2\nstatsmodels: 0.14.2\n\nData\nAll the data fetching code is included, but will eventually break as websites change or shut down. The smaller datasets have been checked in here for posterity.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Modern Polars",
    "section": "Contributing",
    "text": "Contributing\nThis book is free and open source, so please do open an issue if you notice a problem!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "indexing.html",
    "href": "indexing.html",
    "title": "1  Indexing (Or Lack Thereof)",
    "section": "",
    "text": "1.1 Fetch Data (No Dataframes Here)\nFirst we fetch some flight delay data. This part isn’t about dataframes so feel free to skip the code.\nCode\nfrom pathlib import Path\nfrom zipfile import ZipFile\nimport requests\n\ndata_dir = Path(\"../data\") # replace this with a directory of your choice\ndest = data_dir / \"flights.csv.zip\"\n\nif not dest.exists():\n    r = requests.get(\n        \"https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_1.zip\",\n        verify=False,\n        stream=True,\n    )\n\n    data_dir.mkdir(exist_ok=True)\n    with dest.open(\"wb\") as f:\n        for chunk in r.iter_content(chunk_size=102400):\n            if chunk:\n                f.write(chunk)\n\n    with ZipFile(dest) as zf:\n        zf.extract(zf.filelist[0].filename, path=data_dir)\n\nextracted = data_dir / \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Indexing (Or Lack Thereof)</span>"
    ]
  },
  {
    "objectID": "indexing.html#read-the-data",
    "href": "indexing.html#read-the-data",
    "title": "1  Indexing (Or Lack Thereof)",
    "section": "1.2 Read the data",
    "text": "1.2 Read the data\n\n\n\n\n\n\nTip\n\n\n\nThe examples in this book use the lazy evaluation feature of Polars less than you should. It’s just inconvenient to use the lazy API when displaying dozens of intermediate results for educational purposes.\n\n\n\nPolarsPandas\n\n\n\nimport polars as pl\npl.Config.set_tbl_rows(5) # don't print too many rows in the book\ndf_pl = pl.read_csv(extracted, truncate_ragged_lines=True)\ndf_pl\n\n\nshape: (537_902, 110)\n\n\n\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\nFlightDate\nReporting_Airline\nDOT_ID_Reporting_Airline\nIATA_CODE_Reporting_Airline\nTail_Number\nFlight_Number_Reporting_Airline\nOriginAirportID\nOriginAirportSeqID\nOriginCityMarketID\nOrigin\nOriginCityName\nOriginState\nOriginStateFips\nOriginStateName\nOriginWac\nDestAirportID\nDestAirportSeqID\nDestCityMarketID\nDest\nDestCityName\nDestState\nDestStateFips\nDestStateName\nDestWac\nCRSDepTime\nDepTime\nDepDelay\nDepDelayMinutes\nDepDel15\nDepartureDelayGroups\nDepTimeBlk\nTaxiOut\n…\nDiv1TotalGTime\nDiv1LongestGTime\nDiv1WheelsOff\nDiv1TailNum\nDiv2Airport\nDiv2AirportID\nDiv2AirportSeqID\nDiv2WheelsOn\nDiv2TotalGTime\nDiv2LongestGTime\nDiv2WheelsOff\nDiv2TailNum\nDiv3Airport\nDiv3AirportID\nDiv3AirportSeqID\nDiv3WheelsOn\nDiv3TotalGTime\nDiv3LongestGTime\nDiv3WheelsOff\nDiv3TailNum\nDiv4Airport\nDiv4AirportID\nDiv4AirportSeqID\nDiv4WheelsOn\nDiv4TotalGTime\nDiv4LongestGTime\nDiv4WheelsOff\nDiv4TailNum\nDiv5Airport\nDiv5AirportID\nDiv5AirportSeqID\nDiv5WheelsOn\nDiv5TotalGTime\nDiv5LongestGTime\nDiv5WheelsOff\nDiv5TailNum\n\n\n\ni64\ni64\ni64\ni64\ni64\nstr\nstr\ni64\nstr\nstr\ni64\ni64\ni64\ni64\nstr\nstr\nstr\ni64\nstr\ni64\ni64\ni64\ni64\nstr\nstr\nstr\ni64\nstr\ni64\ni64\nstr\nf64\nf64\nf64\ni64\nstr\nf64\n…\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n2022\n1\n1\n14\n5\n\"2022-01-14\"\n\"YX\"\n20452\n\"YX\"\n\"N119HQ\"\n4879\n11066\n1106606\n31066\n\"CMH\"\n\"Columbus, OH\"\n\"OH\"\n39\n\"Ohio\"\n44\n11278\n1127805\n30852\n\"DCA\"\n\"Washington, DC\"\n\"VA\"\n51\n\"Virginia\"\n38\n1224\n\"1221\"\n-3.0\n0.0\n0.0\n-1\n\"1200-1259\"\n28.0\n…\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n15\n6\n\"2022-01-15\"\n\"YX\"\n20452\n\"YX\"\n\"N122HQ\"\n4879\n11066\n1106606\n31066\n\"CMH\"\n\"Columbus, OH\"\n\"OH\"\n39\n\"Ohio\"\n44\n11278\n1127805\n30852\n\"DCA\"\n\"Washington, DC\"\n\"VA\"\n51\n\"Virginia\"\n38\n1224\n\"1214\"\n-10.0\n0.0\n0.0\n-1\n\"1200-1259\"\n19.0\n…\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n16\n7\n\"2022-01-16\"\n\"YX\"\n20452\n\"YX\"\n\"N412YX\"\n4879\n11066\n1106606\n31066\n\"CMH\"\n\"Columbus, OH\"\n\"OH\"\n39\n\"Ohio\"\n44\n11278\n1127805\n30852\n\"DCA\"\n\"Washington, DC\"\n\"VA\"\n51\n\"Virginia\"\n38\n1224\n\"1218\"\n-6.0\n0.0\n0.0\n-1\n\"1200-1259\"\n16.0\n…\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n2022\n1\n1\n6\n4\n\"2022-01-06\"\n\"DL\"\n19790\n\"DL\"\n\"N989AT\"\n1579\n11057\n1105703\n31057\n\"CLT\"\n\"Charlotte, NC\"\n\"NC\"\n37\n\"North Carolina\"\n36\n10397\n1039707\n30397\n\"ATL\"\n\"Atlanta, GA\"\n\"GA\"\n13\n\"Georgia\"\n34\n1258\n\"1257\"\n-1.0\n0.0\n0.0\n-1\n\"1200-1259\"\n15.0\n…\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n2022\n1\n1\n6\n4\n\"2022-01-06\"\n\"DL\"\n19790\n\"DL\"\n\"N815DN\"\n1580\n14869\n1486903\n34614\n\"SLC\"\n\"Salt Lake City, UT\"\n\"UT\"\n49\n\"Utah\"\n87\n14057\n1405702\n34057\n\"PDX\"\n\"Portland, OR\"\n\"OR\"\n41\n\"Oregon\"\n92\n2240\n\"2231\"\n-9.0\n0.0\n0.0\n-1\n\"2200-2259\"\n10.0\n…\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\n\"\"\nnull\nnull\n\"\"\nnull\nnull\n\"\"\n\"\"\nnull\n\n\n\n\n\n\n\n\n\nimport pandas as pd\npd.options.display.max_rows = 5\ndf_pd = pd.read_csv(extracted)\ndf_pd\n\n/tmp/ipykernel_353510/2805799744.py:3: DtypeWarning: Columns (76,77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n  df_pd = pd.read_csv(extracted)\n\n\n\n\n\n\n\n\n\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\nFlightDate\nReporting_Airline\nDOT_ID_Reporting_Airline\nIATA_CODE_Reporting_Airline\nTail_Number\n...\nDiv4TailNum\nDiv5Airport\nDiv5AirportID\nDiv5AirportSeqID\nDiv5WheelsOn\nDiv5TotalGTime\nDiv5LongestGTime\nDiv5WheelsOff\nDiv5TailNum\nUnnamed: 109\n\n\n\n\n0\n2022\n1\n1\n14\n5\n2022-01-14\nYX\n20452\nYX\nN119HQ\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2022\n1\n1\n15\n6\n2022-01-15\nYX\n20452\nYX\nN122HQ\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n537900\n2022\n1\n1\n6\n4\n2022-01-06\nDL\n19790\nDL\nN989AT\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n537901\n2022\n1\n1\n6\n4\n2022-01-06\nDL\n19790\nDL\nN815DN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n537902 rows × 110 columns",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Indexing (Or Lack Thereof)</span>"
    ]
  },
  {
    "objectID": "indexing.html#indexing",
    "href": "indexing.html#indexing",
    "title": "1  Indexing (Or Lack Thereof)",
    "section": "1.3 Indexing",
    "text": "1.3 Indexing\nPandas uses a special index type that is quite powerful for selecting rows and columns but is also very complicated. To quote Modern Pandas:\n\nThe complexity of pandas’ indexing is a microcosm for the complexity of the pandas API in general. There’s a reason for the complexity (well, most of it), but that’s not much consolation while you’re learning. Still, all of these ways of indexing really are useful enough to justify their inclusion in the library.\n\nIt’s true that Pandas indexing is quite useful, but it’s also true that everyone always forgets how to do anything non-trivial with it. The benefits of being able to put df.loc[pd.IndexSlice[:, 'B0':'B1'], :]] in your code base are somewhat dubious.\nPolars avoids this complexity by simply not having an index. It just has ordinary methods like .select, .filter and .head for accessing a subset of rows or columns.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Indexing (Or Lack Thereof)</span>"
    ]
  },
  {
    "objectID": "indexing.html#slicing-vs-selecting",
    "href": "indexing.html#slicing-vs-selecting",
    "title": "1  Indexing (Or Lack Thereof)",
    "section": "1.4 Slicing vs Selecting",
    "text": "1.4 Slicing vs Selecting\nIn Pandas you can subset a dataframe with .loc[], .iloc[] or just []. In Polars you select rows and columns with expressions as noted above.\nYou can also use square bracket indexing in Polars, but it doesn’t work in lazy mode so is only to be used when convenience is most important.\nHere are some examples:\n\n1.4.1 Rows by number, columns by name\n\nPolars (recommended)Polars (square bracket)Pandas\n\n\nUsing head and tail:\n\ndf_pl.select([\"Dest\", \"Tail_Number\"]).head(16).tail(4)\n\n\nshape: (4, 2)\n\n\n\nDest\nTail_Number\n\n\nstr\nstr\n\n\n\n\n\"DCA\"\n\"N132HQ\"\n\n\n\"DCA\"\n\"N109HQ\"\n\n\n\"DCA\"\n\"N421YX\"\n\n\n\"DCA\"\n\"N137HQ\"\n\n\n\n\n\n\nOr using gather:\n\ndf_pl.select(pl.col([\"Dest\", \"Tail_Number\"]).gather(list(range(12, 16))))\n\n\nshape: (4, 2)\n\n\n\nDest\nTail_Number\n\n\nstr\nstr\n\n\n\n\n\"DCA\"\n\"N132HQ\"\n\n\n\"DCA\"\n\"N109HQ\"\n\n\n\"DCA\"\n\"N421YX\"\n\n\n\"DCA\"\n\"N137HQ\"\n\n\n\n\n\n\n\n\n\ndf_pl[12:16, [\"Dest\", \"Tail_Number\"]]\n\n\nshape: (4, 2)\n\n\n\nDest\nTail_Number\n\n\nstr\nstr\n\n\n\n\n\"DCA\"\n\"N132HQ\"\n\n\n\"DCA\"\n\"N109HQ\"\n\n\n\"DCA\"\n\"N421YX\"\n\n\n\"DCA\"\n\"N137HQ\"\n\n\n\n\n\n\n\n\n\ndf_pd.loc[12:15, [\"Dest\", \"Tail_Number\"]]\n\n\n\n\n\n\n\n\nDest\nTail_Number\n\n\n\n\n12\nDCA\nN132HQ\n\n\n13\nDCA\nN109HQ\n\n\n14\nDCA\nN421YX\n\n\n15\nDCA\nN137HQ\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.2 Rows by string index, columns by name\nSince there’s no such thing as an index in Polars, so we just use .filter:\n\nPolarsPandas\n\n\n\n(\n    df_pl\n    .filter(pl.col(\"IATA_CODE_Reporting_Airline\").is_in(['AA', 'DL']))\n    .select([\"IATA_CODE_Reporting_Airline\", \"Dest\", \"Tail_Number\"])\n)\n\n\nshape: (138_363, 3)\n\n\n\nIATA_CODE_Reporting_Airline\nDest\nTail_Number\n\n\nstr\nstr\nstr\n\n\n\n\n\"DL\"\n\"LGA\"\n\"N315DN\"\n\n\n\"DL\"\n\"FLL\"\n\"N545US\"\n\n\n\"DL\"\n\"ATL\"\n\"N545US\"\n\n\n…\n…\n…\n\n\n\"DL\"\n\"ATL\"\n\"N989AT\"\n\n\n\"DL\"\n\"PDX\"\n\"N815DN\"\n\n\n\n\n\n\n\n\n\n(\n    df_pd\n    .set_index(\"IATA_CODE_Reporting_Airline\")\n    .loc[['AA', 'DL'], [\"Dest\", \"Tail_Number\"]]\n)\n\n\n\n\n\n\n\n\nDest\nTail_Number\n\n\nIATA_CODE_Reporting_Airline\n\n\n\n\n\n\nAA\nLAX\nN106NN\n\n\nAA\nLAX\nN112AN\n\n\n...\n...\n...\n\n\nDL\nATL\nN989AT\n\n\nDL\nPDX\nN815DN\n\n\n\n\n138363 rows × 2 columns\n\n\n\n\n\n\n\n\n1.4.3 Rows by number, columns by number\nThe Polars docs recommend doing this the evil way with square brackets, so make of that what you will. Selecting columns by number isn’t a very common operation anyway.\n\nPolarsPandas\n\n\n\ndf_pl[[0, 1, 3], [0, 1]]\n\n\nshape: (3, 2)\n\n\n\nYear\nQuarter\n\n\ni64\ni64\n\n\n\n\n2022\n1\n\n\n2022\n1\n\n\n2022\n1\n\n\n\n\n\n\n\n\n\ndf_pd.iloc[[0, 1, 3], [0, 1]]\n\n\n\n\n\n\n\n\nYear\nQuarter\n\n\n\n\n0\n2022\n1\n\n\n1\n2022\n1\n\n\n3\n2022\n1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Indexing (Or Lack Thereof)</span>"
    ]
  },
  {
    "objectID": "indexing.html#settingwithcopy",
    "href": "indexing.html#settingwithcopy",
    "title": "1  Indexing (Or Lack Thereof)",
    "section": "1.5 SettingWithCopy",
    "text": "1.5 SettingWithCopy\nPandas has this cute thing where if you assign values to some subset of the dataframe with square bracket indexing, it doesn’t work and gives the notorious SettingWithCopyWarning. To be fair, this warning also tells you to assign using .loc. Unfortunately many people in the Pandas community can’t read and instead just ignore the warning.\nPolars is not yet popular enough to attract the same crowd, but when it does it should not run into the same problem, as the only way to add or overwrite columns in Polars is the with_columns method.\n\nPolarsPandas (bad)Pandas (good)Pandas (better)\n\n\n\nf = pl.DataFrame({'a': [1,2,3,4,5], 'b': [10,20,30,40,50]})\nf.with_columns(\n    pl.when(pl.col(\"a\") &lt;= 3)\n    .then(pl.col(\"b\") // 10)\n    .otherwise(pl.col(\"b\"))\n)\n\n\nshape: (5, 2)\n\n\n\na\nb\n\n\ni64\ni64\n\n\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n40\n\n\n5\n50\n\n\n\n\n\n\n\n\n\nf = pd.DataFrame({'a': [1,2,3,4,5], 'b': [10,20,30,40,50]})\nf[f['a'] &lt;= 3]['b'] = f['b'] // 10\nf\n\n/tmp/ipykernel_353510/1317853993.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  f[f['a'] &lt;= 3]['b'] = f['b'] // 10\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n10\n\n\n1\n2\n20\n\n\n2\n3\n30\n\n\n3\n4\n40\n\n\n4\n5\n50\n\n\n\n\n\n\n\n\n\n\nf = pd.DataFrame({'a': [1,2,3,4,5], 'b': [10,20,30,40,50]})\nf.loc[f['a'] &lt;= 3, \"b\"] = f['b'] // 10\nf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1\n\n\n1\n2\n2\n\n\n2\n3\n3\n\n\n3\n4\n40\n\n\n4\n5\n50\n\n\n\n\n\n\n\n\n\n\nf = pd.DataFrame({'a': [1,2,3,4,5], 'b': [10,20,30,40,50]})\nf.assign(b=f[\"b\"].mask(f[\"a\"] &lt;=3, f[\"b\"] // 10))\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n1\n\n\n1\n2\n2\n\n\n2\n3\n3\n\n\n3\n4\n40\n\n\n4\n5\n50",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Indexing (Or Lack Thereof)</span>"
    ]
  },
  {
    "objectID": "indexing.html#summary",
    "href": "indexing.html#summary",
    "title": "1  Indexing (Or Lack Thereof)",
    "section": "1.6 Summary",
    "text": "1.6 Summary\nBasically, there’s no index in Polars and square brackets are bad most of the time. I think the lack of an index is quite acceptable even if there are cases where it’s useful. Most Pandas users just call .reset_index() all the time anyway.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Indexing (Or Lack Thereof)</span>"
    ]
  },
  {
    "objectID": "method_chaining.html",
    "href": "method_chaining.html",
    "title": "2  Method Chaining",
    "section": "",
    "text": "2.1 Setup\nfrom pathlib import Path\nimport polars as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npl.Config.set_tbl_rows(5)\npd.options.display.max_rows = 5\n\ndata_dir = Path(\"../data\")\nextracted = data_dir / \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Method Chaining</span>"
    ]
  },
  {
    "objectID": "method_chaining.html#extract-city-names",
    "href": "method_chaining.html#extract-city-names",
    "title": "2  Method Chaining",
    "section": "2.2 Extract city names",
    "text": "2.2 Extract city names\nThe dataset has two columns that look like $city, $state. Let’s define a function that removes the state part from these columns. There’s no method chaining yet but we do have a few things to talk about while we’re here:\n\nPolarsPandas\n\n\n\ndef extract_city_name_pl() -&gt; pl.Expr:\n    \"\"\"\n    Chicago, IL -&gt; Chicago for OriginCityName and DestCityName\n    \"\"\"\n    cols = [\"OriginCityName\", \"DestCityName\"]\n    return pl.col(cols).str.split(\",\").list.get(0)\n\n\n\n\ndef extract_city_name_pd(df: pd.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Chicago, IL -&gt; Chicago for OriginCityName and DestCityName\n    \"\"\"\n    cols = [\"OriginCityName\", \"DestCityName\"]\n    return df.assign(**{col: df[col].str.split(\",\", regex=False).str[0] for col in cols})\n\n\n\n\nSome items to note:\n\nOur Pandas function adds columns to a dataframe, while our Polars function simply generates a Polars expression. You’ll find it’s often easier to pass around Exprs than dataframes because:\n\nThey work on both DataFrame and LazyFrame, and they aren’t bound to any particular data.\nPolars performs better if you put everything in one .select or .with_columns call, rather than calling .select multiple times. If you pass around expressions, this pattern is easy.\n\nPolars is fast and convenient for doing the same thing to multiple columns. We can pass a list of columns to pl.col and then call a method on that pl.col as if it were one column. When the expression gets executed it will be parallelized by Polars.\nMeanwhile in Pandas we have to loop through the columns to create a dictionary of kwargs for .assign. This is not parallelized. (We could use .apply with axis=0 instead but this would still take place sequentially and isn’t any easier to read, in my opinion).\nCalling .str.split in Polars creates a column where every element is a list. This kind of data is annoying in Pandas because it’s slow and awkward to work with - notice how the most convenient way to get the first element of a list column in Pandas is to call .str[0], even though this is a list, not a string 🤔\nI’m not sure if that’s even supposed to work. In contrast, Polars actually has first class support for list columns, and they are fast as long as they don’t have mixed types.\n\n\nPolarsPandas\n\n\n\ndef time_col_pl(col: str) -&gt; pl.Expr:\n    col_expr = pl.col(col)\n    return (\n        pl.when(col_expr == \"2400\")\n        .then(pl.lit(\"0000\"))\n        .otherwise(col_expr)\n        .str.strptime(pl.Time, \"%H%M\", strict=True)\n        .alias(col)\n    )\n\n\ndef time_to_datetime_pl(columns: list[str]) -&gt; list[pl.Expr]:\n    \"\"\"\n    Combine all time items into datetimes.\n\n    2014-01-01,0914 -&gt; 2014-01-01 09:14:00\n    \"\"\"\n    date_col = pl.col(\"FlightDate\")\n    return [\n        date_col\n        .dt.combine(time_col_pl(col))\n        .alias(col)\n        for col in columns\n    ]\n\n\n\n\ndef time_col_pd(col: str, df: pd.DataFrame) -&gt; pd.Series:\n    timepart = df[col].replace(\"2400\", \"0000\")\n    return pd.to_datetime(df[\"FlightDate\"] + ' ' +\n                            timepart.str.slice(0, 2) + ':' +\n                            timepart.str.slice(2, 4),\n                            errors='coerce')\n\ndef time_to_datetime_pd(df: pd.DataFrame, columns: list[str]) -&gt; pd.DataFrame:\n    '''\n    Combine all time items into datetimes.\n\n    2014-01-01,0914 -&gt; 2014-01-01 09:14:00\n    '''\n    return df.assign(**{col: time_col_pd(col, df) for col in columns})\n\n\n\n\nThe Pandas version concatenates the date and time strings then parses the datetime string. We could do the same in Polars but I wanted to show you the pl.Date and pl.Time dtypes, which Pandas lacks. We can then combine pl.Date and pl.Time with .dt.combine().",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Method Chaining</span>"
    ]
  },
  {
    "objectID": "method_chaining.html#bringing-it-all-back-home",
    "href": "method_chaining.html#bringing-it-all-back-home",
    "title": "2  Method Chaining",
    "section": "2.3 Bringing It All Back Home",
    "text": "2.3 Bringing It All Back Home\nIt’s time for some method chaining. First, some common variables for both the Polars and Pandas code:\n\ncategory_cols = [\n    \"Dest\",\n    \"Tail_Number\",\n    \"IATA_CODE_Reporting_Airline\",\n    \"CancellationCode\",\n]\ntime_cols = [\"DepTime\", \"ArrTime\", \"CRSArrTime\", \"CRSDepTime\"]\ncols = (\n    category_cols\n    + time_cols\n    + [\n        \"FlightDate\",\n        \"Flight_Number_Reporting_Airline\",\n        \"OriginCityName\",\n        \"DestCityName\",\n        \"Origin\",\n        \"DepDelay\",\n    ]\n)\n\nNow to read the CSVs and use the functions we defined above:\n\nPolarsPandas\n\n\n\ndtypes_pl = (\n    {col: pl.Categorical for col in category_cols}\n    | {\"FlightDate\": pl.Date}\n    | {col: pl.Utf8 for col in time_cols}\n)\ndf_pl = (\n    pl.scan_csv(extracted, schema_overrides=dtypes_pl, null_values=\"\")\n    .select(cols)\n    .with_columns([extract_city_name_pl(), *time_to_datetime_pl(time_cols)])\n    .collect()\n)\ndf_pl.head()\n\n\nshape: (5, 14)\n\n\n\nDest\nTail_Number\nIATA_CODE_Reporting_Airline\nCancellationCode\nDepTime\nArrTime\nCRSArrTime\nCRSDepTime\nFlightDate\nFlight_Number_Reporting_Airline\nOriginCityName\nDestCityName\nOrigin\nDepDelay\n\n\ncat\ncat\ncat\ncat\ndatetime[μs]\ndatetime[μs]\ndatetime[μs]\ndatetime[μs]\ndate\ni64\nstr\nstr\nstr\nf64\n\n\n\n\n\"DCA\"\n\"N119HQ\"\n\"YX\"\nnull\n2022-01-14 12:21:00\n2022-01-14 13:56:00\n2022-01-14 13:52:00\n2022-01-14 12:24:00\n2022-01-14\n4879\n\"Columbus\"\n\"Washington\"\n\"CMH\"\n-3.0\n\n\n\"DCA\"\n\"N122HQ\"\n\"YX\"\nnull\n2022-01-15 12:14:00\n2022-01-15 13:28:00\n2022-01-15 13:52:00\n2022-01-15 12:24:00\n2022-01-15\n4879\n\"Columbus\"\n\"Washington\"\n\"CMH\"\n-10.0\n\n\n\"DCA\"\n\"N412YX\"\n\"YX\"\nnull\n2022-01-16 12:18:00\n2022-01-16 13:39:00\n2022-01-16 13:52:00\n2022-01-16 12:24:00\n2022-01-16\n4879\n\"Columbus\"\n\"Washington\"\n\"CMH\"\n-6.0\n\n\n\"DCA\"\n\"N405YX\"\n\"YX\"\nnull\n2022-01-17 12:17:00\n2022-01-17 14:01:00\n2022-01-17 13:52:00\n2022-01-17 12:24:00\n2022-01-17\n4879\n\"Columbus\"\n\"Washington\"\n\"CMH\"\n-7.0\n\n\n\"DCA\"\n\"N420YX\"\n\"YX\"\nnull\n2022-01-18 12:18:00\n2022-01-18 13:23:00\n2022-01-18 13:52:00\n2022-01-18 12:24:00\n2022-01-18\n4879\n\"Columbus\"\n\"Washington\"\n\"CMH\"\n-6.0\n\n\n\n\n\n\n\n\n\ndtypes_pd = (\n    {col: pd.CategoricalDtype() for col in category_cols}\n    | {col: pd.StringDtype() for col in time_cols}\n)\ndf_pd = (\n    pd.read_csv(extracted, dtype=dtypes_pd, usecols=cols, na_values=\"\")\n    .pipe(extract_city_name_pd)\n    .pipe(time_to_datetime_pd, time_cols)\n    .assign(FlightDate=lambda df: pd.to_datetime(df[\"FlightDate\"]))\n)\ndf_pd[cols].head()\n\n\n\n\n\n\n\n\nDest\nTail_Number\nIATA_CODE_Reporting_Airline\nCancellationCode\nDepTime\nArrTime\nCRSArrTime\nCRSDepTime\nFlightDate\nFlight_Number_Reporting_Airline\nOriginCityName\nDestCityName\nOrigin\nDepDelay\n\n\n\n\n0\nDCA\nN119HQ\nYX\nNaN\n2022-01-14 12:21:00\n2022-01-14 13:56:00\n2022-01-14 13:52:00\n2022-01-14 12:24:00\n2022-01-14\n4879\nColumbus\nWashington\nCMH\n-3.0\n\n\n1\nDCA\nN122HQ\nYX\nNaN\n2022-01-15 12:14:00\n2022-01-15 13:28:00\n2022-01-15 13:52:00\n2022-01-15 12:24:00\n2022-01-15\n4879\nColumbus\nWashington\nCMH\n-10.0\n\n\n2\nDCA\nN412YX\nYX\nNaN\n2022-01-16 12:18:00\n2022-01-16 13:39:00\n2022-01-16 13:52:00\n2022-01-16 12:24:00\n2022-01-16\n4879\nColumbus\nWashington\nCMH\n-6.0\n\n\n3\nDCA\nN405YX\nYX\nNaN\n2022-01-17 12:17:00\n2022-01-17 14:01:00\n2022-01-17 13:52:00\n2022-01-17 12:24:00\n2022-01-17\n4879\nColumbus\nWashington\nCMH\n-7.0\n\n\n4\nDCA\nN420YX\nYX\nNaN\n2022-01-18 12:18:00\n2022-01-18 13:23:00\n2022-01-18 13:52:00\n2022-01-18 12:24:00\n2022-01-18\n4879\nColumbus\nWashington\nCMH\n-6.0\n\n\n\n\n\n\n\n\n\n\nDifferences between the two approaches:\n\nSince scan_csv is lazy, using scan_csv followed by .selecting a subset of columns is equivalent to usecols in pd.read_csv. This is why pl.scan_csv itself doesn’t have a parameter for choosing a subset of columns to read.\nPolars does have a .pipe method, but we don’t use it in this case since it’s easier to work with expressions.\n\n\n\n\n\n\n\nTip\n\n\n\nThe .with_columns method is for adding new columns or overwriting existing ones. But .select can also do those things, so you may be wondering: what’s the difference?\nBasically the .with_columns method is just convenient for when you don’t want to reselect all the columns you’re not modifying.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Method Chaining</span>"
    ]
  },
  {
    "objectID": "method_chaining.html#example-plots",
    "href": "method_chaining.html#example-plots",
    "title": "2  Method Chaining",
    "section": "2.4 Example plots",
    "text": "2.4 Example plots\n\n2.4.1 Daily Flights\nHere’s how plotting the number of daily flights looks in Polars and Pandas:\n\n\n\n\n\n\nNote\n\n\n\nPolars has its own built-in plotting with hvPlot but the time series plots are not great. Since most of the plots in this book are time series plots, we’ll just use .to_pandas() followed by .plot() function after doing all the data manipulation in Polars.\n\n\n\nPolarsPandas\n\n\n\n# filter for the busiest airlines\nfilter_expr = pl.col(\"IATA_CODE_Reporting_Airline\").is_in(\n    pl.col(\"IATA_CODE_Reporting_Airline\")\n    .value_counts(sort=True)\n    .struct.field(\"IATA_CODE_Reporting_Airline\")\n    .head(5)\n)\n(\n    df_pl\n    .drop_nulls(subset=[\"DepTime\", \"IATA_CODE_Reporting_Airline\"])\n    .filter(filter_expr)\n    .sort(\"DepTime\")\n    .group_by_dynamic(\n        \"DepTime\",\n        every=\"1h\",\n        group_by=\"IATA_CODE_Reporting_Airline\")\n    .agg(pl.col(\"Flight_Number_Reporting_Airline\").count())\n    .pivot(\n        index=\"DepTime\",\n        on=\"IATA_CODE_Reporting_Airline\",\n        values=\"Flight_Number_Reporting_Airline\",\n    )\n    .sort(\"DepTime\")\n    # fill every missing hour with 0 so the plot looks better\n    .upsample(time_column=\"DepTime\", every=\"1h\")\n    .fill_null(0)\n    .select([pl.col(\"DepTime\"), pl.col(pl.UInt32).rolling_sum(24)])\n    .to_pandas()\n    .set_index(\"DepTime\")\n    .rename_axis(\"Flights per Day\", axis=1)\n    .plot()\n)\n\n\n\n\n\n\n\n\n\n\n\n(\n    df_pd\n    .dropna(subset=[\"DepTime\", \"IATA_CODE_Reporting_Airline\"])\n    # filter for the busiest airlines\n    .loc[\n        lambda x: x[\"IATA_CODE_Reporting_Airline\"].isin(\n            x[\"IATA_CODE_Reporting_Airline\"].value_counts().index[:5]\n        )\n    ]\n    .assign(\n        IATA_CODE_Reporting_Airline=lambda x: x[\n            \"IATA_CODE_Reporting_Airline\"\n        ].cat.remove_unused_categories()  #  annoying pandas behaviour\n    )\n    .set_index(\"DepTime\")\n    # TimeGrouper to resample & groupby at once\n    .groupby([\"IATA_CODE_Reporting_Airline\", pd.Grouper(freq=\"h\")])[\n        \"Flight_Number_Reporting_Airline\"\n    ]\n    .count()\n    # the .pivot takes care of this in the Polars code.\n    .unstack(0)\n    .fillna(0)\n    .rolling(24)\n    .sum()\n    .rename_axis(\"Flights per Day\", axis=1)\n    .plot()\n)\n\n/tmp/ipykernel_353606/2762846539.py:17: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  .groupby([\"IATA_CODE_Reporting_Airline\", pd.Grouper(freq=\"h\")])[\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences between Polars and Pandas:\n\nTo group by a time window and another value, we use .groupby_dynamic. In Pandas we use .groupby with the pd.Grouper helper.\nInstead of .rolling(n).sum(), Polars has .rolling_sum(n).\nIf you see Pandas code using .unstack, the corresponding Polars code probably needs .pivot.\nIn Polars, .value_counts returns a pl.Struct column containing the value and the value count. In Pandas it returns a series where the elements are the value counts and the index contains the values themselves.\nIn Polars we need to select all the UInt32 cols at one point using pl.col(pl.UInt32). In Pandas, the way .rolling works means we don’t need to select these cols explicitly, but if we did it would look like df.select_dtypes(\"uint32\").\n\n\n\n2.4.2 Planes With Multiple Daily Flights\nNow let’s see if planes with multiple flights per day tend to get delayed as the day goes on:\n\nPolarsPandas\n\n\n\nflights_pl = (\n    df_pl.select(\n        pl.col([\n            \"FlightDate\",\n            \"Tail_Number\",\n            \"DepTime\",\n            \"DepDelay\"\n        ])\n    )\n    .drop_nulls()\n    .sort(\"DepTime\")\n    .filter(pl.col(\"DepDelay\") &lt; 500)\n    .with_columns(\n        pl.col(\"DepTime\")\n        .rank()\n        .over([\"FlightDate\", \"Tail_Number\"])\n        .alias(\"turn\")\n    )\n)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.boxplot(x=\"turn\", y=\"DepDelay\", data=flights_pl, ax=ax)\nax.set_ylim(-50, 50)\n\n\n\n\n\n\n\n\n\n\n\nflights_pd = (\n    df_pd[[\n        \"FlightDate\",\n        \"Tail_Number\",\n        \"DepTime\",\n        \"DepDelay\"\n    ]]\n    .dropna()\n    .sort_values('DepTime')\n    .loc[lambda x: x[\"DepDelay\"] &lt; 500]\n    .assign(turn = lambda x:\n        x.groupby([\"FlightDate\", \"Tail_Number\"])\n        [\"DepTime\"].transform('rank')\n        .astype(int)\n    )\n)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.boxplot(x=\"turn\", y=\"DepDelay\", data=flights_pd, ax=ax)\nax.set_ylim(-50, 50)\n\n/tmp/ipykernel_353606/2848021590.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  x.groupby([\"FlightDate\", \"Tail_Number\"])\n\n\n\n\n\n\n\n\n\n\n\n\nOne new thing here: window functions. When Pandas code looks like:\n.groupby(\"country\")[\"population\"].transform(\"sum\")\nthe equivalent Polars code will look like:\npl.col(\"population\").sum().over(\"country\")\n\n\n2.4.3 Delay by hour of the day\nMaybe later flights have longer delays:\n\nPolarsPandas\n\n\n\nplt.figure(figsize=(10, 5))\n(\n    df_pl.select(\n        pl.col(\n            [\"FlightDate\", \"Tail_Number\", \"DepTime\", \"DepDelay\"],\n        )\n    )\n    .drop_nulls()\n    .filter(pl.col(\"DepDelay\").is_between(5, 600, closed=\"none\"))\n    .with_columns(pl.col(\"DepTime\").dt.hour().alias(\"hour\"))\n    .to_pandas()\n    .pipe((sns.boxplot, \"data\"), x=\"hour\", y=\"DepDelay\")\n)\n\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\n(\n    df_pd[[\"FlightDate\", \"Tail_Number\", \"DepTime\", \"DepDelay\"]]\n    .dropna()\n    .loc[lambda df: df[\"DepDelay\"].between(5, 600, inclusive=\"neither\")]\n    .assign(hour=lambda df: df[\"DepTime\"].dt.hour)\n    .pipe((sns.boxplot, \"data\"), x=\"hour\", y=\"DepDelay\")\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Method Chaining</span>"
    ]
  },
  {
    "objectID": "method_chaining.html#how-much-is-too-much",
    "href": "method_chaining.html#how-much-is-too-much",
    "title": "2  Method Chaining",
    "section": "2.5 How much is too much?",
    "text": "2.5 How much is too much?\nThe above examples have some fairly long method chains that could perhaps be split up. That said, if I saw them in a PR I probably wouldn’t mind. Here’s a chain that’s too long:\n\nI would argue this code is not hard to follow, but empirically it provokes disgust in people. I think it’s just visually shocking, which is often a hallmark of terrible code. That’s enough reason to avoid doing this. You don’t want folks assuming you’ve lost your mind.\nIn the above example, it would probably be enough to just move those long lists to their own variables. That would make it more obvious that the code is really doing simple, menial things.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Method Chaining</span>"
    ]
  },
  {
    "objectID": "method_chaining.html#summary",
    "href": "method_chaining.html#summary",
    "title": "2  Method Chaining",
    "section": "2.6 Summary",
    "text": "2.6 Summary\n\nMethod chaining is great but use it with care.\nThe Polars code looks pretty good here next to Pandas. It’s less clunky and more conducive to fluent programming. I’ve noticed some people are under the impression that you should only use Polars when you need the performance, but I would argue the above examples show that it’s better for small data too.\nIf you want to translate Pandas code to Polars, pay attention to the examples above and the accompanying notes on the API differences.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Method Chaining</span>"
    ]
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "3  Performance",
    "section": "",
    "text": "3.1 Six fairly obvious performance rules\nHere are some tips that are almost always a good idea:\nThese are basically the same rules you’d follow when using Pandas, except for the one about the lazy API. Now for some comparisons between the performance of idiomatic Pandas and Polars.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#six-fairly-obvious-performance-rules",
    "href": "performance.html#six-fairly-obvious-performance-rules",
    "title": "3  Performance",
    "section": "",
    "text": "Use the lazy API.\nUse Exprs, and don’t use .apply unless you really have to.\nUse the smallest necessary numeric types (so if you have an integer between 0 and 255, use pl.UInt8, not pl.Int64). This will save both time and space.\nUse efficient storage (if you’re dumping stuff in files, Parquet is a good choice).\nUse categoricals for recurring strings (but note that it may not be worth it if there’s not much repetition).\nOnly select the columns you need.\n\n\n\n\n\n\n\nTip\n\n\n\nIf your colleagues are happy with CSVs and can’t be convinced to use something else, tell them that the Modern Polars book says they should feel bad.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#polars-is-faster-at-the-boring-stuff",
    "href": "performance.html#polars-is-faster-at-the-boring-stuff",
    "title": "3  Performance",
    "section": "3.2 Polars is faster at the boring stuff",
    "text": "3.2 Polars is faster at the boring stuff\nHere we’ll clean up a messy dataset, kindly provided by Kaggle user Rachit Toshniwal as a deliberate example of a really crap CSV. Most of the cleanup involves extracting numeric data from awkward strings.\nAlso, the data is too small so I’ve concatenated it to itself 20 times. We’re not doing anything that will care about the duplication. Here’s how the raw table looks:\n\n\nCode\nimport pandas as pd\npd.read_csv(\"../data/fifa21_raw_big.csv\", dtype=\"string\", nrows=2)\n\n\n\n\n\n\n\n\n\nID\nName\nLongName\nphotoUrl\nplayerUrl\nNationality\nAge\n↓OVA\nPOT\nClub\n...\nA/W\nD/W\nIR\nPAC\nSHO\nPAS\nDRI\nDEF\nPHY\nHits\n\n\n\n\n0\n158023\nL. Messi\nLionel Messi\nhttps://cdn.sofifa.com/players/158/023/21_60.png\nhttp://sofifa.com/player/158023/lionel-messi/2...\nArgentina\n33\n93\n93\nFC Barcelona\n...\nMedium\nLow\n5 ★\n85\n92\n91\n95\n38\n65\n771\n\n\n1\n20801\nCristiano Ronaldo\nC. Ronaldo dos Santos Aveiro\nhttps://cdn.sofifa.com/players/020/801/21_60.png\nhttp://sofifa.com/player/20801/c-ronaldo-dos-s...\nPortugal\n35\n92\n92\nJuventus\n...\nHigh\nLow\n5 ★\n89\n93\n81\n89\n35\n77\n562\n\n\n\n\n2 rows × 77 columns\n\n\n\nFor this exercise we’ll assume we want to make use of all the columns. First some boilerplate where we map out the different data types:\n\n\nCode\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nimport math\nstr_cols = [\n    \"Name\",\n    \"LongName\",\n    \"playerUrl\",\n    \"photoUrl\",\n]\ninitial_category_cols_pl = [\n    \"Nationality\",\n    \"Preferred Foot\",\n    \"Best Position\",\n    \"A/W\",\n    \"D/W\"\n]\ncategory_cols = [*initial_category_cols_pl, \"Club\"]\ndate_cols = [\n    \"Joined\",\n    \"Loan Date End\"\n]\n# these all start with the euro symbol and end with 0, M or K\nmoney_cols = [\n    \"Value\",\n    \"Wage\",\n    \"Release Clause\"\n]\nstar_cols = [\n    \"W/F\",\n    \"SM\",\n    \"IR\",\n]\n# Contract col is a range of years\n# Positions is a list of positions\n# Height is in cm\n# Weight is in kg\n# Hits is numbers with K and M \nmessy_cols = [\n    \"Contract\",\n    \"Positions\",\n    \"Height\",\n    \"Weight\",\n    \"Hits\"\n]\ninitially_str_cols = str_cols + date_cols + money_cols + star_cols + messy_cols\ninitially_str_cols_pl = [*initially_str_cols, \"Club\"]\nu32_cols = [\n    \"ID\",\n    \"Total Stats\"\n]\nu8_cols = [\n    'Age',\n    '↓OVA',\n    'POT',\n    'BOV',\n    'Crossing',\n    'Finishing',\n    'Heading Accuracy',\n    'Short Passing',\n    'Volleys',\n    'Dribbling',\n    'Curve',\n    'FK Accuracy',\n    'Long Passing',\n    'Ball Control',\n    'Acceleration',\n    'Sprint Speed',\n    'Agility',\n    'Reactions',\n    'Balance',\n    'Shot Power',\n    'Jumping',\n    'Stamina',\n    'Strength',\n    'Long Shots',\n    'Aggression',\n    'Interceptions',\n    'Positioning',\n    'Vision',\n    'Penalties',\n    'Composure',\n    'Marking',\n    'Standing Tackle',\n    'Sliding Tackle',\n    'GK Diving',\n    'GK Handling',\n    'GK Kicking',\n    'GK Positioning',\n    'GK Reflexes',\n    'PAC',\n    'SHO',\n    'PAS',\n    'DRI',\n    'DEF',\n    'PHY'\n]\n\nu16_cols = [\n    'Attacking',\n    'Skill',\n    'Movement',\n    'Power',\n    'Mentality',\n    'Defending',\n    'Goalkeeping',\n    'Total Stats',\n    'Base Stats'\n]\n\n\n\n3.2.1 Dtypes\nHere are the initial dtypes for the two dataframes:\n\nPolarsPandas\n\n\n\n# can't use UInt8/16 in scan_csv\ndtypes_pl = (\n    {col: pl.Utf8 for col in initially_str_cols_pl}\n    | {col: pl.Categorical for col in initial_category_cols_pl}\n    | {col: pl.UInt32 for col in [*u32_cols, *u16_cols, *u8_cols]}\n)\n\n\n\n\ndtypes_pd = (\n    {col: pd.StringDtype() for col in initially_str_cols}\n    | {col: pd.CategoricalDtype() for col in category_cols}\n    | {col: \"uint32\" for col in u32_cols}\n    | {col: \"uint8\" for col in u8_cols}\n    | {col: \"uint16\" for col in u16_cols}\n)\n\n\n\n\nOne thing I’ll note here is that Pandas numeric types are somewhat confusing: \"uint32\" means np.uint32 which is not the same thing as pd.UInt32Dtype(). Only the latter is nullable. On the other hand, Polars has just one unsigned 32-bit integer type, and it’s nullable.\n\n\n\n\n\n\nTip\n\n\n\nPolars expressions have a shrink_dtype method that can be more convenient than manually specifying the dtypes yourself. It’s not magic though, and it has to spend time finding the min and max of the column.\n\n\n\n\n3.2.2 Data cleaning\nThere’s not much that you haven’t seen here already, so we won’t explain the code line by line. The main new thing here is pl.when for ternary expressions.\n\nPolarsPandas\n\n\n\ndef parse_date_pl(col: pl.Expr) -&gt; pl.Expr:\n    return col.str.strptime(pl.Date, format=\"%b %d, %Y\")\n\ndef parse_suffixed_num_pl(col: pl.Expr) -&gt; pl.Expr:\n    suffix = col.str.slice(-1, 1)\n    suffix_value = (\n        pl.when(suffix == \"K\")\n        .then(1_000)\n        .when(suffix == \"M\")\n        .then(1_000_000)\n        .otherwise(1)\n        .cast(pl.UInt32)\n    )\n    without_suffix = (\n        col\n        .str.replace(\"K\", \"\", literal=True)\n        .str.replace(\"M\", \"\", literal=True)\n        .cast(pl.Float32)\n    )\n    original_name = col.meta.output_name()\n    return (suffix_value * without_suffix).alias(original_name)\n\ndef parse_money_pl(col: pl.Expr) -&gt; pl.Expr:\n    return parse_suffixed_num_pl(col.str.slice(1)).cast(pl.UInt32)\n\ndef parse_star_pl(col: pl.Expr) -&gt; pl.Expr:\n    return col.str.slice(0, 1).cast(pl.UInt8)\n\ndef feet_to_cm_pl(col: pl.Expr) -&gt; pl.Expr:\n    feet_inches_split = col.str.split_exact(\"'\", 1)\n    total_inches = (\n        (feet_inches_split.struct.field(\"field_0\").cast(pl.UInt8, strict=False) * 12)\n        + feet_inches_split.struct.field(\"field_1\").str.strip_chars_end('\"').cast(pl.UInt8, strict=False)\n    )\n    return (total_inches * 2.54).round(0).cast(pl.UInt8)\n\ndef parse_height_pl(col: pl.Expr) -&gt; pl.Expr:\n    is_cm = col.str.ends_with(\"cm\")\n    return (\n        pl.when(is_cm)\n        .then(col.str.slice(0, 3).cast(pl.UInt8, strict=False))\n        .otherwise(feet_to_cm_pl(col))\n    )\n\ndef parse_weight_pl(col: pl.Expr) -&gt; pl.Expr:\n    is_kg = col.str.ends_with(\"kg\")\n    without_unit = col.str.extract(r\"(\\d+)\").cast(pl.UInt8)\n    return (\n        pl.when(is_kg)\n        .then(without_unit)\n        .otherwise((without_unit * 0.453592).round(0).cast(pl.UInt8))\n    )\n\ndef parse_contract_pl(col: pl.Expr) -&gt; list[pl.Expr]:\n    contains_tilde = col.str.contains(\" ~ \", literal=True)\n    loan_str = \" On Loan\"\n    loan_col = col.str.ends_with(loan_str)\n    split = (\n        pl.when(contains_tilde)\n        .then(col)\n        .otherwise(None)\n        .str.split_exact(\" ~ \", 1)\n    )\n    start = split.struct.field(\"field_0\").cast(pl.UInt16).alias(\"contract_start\")\n    end = split.struct.field(\"field_1\").cast(pl.UInt16).alias(\"contract_end\")\n    free_agent = (col == \"Free\").alias(\"free_agent\").fill_null(False)\n    loan_date = (\n        pl.when(loan_col)\n        .then(col)\n        .otherwise(None)\n        .str.split_exact(\" On Loan\", 1)\n        .struct.field(\"field_0\")\n        .alias(\"loan_date_start\")\n    )\n    return [start, end, free_agent, parse_date_pl(loan_date)]\n\n\n\n\ndef parse_date_pd(col: pd.Series) -&gt; pd.Series:\n    return pd.to_datetime(col, format=\"%b %d, %Y\")\n\ndef parse_suffixed_num_pd(col: pd.Series) -&gt; pd.Series:\n    suffix_value = (\n        col\n        .str[-1]\n        .map({\"K\": 1_000, \"M\": 1_000_000})\n        .fillna(1)\n        .astype(\"uint32\")\n    )\n    without_suffix = (\n        col\n        .str.replace(\"K\", \"\", regex=False)\n        .str.replace(\"M\", \"\", regex=False)\n        .astype(\"float\")\n    )\n    return suffix_value * without_suffix\n\ndef parse_money_pd(col: pd.Series) -&gt; pd.Series:\n    return parse_suffixed_num_pd(col.str[1:]).astype(\"uint32\")\n\ndef parse_star_pd(col: pd.Series) -&gt; pd.Series:\n    return col.str[0].astype(\"uint8\")\n\ndef feet_to_cm_pd(col: pd.Series) -&gt; pd.Series:\n    feet_inches_split = col.str.split(\"'\", expand=True)\n    total_inches = (\n        feet_inches_split[0].astype(\"uint8\").mul(12)\n        + feet_inches_split[1].str[:-1].astype(\"uint8\")\n    )\n    return total_inches.mul(2.54).round().astype(\"uint8\")\n\ndef parse_height_pd(col: pd.Series) -&gt; pd.Series:\n    is_cm = col.str.endswith(\"cm\")\n    cm_values = col.loc[is_cm].str[:-2].astype(\"uint8\")\n    inches_as_cm = feet_to_cm_pd(col.loc[~is_cm])\n    return pd.concat([cm_values, inches_as_cm])\n\ndef parse_weight_pd(col: pd.Series) -&gt; pd.Series:\n    is_kg = col.str.endswith(\"kg\")\n    without_unit = col.where(is_kg, col.str[:-3]).mask(is_kg, col.str[:-2]).astype(\"uint8\")\n    return without_unit.where(is_kg, without_unit.mul(0.453592).round().astype(\"uint8\"))\n\ndef parse_contract_pd(df: pd.DataFrame) -&gt; pd.DataFrame:\n    contract_col = df[\"Contract\"]\n    contains_tilde = contract_col.str.contains(\" ~ \", regex=False)\n    split = (\n        contract_col.loc[contains_tilde].str.split(\" ~ \", expand=True).astype(pd.UInt16Dtype())\n    )\n    split.columns = [\"contract_start\", \"contract_end\"]\n    not_tilde = contract_col.loc[~contains_tilde]\n    free_agent = (contract_col == \"Free\").rename(\"free_agent\").fillna(False)\n    loan_date = parse_date_pd(not_tilde.loc[~free_agent].str[:-8]).rename(\"loan_date_start\")\n    return pd.concat([df.drop(\"Contract\", axis=1), split, free_agent, loan_date], axis=1)\n\n\n\n\n\n\n3.2.3 Performance comparison\nIn this example, Polars is ~150x faster than Pandas:\n\nPolarsPandas\n\n\n\n%%time\nnew_cols_pl = ([\n    pl.col(\"Club\").str.strip_chars().cast(pl.Categorical),\n    parse_suffixed_num_pl(pl.col(\"Hits\")).cast(pl.UInt32),\n    pl.col(\"Positions\").str.split(\",\"),\n    parse_height_pl(pl.col(\"Height\")),\n    parse_weight_pl(pl.col(\"Weight\")),\n]\n+ [parse_date_pl(pl.col(col)) for col in date_cols]\n+ [parse_money_pl(pl.col(col)) for col in money_cols]\n+ [parse_star_pl(pl.col(col)) for col in star_cols]\n+ parse_contract_pl(pl.col(\"Contract\"))\n+ [pl.col(col).cast(pl.UInt16) for col in u16_cols]\n+ [pl.col(col).cast(pl.UInt8) for col in u8_cols]\n)\nfifa_pl = (\n    pl.scan_csv(\"../data/fifa21_raw_big.csv\", schema_overrides=dtypes_pl)\n    .with_columns(new_cols_pl)\n    .drop(\"Contract\")\n    .rename({\"↓OVA\": \"OVA\"})\n    .collect()\n)\n\nCPU times: user 1.73 s, sys: 189 ms, total: 1.91 s\nWall time: 199 ms\n\n\n&lt;timed exec&gt;:20: CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation. Consider using a StringCache or an Enum type if the categories are known in advance\n\n\n\n\n\n%%time\nfifa_pd = (\n    pd.read_csv(\"../data/fifa21_raw_big.csv\", dtype=dtypes_pd)\n    .assign(Club=lambda df: df[\"Club\"].cat.rename_categories(lambda c: c.strip()),\n        **{col: lambda df: parse_date_pd(df[col]) for col in date_cols},\n        **{col: lambda df: parse_money_pd(df[col]) for col in money_cols},\n        **{col: lambda df: parse_star_pd(df[col]) for col in star_cols},\n        Hits=lambda df: parse_suffixed_num_pd(df[\"Hits\"]).astype(pd.UInt32Dtype()),\n        Positions=lambda df: df[\"Positions\"].str.split(\",\"),\n        Height=lambda df: parse_height_pd(df[\"Height\"]),\n        Weight=lambda df: parse_weight_pd(df[\"Weight\"])\n    )\n    .pipe(parse_contract_pd)\n    .rename(columns={\"↓OVA\": \"OVA\"})\n)\n\nCPU times: user 3.5 s, sys: 368 ms, total: 3.87 s\nWall time: 3.87 s\n\n\n\n\n\nOutput:\n\nPolarsPandas\n\n\n\nfifa_pl.head()\n\n\nshape: (5, 80)\n\n\n\nID\nName\nLongName\nphotoUrl\nplayerUrl\nNationality\nAge\nOVA\nPOT\nClub\nPositions\nHeight\nWeight\nPreferred Foot\nBOV\nBest Position\nJoined\nLoan Date End\nValue\nWage\nRelease Clause\nAttacking\nCrossing\nFinishing\nHeading Accuracy\nShort Passing\nVolleys\nSkill\nDribbling\nCurve\nFK Accuracy\nLong Passing\nBall Control\nMovement\nAcceleration\nSprint Speed\nAgility\n…\nStrength\nLong Shots\nMentality\nAggression\nInterceptions\nPositioning\nVision\nPenalties\nComposure\nDefending\nMarking\nStanding Tackle\nSliding Tackle\nGoalkeeping\nGK Diving\nGK Handling\nGK Kicking\nGK Positioning\nGK Reflexes\nTotal Stats\nBase Stats\nW/F\nSM\nA/W\nD/W\nIR\nPAC\nSHO\nPAS\nDRI\nDEF\nPHY\nHits\ncontract_start\ncontract_end\nfree_agent\nloan_date_start\n\n\nu32\nstr\nstr\nstr\nstr\ncat\nu8\nu8\nu8\ncat\nlist[str]\nu8\nu8\ncat\nu8\ncat\ndate\ndate\nu32\nu32\nu32\nu16\nu8\nu8\nu8\nu8\nu8\nu16\nu8\nu8\nu8\nu8\nu8\nu16\nu8\nu8\nu8\n…\nu8\nu8\nu16\nu8\nu8\nu8\nu8\nu8\nu8\nu16\nu8\nu8\nu8\nu16\nu8\nu8\nu8\nu8\nu8\nu16\nu16\nu8\nu8\ncat\ncat\nu8\nu8\nu8\nu8\nu8\nu8\nu8\nu32\nu16\nu16\nbool\ndate\n\n\n\n\n158023\n\"L. Messi\"\n\"Lionel Messi\"\n\"https://cdn.sofifa.com/players…\n\"http://sofifa.com/player/15802…\n\"Argentina\"\n33\n93\n93\n\"FC Barcelona\"\n[\"RW\", \" ST\", \" CF\"]\n170\n72\n\"Left\"\n93\n\"RW\"\n2004-07-01\nnull\n103500000\n560000\n138399993\n429\n85\n95\n70\n91\n88\n470\n96\n93\n94\n91\n96\n451\n91\n80\n91\n…\n69\n94\n347\n44\n40\n93\n95\n75\n96\n91\n32\n35\n24\n54\n6\n11\n15\n14\n8\n2231\n466\n4\n4\n\"Medium\"\n\"Low\"\n5\n85\n92\n91\n95\n38\n65\n771\n2004\n2021\nfalse\nnull\n\n\n20801\n\"Cristiano Ronaldo\"\n\"C. Ronaldo dos Santos Aveiro\"\n\"https://cdn.sofifa.com/players…\n\"http://sofifa.com/player/20801…\n\"Portugal\"\n35\n92\n92\n\"Juventus\"\n[\"ST\", \" LW\"]\n187\n83\n\"Right\"\n92\n\"ST\"\n2018-07-10\nnull\n63000000\n220000\n75900001\n437\n84\n95\n90\n82\n86\n414\n88\n81\n76\n77\n92\n431\n87\n91\n87\n…\n78\n93\n353\n63\n29\n95\n82\n84\n95\n84\n28\n32\n24\n58\n7\n11\n15\n14\n11\n2221\n464\n4\n5\n\"High\"\n\"Low\"\n5\n89\n93\n81\n89\n35\n77\n562\n2018\n2022\nfalse\nnull\n\n\n200389\n\"J. Oblak\"\n\"Jan Oblak\"\n\"https://cdn.sofifa.com/players…\n\"http://sofifa.com/player/20038…\n\"Slovenia\"\n27\n91\n93\n\"Atlético Madrid\"\n[\"GK\"]\n188\n87\n\"Right\"\n91\n\"GK\"\n2014-07-16\nnull\n120000000\n125000\n159399993\n95\n13\n11\n15\n43\n13\n109\n12\n13\n14\n40\n30\n307\n43\n60\n67\n…\n78\n12\n140\n34\n19\n11\n65\n11\n68\n57\n27\n12\n18\n437\n87\n92\n78\n90\n90\n1413\n489\n3\n1\n\"Medium\"\n\"Medium\"\n3\n87\n92\n78\n90\n52\n90\n150\n2014\n2023\nfalse\nnull\n\n\n192985\n\"K. De Bruyne\"\n\"Kevin De Bruyne\"\n\"https://cdn.sofifa.com/players…\n\"http://sofifa.com/player/19298…\n\"Belgium\"\n29\n91\n91\n\"Manchester City\"\n[\"CAM\", \" CM\"]\n181\n70\n\"Right\"\n91\n\"CAM\"\n2015-08-30\nnull\n129000000\n370000\n161000000\n407\n94\n82\n55\n94\n82\n441\n88\n85\n83\n93\n92\n398\n77\n76\n78\n…\n74\n91\n408\n76\n66\n88\n94\n84\n91\n186\n68\n65\n53\n56\n15\n13\n5\n10\n13\n2304\n485\n5\n4\n\"High\"\n\"High\"\n4\n76\n86\n93\n88\n64\n78\n207\n2015\n2023\nfalse\nnull\n\n\n190871\n\"Neymar Jr\"\n\"Neymar da Silva Santos Jr.\"\n\"https://cdn.sofifa.com/players…\n\"http://sofifa.com/player/19087…\n\"Brazil\"\n28\n91\n91\n\"Paris Saint-Germain\"\n[\"LW\", \" CAM\"]\n175\n68\n\"Right\"\n91\n\"LW\"\n2017-08-03\nnull\n132000000\n270000\n166500000\n408\n85\n87\n62\n87\n87\n448\n95\n88\n89\n81\n95\n453\n94\n89\n96\n…\n50\n84\n356\n51\n36\n87\n90\n92\n93\n94\n35\n30\n29\n59\n9\n9\n15\n15\n11\n2175\n451\n5\n5\n\"High\"\n\"Medium\"\n5\n91\n85\n86\n94\n36\n59\n595\n2017\n2022\nfalse\nnull\n\n\n\n\n\n\n\n\n\nfifa_pd.head()\n\n\n\n\n\n\n\n\nID\nName\nLongName\nphotoUrl\nplayerUrl\nNationality\nAge\nOVA\nPOT\nClub\n...\nSHO\nPAS\nDRI\nDEF\nPHY\nHits\ncontract_start\ncontract_end\nfree_agent\nloan_date_start\n\n\n\n\n0\n158023\nL. Messi\nLionel Messi\nhttps://cdn.sofifa.com/players/158/023/21_60.png\nhttp://sofifa.com/player/158023/lionel-messi/2...\nArgentina\n33\n93\n93\nFC Barcelona\n...\n92\n91\n95\n38\n65\n771\n2004\n2021\nFalse\nNaT\n\n\n1\n20801\nCristiano Ronaldo\nC. Ronaldo dos Santos Aveiro\nhttps://cdn.sofifa.com/players/020/801/21_60.png\nhttp://sofifa.com/player/20801/c-ronaldo-dos-s...\nPortugal\n35\n92\n92\nJuventus\n...\n93\n81\n89\n35\n77\n562\n2018\n2022\nFalse\nNaT\n\n\n2\n200389\nJ. Oblak\nJan Oblak\nhttps://cdn.sofifa.com/players/200/389/21_60.png\nhttp://sofifa.com/player/200389/jan-oblak/210006/\nSlovenia\n27\n91\n93\nAtlético Madrid\n...\n92\n78\n90\n52\n90\n150\n2014\n2023\nFalse\nNaT\n\n\n3\n192985\nK. De Bruyne\nKevin De Bruyne\nhttps://cdn.sofifa.com/players/192/985/21_60.png\nhttp://sofifa.com/player/192985/kevin-de-bruyn...\nBelgium\n29\n91\n91\nManchester City\n...\n86\n93\n88\n64\n78\n207\n2015\n2023\nFalse\nNaT\n\n\n4\n190871\nNeymar Jr\nNeymar da Silva Santos Jr.\nhttps://cdn.sofifa.com/players/190/871/21_60.png\nhttp://sofifa.com/player/190871/neymar-da-silv...\nBrazil\n28\n91\n91\nParis Saint-Germain\n...\n85\n86\n94\n36\n59\n595\n2017\n2022\nFalse\nNaT\n\n\n\n\n5 rows × 80 columns\n\n\n\n\n\n\nYou could play around with the timings here and even try the .profile method to see what Polars spends its time on. In this scenario the speed advantage of Polars likely comes down to three things:\n\nIt is much faster at reading CSVs.\nIt is much faster at processing strings.\nIt can select/assign columns in parallel.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#numpy-might-make-polars-faster-sometimes",
    "href": "performance.html#numpy-might-make-polars-faster-sometimes",
    "title": "3  Performance",
    "section": "3.3 NumPy might make Polars faster sometimes",
    "text": "3.3 NumPy might make Polars faster sometimes\nPolars gets along well with NumPy ufuncs, even in lazy mode (which is interesting because NumPy has no lazy API). Let’s see how this looks by calculating the great-circle distance between a bunch of coordinates.\n\n3.3.1 Get the data\nWe create a lazy dataframe containing pairs of airports and their coordinates:\n\nairports = pl.scan_csv(\"../data/airports.csv\").drop_nulls().unique(subset=[\"AIRPORT\"])\npairs = airports.join(airports, on=\"AIRPORT\", how=\"cross\").filter(\n    (pl.col(\"AIRPORT\") != pl.col(\"AIRPORT_right\"))\n    & (pl.col(\"LATITUDE\") != pl.col(\"LATITUDE_right\"))\n    & (pl.col(\"LONGITUDE\") != pl.col(\"LONGITUDE_right\"))\n)\n\n\n\n3.3.2 Calculate great-circle distance\nOne use case for NumPy ufuncs is doing computations that Polars expressions don’t support. In this example Polars can do everything we need, though the ufunc version ends up being slightly faster:\n\nPolarsNumPy\n\n\n\ndef deg2rad_pl(degrees: pl.Expr) -&gt; pl.Expr:\n    return degrees * math.pi / 180\n\ndef gcd_pl(lat1: pl.Expr, lng1: pl.Expr, lat2: pl.Expr, lng2: pl.Expr):\n    ϕ1 = deg2rad_pl(90 - lat1)\n    ϕ2 = deg2rad_pl(90 - lat2)\n\n    θ1 = deg2rad_pl(lng1)\n    θ2 = deg2rad_pl(lng2)\n\n    cos = ϕ1.sin() * ϕ2.sin() * (θ1 - θ2).cos() + ϕ1.cos() * ϕ2.cos()\n    arc = cos.arccos()\n    return arc * 6373\n\n\n\n\ndef gcd_np(lat1, lng1, lat2, lng2):\n    ϕ1 = np.deg2rad(90 - lat1)\n    ϕ2 = np.deg2rad(90 - lat2)\n\n    θ1 = np.deg2rad(lng1)\n    θ2 = np.deg2rad(lng2)\n\n    cos = np.sin(ϕ1) * np.sin(ϕ2) * np.cos(θ1 - θ2) + np.cos(ϕ1) * np.cos(ϕ2)\n    arc = np.arccos(cos)\n    return arc * 6373\n\n\n\n\nWe can pass Polars expressions directly to our gcd_np function, which is pretty nice since these things don’t even store the data themselves:\n\n%%timeit\npairs.select(\n    gcd_np(\n        pl.col(\"LATITUDE\"),\n        pl.col(\"LONGITUDE\"),\n        pl.col(\"LATITUDE_right\"),\n        pl.col(\"LONGITUDE_right\")\n    )\n).collect()\n\n3.22 s ± 87.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nOn my machine the NumPy version used to be 5-20% faster than the pure Polars version, but this is no longer the case. Still you may want to see if it helps you:\n\n%%timeit\npairs.select(\n    gcd_pl(\n        pl.col(\"LATITUDE\"),\n        pl.col(\"LONGITUDE\"),\n        pl.col(\"LATITUDE_right\"),\n        pl.col(\"LONGITUDE_right\")\n    )\n).collect()\n\n4.11 s ± 51.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nThis may not be a huge performance difference, but it at least means you don’t sacrifice speed when relying on NumPy. There are some gotchas though so watch out for those.\nAlso watch out for .to_numpy() - you don’t always need to call this and it can slow things down:\n\n%%timeit\ncollected = pairs.collect()\ngcd_np(\n    collected[\"LATITUDE\"].to_numpy(),\n    collected[\"LONGITUDE\"].to_numpy(),\n    collected[\"LATITUDE_right\"].to_numpy(),\n    collected[\"LONGITUDE_right\"].to_numpy()\n)\n\n4.33 s ± 16 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#polars-can-be-slower-than-pandas-sometimes-maybe",
    "href": "performance.html#polars-can-be-slower-than-pandas-sometimes-maybe",
    "title": "3  Performance",
    "section": "3.4 Polars can be slower than Pandas sometimes, maybe",
    "text": "3.4 Polars can be slower than Pandas sometimes, maybe\nHere’s an example where we calculate z-scores, using window functions in Polars and using groupby-transform in Pandas:\n\ndef create_frame(n, n_groups):\n    return pl.DataFrame(\n        {\"name\": np.random.randint(0, n_groups, size=n), \"value2\": np.random.randn(n)}\n    )\n\ndef pandas_transform(df: pd.DataFrame) -&gt; pd.DataFrame:\n    g = df.groupby(\"name\")[\"value2\"]\n    v = df[\"value2\"]\n    return (v - g.transform(\"mean\")) / g.transform(\"std\")\n\n\ndef polars_transform() -&gt; pl.Expr:\n    v = pl.col(\"value2\")\n    return (v - v.mean().over(\"name\")) / v.std().over(\"name\")\n\nrand_df_pl = create_frame(50_000_000, 50_000)\nrand_df_pd = rand_df_pl.to_pandas()\n\nThe Polars version tends to be 10-100% slower on my machine:\n\nPolarsPandas\n\n\n\n%timeit rand_df_pl.select(polars_transform())\n\n1.86 s ± 9.81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\n\n%timeit pandas_transform(rand_df_pd)\n\n1.51 s ± 17.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n\n\nThis example isn’t telling you to use Pandas in this specific situation. Once you add in the time spent reading a file, Polars likely wins.\nAnd even here, if you sort by the name col, Polars wins again. It has fast-track algorithms for sorted data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#summary",
    "href": "performance.html#summary",
    "title": "3  Performance",
    "section": "3.5 Summary",
    "text": "3.5 Summary\n\nPolars is really fast. Pandas was already respectably fast and Polars wipes the floor with it.\nYou can still make Polars slow if you do silly things with it, but compared to Pandas it’s easier to do the right thing in the first place.\nPolars works well with NumPy ufuncs.\nThere are still some situations where Pandas can be faster. They are probably not compelling, but we shouldn’t pretend they don’t exist.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "tidy.html",
    "href": "tidy.html",
    "title": "4  Reshaping and Tidy Data",
    "section": "",
    "text": "4.1 Get the data\nfrom pathlib import Path\nimport polars as pl\nimport pandas as pd\n\npl.Config.set_tbl_rows(5)\npd.options.display.max_rows = 5\n\nnba_dir = Path(\"../data/nba/\")\n\ncolumn_names = {\n    \"Date\": \"date\",\n    \"Visitor/Neutral\": \"away_team\",\n    \"PTS\": \"away_points\",\n    \"Home/Neutral\": \"home_team\",\n    \"PTS.1\": \"home_points\",\n}\n\nif not nba_dir.exists():\n    nba_dir.mkdir()\n    for month in (\n        \"october\",\n        \"november\",\n        \"december\",\n        \"january\",\n        \"february\",\n        \"march\",\n        \"april\",\n        \"may\",\n        \"june\",\n    ):\n        # In practice we would do more data cleaning here, and save to parquet not CSV.\n        # But we save messy data here so we can clean it later for pedagogical purposes.\n        url = f\"http://www.basketball-reference.com/leagues/NBA_2016_games-{month}.html\"\n        tables = pd.read_html(url)\n        raw = (\n            pl.from_pandas(tables[0].query(\"Date != 'Playoffs'\"))\n            .rename(column_names)\n            .select(column_names.values())\n        )\n        raw.write_csv(nba_dir / f\"{month}.csv\")\n\nnba_glob = nba_dir / \"*.csv\"\npl.scan_csv(nba_glob).head().collect()\n\n\nshape: (5, 5)\n\n\n\ndate\naway_team\naway_points\nhome_team\nhome_points\n\n\nstr\nstr\ni64\nstr\ni64\n\n\n\n\n\"Fri, Apr 1, 2016\"\n\"Philadelphia 76ers\"\n91\n\"Charlotte Hornets\"\n100\n\n\n\"Fri, Apr 1, 2016\"\n\"Dallas Mavericks\"\n98\n\"Detroit Pistons\"\n89\n\n\n\"Fri, Apr 1, 2016\"\n\"Brooklyn Nets\"\n91\n\"New York Knicks\"\n105\n\n\n\"Fri, Apr 1, 2016\"\n\"Cleveland Cavaliers\"\n110\n\"Atlanta Hawks\"\n108\n\n\n\"Fri, Apr 1, 2016\"\n\"Toronto Raptors\"\n99\n\"Memphis Grizzlies\"\n95",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reshaping and Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy.html#cleaning",
    "href": "tidy.html#cleaning",
    "title": "4  Reshaping and Tidy Data",
    "section": "4.2 Cleaning 🧹",
    "text": "4.2 Cleaning 🧹\nNothing super interesting here:\n\nPolarsPandas\n\n\n\ngames_pl = (\n    pl.scan_csv(nba_glob)\n    .with_columns(\n        pl.col(\"date\").str.strptime(pl.Date, \"%a, %b %d, %Y\"),\n    )\n    .sort(\"date\")\n    .with_row_index(\"game_id\")\n)\ngames_pl.head().collect()\n\n\nshape: (5, 6)\n\n\n\ngame_id\ndate\naway_team\naway_points\nhome_team\nhome_points\n\n\nu32\ndate\nstr\ni64\nstr\ni64\n\n\n\n\n0\n2015-10-27\n\"Cleveland Cavaliers\"\n95\n\"Chicago Bulls\"\n97\n\n\n1\n2015-10-27\n\"Detroit Pistons\"\n106\n\"Atlanta Hawks\"\n94\n\n\n2\n2015-10-27\n\"New Orleans Pelicans\"\n95\n\"Golden State Warriors\"\n111\n\n\n3\n2015-10-28\n\"Washington Wizards\"\n88\n\"Orlando Magic\"\n87\n\n\n4\n2015-10-28\n\"Philadelphia 76ers\"\n95\n\"Boston Celtics\"\n112\n\n\n\n\n\n\n\n\n\ngames_pd = (\n    pl.read_csv(nba_glob)\n    .to_pandas()\n    .dropna(how=\"all\")\n    .assign(date=lambda x: pd.to_datetime(x[\"date\"], format=\"%a, %b %d, %Y\"))\n    .sort_values(\"date\")\n    .reset_index(drop=True)\n    .set_index(\"date\", append=True)\n    .rename_axis([\"game_id\", \"date\"])\n    .sort_index()\n)\ngames_pd.head()\n\n\n\n\n\n\n\n\n\naway_team\naway_points\nhome_team\nhome_points\n\n\ngame_id\ndate\n\n\n\n\n\n\n\n\n0\n2015-10-27\nCleveland Cavaliers\n95\nChicago Bulls\n97\n\n\n1\n2015-10-27\nDetroit Pistons\n106\nAtlanta Hawks\n94\n\n\n2\n2015-10-27\nNew Orleans Pelicans\n95\nGolden State Warriors\n111\n\n\n3\n2015-10-28\nPhiladelphia 76ers\n95\nBoston Celtics\n112\n\n\n4\n2015-10-28\nWashington Wizards\n88\nOrlando Magic\n87\n\n\n\n\n\n\n\n\n\n\nPolars does have a drop_nulls method but the only parameter it takes is subset, which — like in Pandas — lets you consider null values just for a subset of the columns. Pandas additionally lets you specify how=\"all\" to drop a row only if every value is null, but Polars drop_nulls has no such parameter and will drop the row if any values are null. If you only want to drop when all values are null, the docs recommend .filter(~pl.all(pl.all().is_null())).\n\n\n\n\n\n\nNote\n\n\n\nA previous version of the Polars example used pl.fold, which is for fast horizontal operations. It doesn’t come up anywhere else in this book, so consider this your warning that it exists.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reshaping and Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy.html#pivot-and-melt",
    "href": "tidy.html#pivot-and-melt",
    "title": "4  Reshaping and Tidy Data",
    "section": "4.3 Pivot and Melt",
    "text": "4.3 Pivot and Melt\nI recently came across someone who was doing advanced quantitative research in Python but had never heard of the Pandas .pivot method. I shudder to imagine the code he must have written in the absence of this knowledge, so here’s a simple explanation of pivoting and melting, lest anyone else suffer in ignorance. If you already know what pivot and melt are, feel free to scroll past this bit.\n\n4.3.1 Pivot\nSuppose you have a dataframe that looks like this:\n\n\nCode\nfrom datetime import date\nprices = pl.DataFrame({\n    \"date\": [*[date(2020, 1, 1)]*4, *[date(2020, 1, 2)]*4, *[date(2020, 1, 3)]*4],\n    \"ticker\": [*[\"AAPL\", \"TSLA\", \"MSFT\", \"NFLX\"]*3],\n    \"price\": [100, 200, 300, 400, 110, 220, 330, 420, 105, 210, 315, 440],\n})\nprices\n\n\n\nshape: (12, 3)\n\n\n\ndate\nticker\nprice\n\n\ndate\nstr\ni64\n\n\n\n\n2020-01-01\n\"AAPL\"\n100\n\n\n2020-01-01\n\"TSLA\"\n200\n\n\n2020-01-01\n\"MSFT\"\n300\n\n\n…\n…\n…\n\n\n2020-01-03\n\"MSFT\"\n315\n\n\n2020-01-03\n\"NFLX\"\n440\n\n\n\n\n\n\nIn both Polars and Pandas you can call df.pivot to get a dataframe that looks like this:\n\n\nCode\npivoted = prices.pivot(index=\"date\", values=\"price\", on=\"ticker\")\npivoted\n\n\n\nshape: (3, 5)\n\n\n\ndate\nAAPL\nTSLA\nMSFT\nNFLX\n\n\ndate\ni64\ni64\ni64\ni64\n\n\n\n\n2020-01-01\n100\n200\n300\n400\n\n\n2020-01-02\n110\n220\n330\n420\n\n\n2020-01-03\n105\n210\n315\n440\n\n\n\n\n\n\nAs you can see, .pivot creates a dataframe where the columns are the unique labels from one column (“ticker”), alongside the index column (“date”). The values for the non-index columns are taken from the corresponding rows of the values column (“price”).\nIf our dataframe had multiple prices for the same ticker on the same date, we would use the aggregate_fn parameter of the .pivot method, e.g.: prices.pivot(..., aggregate_fn=\"mean\"). Pivoting with an aggregate function gives us similar behaviour to what Excel calls “pivot tables”.\n\n\n4.3.2 Melt / Unpivot\nMelt is the inverse of pivot. While pivot takes us from long data to wide data, melt goes from wide to long. Note: Polars has recently replaced its melt method with an unpivot method.\nIf we call .unpivot(index=\"date\", value_name=\"price\") on our pivoted dataframe we get our original dataframe back:\n\n\nCode\npivoted.unpivot(index=\"date\", value_name=\"price\")\n\n\n\nshape: (12, 3)\n\n\n\ndate\nvariable\nprice\n\n\ndate\nstr\ni64\n\n\n\n\n2020-01-01\n\"AAPL\"\n100\n\n\n2020-01-02\n\"AAPL\"\n110\n\n\n2020-01-03\n\"AAPL\"\n105\n\n\n…\n…\n…\n\n\n2020-01-02\n\"NFLX\"\n420\n\n\n2020-01-03\n\"NFLX\"\n440",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reshaping and Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy.html#tidy-nba-data",
    "href": "tidy.html#tidy-nba-data",
    "title": "4  Reshaping and Tidy Data",
    "section": "4.4 Tidy NBA data",
    "text": "4.4 Tidy NBA data\nSuppose we want to calculate the days of rest each team had before each game. In the current structure this is difficult because we need to track both the home_team and away_team columns. We’ll use .unpivot so that there’s a single team column. This makes it easier to add a rest column with the per-team rest days between games.\n\nPolarsPandas\n\n\n\ntidy_pl = (\n    games_pl\n    .unpivot(\n        index=[\"game_id\", \"date\"],\n        on=[\"away_team\", \"home_team\"],\n        value_name=\"team\",\n    )\n    .sort(\"game_id\")\n    .with_columns((\n        pl.col(\"date\")\n        .alias(\"rest\")\n        .diff().over(\"team\")\n        .dt.total_days() - 1).cast(pl.Int8))\n    .drop_nulls(\"rest\")\n    .collect()\n)\ntidy_pl\n\n\nshape: (2_602, 5)\n\n\n\ngame_id\ndate\nvariable\nteam\nrest\n\n\nu32\ndate\nstr\nstr\ni8\n\n\n\n\n5\n2015-10-28\n\"away_team\"\n\"Chicago Bulls\"\n0\n\n\n6\n2015-10-28\n\"home_team\"\n\"Detroit Pistons\"\n0\n\n\n11\n2015-10-28\n\"away_team\"\n\"Cleveland Cavaliers\"\n0\n\n\n…\n…\n…\n…\n…\n\n\n1315\n2016-06-19\n\"away_team\"\n\"Cleveland Cavaliers\"\n2\n\n\n1315\n2016-06-19\n\"home_team\"\n\"Golden State Warriors\"\n2\n\n\n\n\n\n\n\n\n\ntidy_pd = (\n    games_pd.reset_index()\n    .melt(\n        id_vars=[\"game_id\", \"date\"],\n        value_vars=[\"away_team\", \"home_team\"],\n        value_name=\"team\",\n    )\n    .sort_values(\"game_id\")\n    .assign(\n        rest=lambda df: (\n            df\n            .sort_values(\"date\")\n            .groupby(\"team\")\n            [\"date\"]\n            .diff()\n            .dt.days\n            .sub(1)\n        )\n    )\n    .dropna(subset=[\"rest\"])\n    .astype({\"rest\": pd.Int8Dtype()})\n)\ntidy_pd\n\n\n\n\n\n\n\n\ngame_id\ndate\nvariable\nteam\nrest\n\n\n\n\n7\n7\n2015-10-28\naway_team\nNew Orleans Pelicans\n0\n\n\n11\n11\n2015-10-28\naway_team\nChicago Bulls\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n1315\n1315\n2016-06-19\naway_team\nCleveland Cavaliers\n2\n\n\n2631\n1315\n2016-06-19\nhome_team\nGolden State Warriors\n2\n\n\n\n\n2602 rows × 5 columns\n\n\n\n\n\n\nNow we use .pivot so that this days-of-rest data can be added back to the original dataframe. We’ll also add columns for the spread between the home team’s rest and away team’s rest, and a flag for whether the home team won.\n\nPolarsPandas\n\n\n\nby_game_pl = (\n    tidy_pl\n    .pivot(\n        values=\"rest\",\n        index=[\"game_id\", \"date\"],\n        on=\"variable\"\n    )\n    .rename({\"away_team\": \"away_rest\", \"home_team\": \"home_rest\"})\n)\njoined_pl = (\n    by_game_pl\n    .join(games_pl.collect(), on=[\"game_id\", \"date\"])\n    .with_columns([\n        pl.col(\"home_points\").alias(\"home_win\") &gt; pl.col(\"away_points\"),\n        pl.col(\"home_rest\").alias(\"rest_spread\") - pl.col(\"away_rest\"),\n    ])\n)\njoined_pl\n\n\nshape: (1_303, 10)\n\n\n\ngame_id\ndate\naway_rest\nhome_rest\naway_team\naway_points\nhome_team\nhome_points\nhome_win\nrest_spread\n\n\nu32\ndate\ni8\ni8\nstr\ni64\nstr\ni64\nbool\ni8\n\n\n\n\n5\n2015-10-28\n0\nnull\n\"Chicago Bulls\"\n115\n\"Brooklyn Nets\"\n100\nfalse\nnull\n\n\n6\n2015-10-28\nnull\n0\n\"Utah Jazz\"\n87\n\"Detroit Pistons\"\n92\ntrue\nnull\n\n\n11\n2015-10-28\n0\nnull\n\"Cleveland Cavaliers\"\n106\n\"Memphis Grizzlies\"\n76\nfalse\nnull\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n1314\n2016-06-16\n2\n2\n\"Golden State Warriors\"\n101\n\"Cleveland Cavaliers\"\n115\ntrue\n0\n\n\n1315\n2016-06-19\n2\n2\n\"Cleveland Cavaliers\"\n93\n\"Golden State Warriors\"\n89\nfalse\n0\n\n\n\n\n\n\n\n\n\nby_game_pd = (\n    tidy_pd\n    .pivot(\n        values=\"rest\",\n        index=[\"game_id\", \"date\"],\n        columns=\"variable\"\n    )\n    .rename(\n        columns={\"away_team\": \"away_rest\", \"home_team\": \"home_rest\"}\n    )\n)\njoined_pd = by_game_pd.join(games_pd).assign(\n    home_win=lambda df: df[\"home_points\"] &gt; df[\"away_points\"],\n    rest_spread=lambda df: df[\"home_rest\"] - df[\"away_rest\"],\n)\njoined_pd\n\n\n\n\n\n\n\n\n\naway_rest\nhome_rest\naway_team\naway_points\nhome_team\nhome_points\nhome_win\nrest_spread\n\n\ngame_id\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n7\n2015-10-28\n0\n&lt;NA&gt;\nNew Orleans Pelicans\n94\nPortland Trail Blazers\n112\nTrue\n&lt;NA&gt;\n\n\n11\n2015-10-28\n0\n&lt;NA&gt;\nChicago Bulls\n115\nBrooklyn Nets\n100\nFalse\n&lt;NA&gt;\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1314\n2016-06-16\n2\n2\nGolden State Warriors\n101\nCleveland Cavaliers\n115\nTrue\n0\n\n\n1315\n2016-06-19\n2\n2\nCleveland Cavaliers\n93\nGolden State Warriors\n89\nFalse\n0\n\n\n\n\n1303 rows × 8 columns\n\n\n\n\n\n\nHere’s a lightly edited quote from Modern Pandas:\n\nOne somewhat subtle point: an “observation” depends on the question being asked. So really, we have two tidy datasets, tidy for answering team-level questions, and joined for answering game-level questions.\n\nLet’s use the team-level dataframe to see each team’s average days of rest, both at home and away:\n\nimport seaborn as sns\nsns.set_theme(font_scale=0.6)\nsns.catplot(\n    tidy_pl,\n    x=\"variable\",\n    y=\"rest\",\n    col=\"team\",\n    col_wrap=5,\n    kind=\"bar\",\n    height=1.5,\n)\n\n\n\n\n\n\n\n\nPlotting the distribution of rest_spread:\n\nPolarsPandas\n\n\n\nimport numpy as np\ndelta_pl = joined_pl[\"rest_spread\"]\nax = (\n    delta_pl\n    .value_counts()\n    .drop_nulls()\n    .to_pandas()\n    .set_index(\"rest_spread\")\n    [\"count\"]\n    .reindex(np.arange(delta_pl.min(), delta_pl.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind=\"bar\", color=\"k\", width=0.9, rot=0, figsize=(9, 6))\n)\nax.set(xlabel=\"Difference in Rest (Home - Away)\", ylabel=\"Games\")\n\n\n\n\n\n\n\n\n\n\n\ndelta_pd = joined_pd[\"rest_spread\"]\nax = (\n    delta_pd\n    .value_counts()\n    .reindex(np.arange(delta_pd.min(), delta_pd.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind=\"bar\", color=\"k\", width=0.9, rot=0, figsize=(9, 6))\n)\nax.set(xlabel=\"Difference in Rest (Home - Away)\", ylabel=\"Games\")\n\n\n\n\n\n\n\n\n\n\n\nPlotting the win percent by rest_spread:\n\nPolarsPandas\n\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(9, 6))\nsns.barplot(\n    x=\"rest_spread\",\n    y=\"home_win\",\n    data=joined_pl.filter(pl.col(\"rest_spread\").is_between(-3, 3, closed=\"both\")),\n    color=\"#4c72b0\",\n    ax=ax,\n)\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(9, 6))\nsns.barplot(\n    x=\"rest_spread\",\n    y=\"home_win\",\n    data=joined_pd.query('-3 &lt;= rest_spread &lt;= 3'),\n    color=\"#4c72b0\",\n    ax=ax,\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reshaping and Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy.html#stack-unstack-vs-melt-pivot",
    "href": "tidy.html#stack-unstack-vs-melt-pivot",
    "title": "4  Reshaping and Tidy Data",
    "section": "4.5 Stack / Unstack vs Melt / Pivot",
    "text": "4.5 Stack / Unstack vs Melt / Pivot\nPandas has special methods stack and unstack for reshaping data with a MultiIndex. Polars doesn’t have an index, so anywhere you see stack / unstack in Pandas, the equivalent Polars code will use melt / pivot.\n\nPolarsPandas\n\n\n\nrest_pl = (\n    tidy_pl\n    .group_by([\"date\", \"variable\"], maintain_order=True)\n    .agg(pl.col(\"rest\").mean())\n)\nrest_pl\n\n\nshape: (418, 3)\n\n\n\ndate\nvariable\nrest\n\n\ndate\nstr\nf64\n\n\n\n\n2015-10-28\n\"away_team\"\n0.0\n\n\n2015-10-28\n\"home_team\"\n0.0\n\n\n2015-10-29\n\"away_team\"\n0.333333\n\n\n…\n…\n…\n\n\n2016-06-19\n\"away_team\"\n2.0\n\n\n2016-06-19\n\"home_team\"\n2.0\n\n\n\n\n\n\n\n\n\nrest_pd = (\n    tidy_pd\n    .groupby([\"date\", \"variable\"])\n    [\"rest\"]\n    .mean()\n)\nrest_pd\n\ndate        variable \n2015-10-28  away_team    0.0\n            home_team    0.0\n                        ... \n2016-06-19  away_team    2.0\n            home_team    2.0\nName: rest, Length: 418, dtype: Float64\n\n\n\n\n\nIn Polars we use .pivot to do what in Pandas would require .unstack:\n\nPolarsPandas\n\n\n\nrest_pl.pivot(index=\"date\", on=\"variable\", values=\"rest\")\n\n\nshape: (209, 3)\n\n\n\ndate\naway_team\nhome_team\n\n\ndate\nf64\nf64\n\n\n\n\n2015-10-28\n0.0\n0.0\n\n\n2015-10-29\n0.333333\n0.0\n\n\n2015-10-30\n1.083333\n0.916667\n\n\n…\n…\n…\n\n\n2016-06-16\n2.0\n2.0\n\n\n2016-06-19\n2.0\n2.0\n\n\n\n\n\n\n\n\n\nrest_pd.unstack()\n\n\n\n\n\n\n\nvariable\naway_team\nhome_team\n\n\ndate\n\n\n\n\n\n\n2015-10-28\n0.0\n0.0\n\n\n2015-10-29\n0.333333\n0.0\n\n\n...\n...\n...\n\n\n2016-06-16\n2.0\n2.0\n\n\n2016-06-19\n2.0\n2.0\n\n\n\n\n209 rows × 2 columns\n\n\n\n\n\n\nPlotting the moving average of rest days:\n\nPolarsPandas\n\n\n\nax = (\n    rest_pl.pivot(index=\"date\", values=\"rest\", on=\"variable\")\n    .filter(pl.col(\"away_team\") &lt; 7)\n    .sort(\"date\")\n    .select([pl.col(\"date\"), pl.col(pl.Float64).rolling_mean(7)])\n    .to_pandas()\n    .set_index(\"date\")\n    .plot(figsize=(9, 6), linewidth=3)\n)\nax.set(ylabel=\"Rest (7 day MA)\")\n\n\n\n\n\n\n\n\n\n\n\nax = (\n    rest_pd.unstack()\n    .query('away_team &lt; 7')\n    .sort_index()\n    .rolling(7)\n    .mean()\n    .plot(figsize=(9, 6), linewidth=3)\n)\nax.set(ylabel=\"Rest (7 day MA)\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reshaping and Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy.html#mini-project-home-court-advantage",
    "href": "tidy.html#mini-project-home-court-advantage",
    "title": "4  Reshaping and Tidy Data",
    "section": "4.6 Mini Project: Home Court Advantage?",
    "text": "4.6 Mini Project: Home Court Advantage?\nWe may as well do some (not very rigorous) analysis: let’s see if home advantage is a real thing.\n\n4.6.1 Find the win percent for each team\nWe want to control for the strength of the teams playing. The team’s victory percentage is probably not a good control but it’s what we’ll use:\n\nPolarsPandas\n\n\n\nwin_col = pl.col(\"win\")\nwins_pl = (\n    joined_pl.unpivot(\n        index=[\"game_id\", \"date\", \"home_win\"],\n        value_name=\"team\",\n        variable_name=\"is_home\",\n        on=[\"home_team\", \"away_team\"],\n    )\n    .with_columns(pl.col(\"home_win\").alias(\"win\") == (pl.col(\"is_home\") == \"home_team\"))\n    .group_by([\"team\", \"is_home\"])\n    .agg(\n        [\n            win_col.sum().alias(\"n_wins\"),\n            win_col.count().alias(\"n_games\"),\n            win_col.mean().alias(\"win_pct\"),\n        ]\n    )\n    .sort([\"team\", \"is_home\"])\n)\nwins_pl\n\n\nshape: (60, 5)\n\n\n\nteam\nis_home\nn_wins\nn_games\nwin_pct\n\n\nstr\nstr\nu32\nu32\nf64\n\n\n\n\n\"Atlanta Hawks\"\n\"away_team\"\n22\n46\n0.478261\n\n\n\"Atlanta Hawks\"\n\"home_team\"\n30\n45\n0.666667\n\n\n\"Boston Celtics\"\n\"away_team\"\n20\n44\n0.454545\n\n\n…\n…\n…\n…\n…\n\n\n\"Washington Wizards\"\n\"away_team\"\n18\n40\n0.45\n\n\n\"Washington Wizards\"\n\"home_team\"\n22\n41\n0.536585\n\n\n\n\n\n\n\n\n\nwins_pd = (\n    joined_pd\n    .reset_index()\n    .melt(\n        id_vars=[\"game_id\", \"date\", \"home_win\"],\n        value_name=\"team\",\n        var_name=\"is_home\",\n        value_vars=[\"home_team\", \"away_team\"],\n    )\n    .assign(win=lambda df: df[\"home_win\"] == (df[\"is_home\"] == \"home_team\"))\n    .groupby([\"team\", \"is_home\"])[\"win\"]\n    .agg(['sum', 'count', 'mean'])\n    .rename(columns={\n        \"sum\": 'n_wins',\n        \"count\": 'n_games',\n        \"mean\": 'win_pct'\n    })\n)\nwins_pd\n\n\n\n\n\n\n\n\n\nn_wins\nn_games\nwin_pct\n\n\nteam\nis_home\n\n\n\n\n\n\n\nAtlanta Hawks\naway_team\n22\n46\n0.478261\n\n\nhome_team\n30\n45\n0.666667\n\n\n...\n...\n...\n...\n...\n\n\nWashington Wizards\naway_team\n18\n40\n0.450000\n\n\nhome_team\n22\n41\n0.536585\n\n\n\n\n60 rows × 3 columns",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reshaping and Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy.html#some-visualisations",
    "href": "tidy.html#some-visualisations",
    "title": "4  Reshaping and Tidy Data",
    "section": "4.7 Some visualisations",
    "text": "4.7 Some visualisations\n\ng = sns.FacetGrid(wins_pl, hue=\"team\", aspect=0.8, palette=[\"k\"], height=5)\ng.map(\n    sns.pointplot,\n    \"is_home\",\n    \"win_pct\",\n    order=[\"away_team\", \"home_team\"]).set(ylim=(0, 1))\n\n\n\n\n\n\n\n\n\nsns.catplot(\n    wins_pl,\n    x=\"is_home\",\n    y=\"win_pct\",\n    col=\"team\",\n    col_wrap=5,\n    hue=\"team\",\n    kind=\"point\",\n    height=1.5,\n)\n\n\n\n\n\n\n\n\nNow we calculate the win percent by team, regardless of whether they’re home or away:\n\nPolarsPandas\n\n\n\nwin_percent_pl = (\n    wins_pl.group_by(\"team\", maintain_order=True).agg(\n        pl.col(\"n_wins\").sum().alias(\"win_pct\") / pl.col(\"n_games\").sum()\n    )\n)\nwin_percent_pl\n\n\nshape: (30, 2)\n\n\n\nteam\nwin_pct\n\n\nstr\nf64\n\n\n\n\n\"Atlanta Hawks\"\n0.571429\n\n\n\"Boston Celtics\"\n0.563218\n\n\n\"Brooklyn Nets\"\n0.256098\n\n\n…\n…\n\n\n\"Utah Jazz\"\n0.487805\n\n\n\"Washington Wizards\"\n0.493827\n\n\n\n\n\n\n\n\n\nwin_percent_pd = (\n    wins_pd\n    .groupby(level=\"team\", as_index=True)\n    .apply(lambda x: x[\"n_wins\"].sum() / x[\"n_games\"].sum())\n)\nwin_percent_pd\n\nteam\nAtlanta Hawks         0.571429\nBoston Celtics        0.563218\n                        ...   \nUtah Jazz             0.487805\nWashington Wizards    0.493827\nLength: 30, dtype: float64\n\n\n\n\n\n\n(\n    win_percent_pl\n    .sort(\"win_pct\")\n    .to_pandas()\n    .set_index(\"team\")\n    .plot.barh(figsize=(6, 12), width=0.85, color=\"k\")\n)\nplt.xlabel(\"Win Percent\")\n\nText(0.5, 0, 'Win Percent')\n\n\n\n\n\n\n\n\n\nHere’s a plot of team home court advantage against team overall win percentage:\n\nPolarsPandas\n\n\n\nwins_to_plot_pl = (\n    wins_pl.pivot(index=\"team\", on=\"is_home\", values=\"win_pct\")\n    .with_columns(\n        [\n            pl.col(\"home_team\").alias(\"Home Win % - Away %\") - pl.col(\"away_team\"),\n            (pl.col(\"home_team\").alias(\"Overall %\") + pl.col(\"away_team\")) / 2,\n        ]\n    )\n)\nsns.regplot(data=wins_to_plot_pl, x='Overall %', y='Home Win % - Away %')\n\n\n\n\n\n\n\n\n\n\n\nwins_to_plot_pd = (\n    wins_pd\n    [\"win_pct\"]\n    .unstack()\n    .assign(**{'Home Win % - Away %': lambda x: x[\"home_team\"] - x[\"away_team\"],\n               'Overall %': lambda x: (x[\"home_team\"] + x[\"away_team\"]) / 2})\n)\nsns.regplot(data=wins_to_plot_pd, x='Overall %', y='Home Win % - Away %')\n\n\n\n\n\n\n\n\n\n\n\nLet’s add the win percent back to the dataframe and run a regression:\n\nPolarsPandas\n\n\n\nreg_df_pl = (\n    joined_pl.join(win_percent_pl, left_on=\"home_team\", right_on=\"team\")\n    .rename({\"win_pct\": \"home_strength\"})\n    .join(win_percent_pl, left_on=\"away_team\", right_on=\"team\")\n    .rename({\"win_pct\": \"away_strength\"})\n    .with_columns(\n        [\n            pl.col(\"home_points\").alias(\"point_diff\") - pl.col(\"away_points\"),\n            pl.col(\"home_rest\").alias(\"rest_diff\") - pl.col(\"away_rest\"),\n            pl.col(\"home_win\").cast(pl.UInt8),  # for statsmodels\n        ]\n    )\n)\nreg_df_pl.head()\n\n\nshape: (5, 14)\n\n\n\ngame_id\ndate\naway_rest\nhome_rest\naway_team\naway_points\nhome_team\nhome_points\nhome_win\nrest_spread\nhome_strength\naway_strength\npoint_diff\nrest_diff\n\n\nu32\ndate\ni8\ni8\nstr\ni64\nstr\ni64\nu8\ni8\nf64\nf64\ni64\ni8\n\n\n\n\n5\n2015-10-28\n0\nnull\n\"Chicago Bulls\"\n115\n\"Brooklyn Nets\"\n100\n0\nnull\n0.256098\n0.506173\n-15\nnull\n\n\n6\n2015-10-28\nnull\n0\n\"Utah Jazz\"\n87\n\"Detroit Pistons\"\n92\n1\nnull\n0.505882\n0.487805\n5\nnull\n\n\n11\n2015-10-28\n0\nnull\n\"Cleveland Cavaliers\"\n106\n\"Memphis Grizzlies\"\n76\n0\nnull\n0.488372\n0.715686\n-30\nnull\n\n\n14\n2015-10-28\n0\nnull\n\"New Orleans Pelicans\"\n94\n\"Portland Trail Blazers\"\n112\n1\nnull\n0.526882\n0.37037\n18\nnull\n\n\n17\n2015-10-29\n0\n0\n\"Memphis Grizzlies\"\n112\n\"Indiana Pacers\"\n103\n0\n0\n0.545455\n0.488372\n-9\n0\n\n\n\n\n\n\n\n\n\nreg_df_pd = (\n    joined_pd.assign(\n        away_strength=joined_pd['away_team'].map(win_percent_pd),\n        home_strength=joined_pd['home_team'].map(win_percent_pd),\n        point_diff=joined_pd['home_points'] - joined_pd['away_points'],\n        rest_diff=joined_pd['home_rest'] - joined_pd['away_rest'])\n)\nreg_df_pd.head()\n\n\n\n\n\n\n\n\n\naway_rest\nhome_rest\naway_team\naway_points\nhome_team\nhome_points\nhome_win\nrest_spread\naway_strength\nhome_strength\npoint_diff\nrest_diff\n\n\ngame_id\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n2015-10-28\n0\n&lt;NA&gt;\nNew Orleans Pelicans\n94\nPortland Trail Blazers\n112\nTrue\n&lt;NA&gt;\n0.370370\n0.526882\n18\n&lt;NA&gt;\n\n\n11\n2015-10-28\n0\n&lt;NA&gt;\nChicago Bulls\n115\nBrooklyn Nets\n100\nFalse\n&lt;NA&gt;\n0.506173\n0.256098\n-15\n&lt;NA&gt;\n\n\n15\n2015-10-28\n&lt;NA&gt;\n0\nUtah Jazz\n87\nDetroit Pistons\n92\nTrue\n&lt;NA&gt;\n0.487805\n0.505882\n5\n&lt;NA&gt;\n\n\n16\n2015-10-28\n0\n&lt;NA&gt;\nCleveland Cavaliers\n106\nMemphis Grizzlies\n76\nFalse\n&lt;NA&gt;\n0.715686\n0.488372\n-30\n&lt;NA&gt;\n\n\n17\n2015-10-29\n1\n0\nAtlanta Hawks\n112\nNew York Knicks\n101\nFalse\n-1\n0.571429\n0.382716\n-11\n-1\n\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.formula.api as sm\n\nmod = sm.logit(\n    \"home_win ~ home_strength + away_strength + home_rest + away_rest\",\n    reg_df_pl.to_pandas(),\n)\nres = mod.fit()\nres.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.554797\n         Iterations 6\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nhome_win\nNo. Observations:\n1299\n\n\nModel:\nLogit\nDf Residuals:\n1294\n\n\nMethod:\nMLE\nDf Model:\n4\n\n\nDate:\nSun, 07 Jul 2024\nPseudo R-squ.:\n0.1777\n\n\nTime:\n17:11:07\nLog-Likelihood:\n-720.68\n\n\nconverged:\nTrue\nLL-Null:\n-876.38\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n3.748e-66\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.0019\n0.304\n-0.006\n0.995\n-0.597\n0.593\n\n\nhome_strength\n5.7161\n0.466\n12.272\n0.000\n4.803\n6.629\n\n\naway_strength\n-4.9133\n0.456\n-10.786\n0.000\n-5.806\n-4.020\n\n\nhome_rest\n0.1045\n0.076\n1.381\n0.167\n-0.044\n0.253\n\n\naway_rest\n-0.0347\n0.066\n-0.526\n0.599\n-0.164\n0.095\n\n\n\n\n\nYou can play around with the regressions yourself but we’ll end them here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reshaping and Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy.html#summary",
    "href": "tidy.html#summary",
    "title": "4  Reshaping and Tidy Data",
    "section": "4.8 Summary",
    "text": "4.8 Summary\nThis was mostly a demonstration of .pivot and .melt, with several different examples of reshaping data in Polars and Pandas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reshaping and Tidy Data</span>"
    ]
  },
  {
    "objectID": "timeseries.html",
    "href": "timeseries.html",
    "title": "5  Timeseries",
    "section": "",
    "text": "5.1 Get the data\nWe’ll download a year’s worth of daily price and volume data for Bitcoin:\nfrom pathlib import Path\nfrom io import StringIO\nfrom datetime import datetime, date\nimport requests\nimport polars as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npl.Config.set_tbl_rows(5)\npd.options.display.max_rows = 5\ndata_path = Path(\"../data/ohlcv.pq\")\n\n\ndef epoch_ms(dt: datetime) -&gt; int:\n    return int(dt.timestamp()) * 1000\n\n\nif data_path.exists():\n    ohlcv_pl = pl.read_parquet(data_path).set_sorted(\"time\")\n\nelse:\n    start = epoch_ms(datetime(2021, 1, 1))\n    end = epoch_ms(datetime(2022, 1, 1))\n    url = (\n        \"https://api.binance.com/api/v3/klines?symbol=BTCUSDT&\"\n        f\"interval=1d&startTime={start}&endTime={end}\"\n    )\n    resp = requests.get(url)\n    time_col = \"time\"\n    ohlcv_cols = [\n        \"open\",\n        \"high\",\n        \"low\",\n        \"close\",\n        \"volume\",\n    ]\n    cols_to_use = [time_col, *ohlcv_cols] \n    cols = cols_to_use + [f\"ignore_{i}\" for i in range(6)]\n    ohlcv_pl = pl.from_records(resp.json(), orient=\"row\", schema=cols).select(\n        [\n            pl.col(time_col).cast(pl.Datetime).dt.with_time_unit(\"ms\").cast(pl.Date),\n            pl.col(ohlcv_cols).cast(pl.Float64),\n        ]\n    ).set_sorted(\"time\")\n    ohlcv_pl.write_parquet(data_path)\n\nohlcv_pd = ohlcv_pl.with_columns(pl.col(\"time\").cast(pl.Datetime)).to_pandas().set_index(\"time\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Timeseries</span>"
    ]
  },
  {
    "objectID": "timeseries.html#filtering",
    "href": "timeseries.html#filtering",
    "title": "5  Timeseries",
    "section": "5.2 Filtering",
    "text": "5.2 Filtering\nPandas has special methods for filtering data with a DatetimeIndex. Since Polars doesn’t have an index, we just use .filter. I will admit the Pandas code is more convenient for things like filtering for a specific month:\n\nPolarsPandas\n\n\n\nohlcv_pl.filter(\n    pl.col(\"time\").is_between(\n        date(2021, 2, 1),\n        date(2021, 3, 1),\n        closed=\"left\"\n    )\n)\n\n\nshape: (28, 6)\n\n\n\ntime\nopen\nhigh\nlow\nclose\nvolume\n\n\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2021-02-01\n33092.97\n34717.27\n32296.16\n33526.37\n82718.276882\n\n\n2021-02-02\n33517.09\n35984.33\n33418.0\n35466.24\n78056.65988\n\n\n2021-02-03\n35472.71\n37662.63\n35362.38\n37618.87\n80784.333663\n\n\n…\n…\n…\n…\n…\n…\n\n\n2021-02-27\n46276.88\n48394.0\n45000.0\n46106.43\n66060.834292\n\n\n2021-02-28\n46103.67\n46638.46\n43000.0\n45135.66\n83055.369042\n\n\n\n\n\n\n\n\n\nohlcv_pd.loc[\"2021-02\"]\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nvolume\n\n\ntime\n\n\n\n\n\n\n\n\n\n2021-02-01\n33092.97\n34717.27\n32296.16\n33526.37\n82718.276882\n\n\n2021-02-02\n33517.09\n35984.33\n33418.00\n35466.24\n78056.659880\n\n\n...\n...\n...\n...\n...\n...\n\n\n2021-02-27\n46276.88\n48394.00\n45000.00\n46106.43\n66060.834292\n\n\n2021-02-28\n46103.67\n46638.46\n43000.00\n45135.66\n83055.369042\n\n\n\n\n28 rows × 5 columns",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Timeseries</span>"
    ]
  },
  {
    "objectID": "timeseries.html#resampling",
    "href": "timeseries.html#resampling",
    "title": "5  Timeseries",
    "section": "5.3 Resampling",
    "text": "5.3 Resampling\nResampling is like a special case of groupby for a time column. You can of course use regular .groupby with a time column, but it won’t be as powerful because it doesn’t understand time like resampling methods do.\nThere are two kinds of resampling: downsampling and upsampling.\n\n5.3.1 Downsampling\nDownsampling moves from a higher time frequency to a lower time frequency. This requires some aggregation or subsetting, since we’re reducing the number of rows in our data.\nIn Polars we use the .groupby_dynamic method for downsampling (we also use groupby_dynamic when we want to combine resampling with regular groupby logic).\n\nPolarsPandas\n\n\n\n(\n    ohlcv_pl\n    .group_by_dynamic(\"time\", every=\"5d\")\n    .agg(pl.col(pl.Float64).mean())\n)\n\n\nshape: (74, 6)\n\n\n\ntime\nopen\nhigh\nlow\nclose\nvolume\n\n\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2020-12-29\n29127.665\n31450.0\n28785.55\n30755.01\n92088.399186\n\n\n2021-01-03\n33577.028\n36008.464\n31916.198\n35027.986\n127574.470245\n\n\n2021-01-08\n38733.61\n39914.548\n34656.422\n37655.352\n143777.954392\n\n\n…\n…\n…\n…\n…\n…\n\n\n2021-12-24\n50707.08\n51407.656\n49540.152\n50048.066\n29607.160572\n\n\n2021-12-29\n46836.5525\n48135.4925\n45970.84\n46881.2775\n31098.406725\n\n\n\n\n\n\n\n\n\nohlcv_pd.resample(\"5d\").mean()\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nvolume\n\n\ntime\n\n\n\n\n\n\n\n\n\n2021-01-01\n31084.316\n33127.622\n29512.818\n32089.662\n112416.849570\n\n\n2021-01-06\n38165.310\n40396.842\n35983.822\n39004.538\n118750.076685\n\n\n...\n...\n...\n...\n...\n...\n\n\n2021-12-27\n48521.240\n49475.878\n47087.400\n47609.528\n35886.943710\n\n\n2022-01-01\n46216.930\n47954.630\n46208.370\n47722.650\n19604.463250\n\n\n\n\n74 rows × 5 columns\n\n\n\n\n\n\nResampling and performing multiple aggregations to each column:\n\nPolarsPandas\n\n\n\n(\n    ohlcv_pl\n    .group_by_dynamic(\"time\", every=\"1w\", start_by=\"friday\")\n    .agg([\n        pl.col(pl.Float64).mean().name.suffix(\"_mean\"),\n        pl.col(pl.Float64).sum().name.suffix(\"_sum\")\n    ])\n)\n\n\nshape: (53, 11)\n\n\n\ntime\nopen_mean\nhigh_mean\nlow_mean\nclose_mean\nvolume_mean\nopen_sum\nhigh_sum\nlow_sum\nclose_sum\nvolume_sum\n\n\ndate\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2021-01-01\n32305.781429\n34706.045714\n31021.727143\n33807.135714\n117435.5928\n226140.47\n242942.32\n217152.09\n236649.95\n822049.149598\n\n\n2021-01-08\n37869.797143\n39646.105714\n34623.334286\n37827.52\n135188.296617\n265088.58\n277522.74\n242363.34\n264792.64\n946318.076319\n\n\n2021-01-15\n36527.891429\n37412.2\n33961.551429\n35343.847143\n94212.715129\n255695.24\n261885.4\n237730.86\n247406.93\n659489.005903\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n2021-12-24\n49649.114286\n50439.622857\n48528.25\n49117.98\n31126.709793\n347543.8\n353077.36\n339697.75\n343825.86\n217886.96855\n\n\n2021-12-31\n46668.905\n48251.445\n45943.185\n46969.79\n27271.230605\n93337.81\n96502.89\n91886.37\n93939.58\n54542.46121\n\n\n\n\n\n\n\n\n\nohlcv_pd.resample(\"W-Fri\", closed=\"left\", label=\"left\").agg(['mean', 'sum'])\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nvolume\n\n\n\nmean\nsum\nmean\nsum\nmean\nsum\nmean\nsum\nmean\nsum\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2021-01-01\n32305.781429\n226140.47\n34706.045714\n242942.32\n31021.727143\n217152.09\n33807.135714\n236649.95\n117435.592800\n822049.149598\n\n\n2021-01-08\n37869.797143\n265088.58\n39646.105714\n277522.74\n34623.334286\n242363.34\n37827.520000\n264792.64\n135188.296617\n946318.076319\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-12-24\n49649.114286\n347543.80\n50439.622857\n353077.36\n48528.250000\n339697.75\n49117.980000\n343825.86\n31126.709793\n217886.968550\n\n\n2021-12-31\n46668.905000\n93337.81\n48251.445000\n96502.89\n45943.185000\n91886.37\n46969.790000\n93939.58\n27271.230605\n54542.461210\n\n\n\n\n53 rows × 10 columns\n\n\n\n\n\n\n\n\n5.3.2 Upsampling\nUpsampling moves in the opposite direction, from low-frequency data to high frequency data. Since we can’t create new data by magic, upsampling defaults to filling the new rows with nulls (which we could then interpolate, perhaps). In Polars we have a special upsample method for this, while Pandas reuses its resample method.\n\nPolarsPandas\n\n\n\nohlcv_pl.upsample(\"time\", every=\"6h\")\n\n\nshape: (1_461, 6)\n\n\n\ntime\nopen\nhigh\nlow\nclose\nvolume\n\n\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2021-01-01\n28923.63\n29600.0\n28624.57\n29331.69\n54182.925011\n\n\n2021-01-01\nnull\nnull\nnull\nnull\nnull\n\n\n2021-01-01\nnull\nnull\nnull\nnull\nnull\n\n\n…\n…\n…\n…\n…\n…\n\n\n2021-12-31\nnull\nnull\nnull\nnull\nnull\n\n\n2022-01-01\n46216.93\n47954.63\n46208.37\n47722.65\n19604.46325\n\n\n\n\n\n\n\n\n\nohlcv_pd.resample(\"6h\").mean()\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nvolume\n\n\ntime\n\n\n\n\n\n\n\n\n\n2021-01-01 00:00:00\n28923.63\n29600.00\n28624.57\n29331.69\n54182.925011\n\n\n2021-01-01 06:00:00\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n2021-12-31 18:00:00\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-01-01 00:00:00\n46216.93\n47954.63\n46208.37\n47722.65\n19604.463250\n\n\n\n\n1461 rows × 5 columns",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Timeseries</span>"
    ]
  },
  {
    "objectID": "timeseries.html#rolling-expanding-ew",
    "href": "timeseries.html#rolling-expanding-ew",
    "title": "5  Timeseries",
    "section": "5.4 Rolling / Expanding / EW",
    "text": "5.4 Rolling / Expanding / EW\nPolars supports all three of these but they’re not quite as powerful as in Pandas, since they don’t have as many different methods. The expanding support is more limited again, though there are workarounds for this (see below):\n\nPolarsPandas\n\n\n\nclose = pl.col(\"close\")\nohlcv_pl.select(\n    [\n        pl.col(\"time\"),\n        close.alias(\"Raw\"),\n        close.rolling_mean(28).alias(\"28D MA\"),\n        close.alias(\"Expanding Average\").cum_sum() / (close.cum_count() + 1),\n        close.ewm_mean(alpha=0.03).alias(\"EWMA($\\\\alpha=.03$)\"),\n    ]\n).to_pandas().set_index(\"time\").plot()\n\nplt.ylabel(\"Close ($)\")\n\nText(0, 0.5, 'Close ($)')\n\n\n\n\n\n\n\n\n\n\n\n\nohlcv_pd[\"close\"].plot(label=\"Raw\")\nohlcv_pd[\"close\"].rolling(28).mean().plot(label=\"28D MA\")\nohlcv_pd[\"close\"].expanding().mean().plot(label=\"Expanding Average\")\nohlcv_pd[\"close\"].ewm(alpha=0.03).mean().plot(label=\"EWMA($\\\\alpha=.03$)\")\n\nplt.legend(bbox_to_anchor=(0.63, 0.27))\nplt.ylabel(\"Close ($)\")\n\nText(0, 0.5, 'Close ($)')\n\n\n\n\n\n\n\n\n\n\n\n\nPolars doesn’t have an expanding_mean yet so we make do by combining cumsum and cumcount.\n\n5.4.1 Combining rolling aggregations\n\nPolarsPandas\n\n\n\nmean_std_pl = ohlcv_pl.select(\n    [\n        \"time\",\n        pl.col(\"close\").rolling_mean(30, center=True).alias(\"mean\"),\n        pl.col(\"close\").rolling_std(30, center=True).alias(\"std\"),\n    ]\n)\nax = mean_std_pl.to_pandas().set_index(\"time\")[\"mean\"].plot()\nax.fill_between(\n    mean_std_pl[\"time\"].to_numpy(),\n    mean_std_pl[\"mean\"] - mean_std_pl[\"std\"],\n    mean_std_pl[\"mean\"] + mean_std_pl[\"std\"],\n    alpha=0.25,\n)\nplt.tight_layout()\nplt.ylabel(\"Close ($)\")\n\nText(26.83333333333334, 0.5, 'Close ($)')\n\n\n\n\n\n\n\n\n\n\n\n\nroll_pd = ohlcv_pd[\"close\"].rolling(30, center=True)\nmean_std_pd = roll_pd.agg([\"mean\", \"std\"])\nax = mean_std_pd[\"mean\"].plot()\nax.fill_between(\n    mean_std_pd.index,\n    mean_std_pd[\"mean\"] - mean_std_pd[\"std\"],\n    mean_std_pd[\"mean\"] + mean_std_pd[\"std\"],\n    alpha=0.25,\n)\nplt.tight_layout()\nplt.ylabel(\"Close ($)\")\n\nText(26.83333333333334, 0.5, 'Close ($)')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Timeseries</span>"
    ]
  },
  {
    "objectID": "timeseries.html#grab-bag",
    "href": "timeseries.html#grab-bag",
    "title": "5  Timeseries",
    "section": "5.5 Grab Bag",
    "text": "5.5 Grab Bag\n\n5.5.1 Offsets\nPandas has two similar objects for datetime arithmetic: DateOffset which respects calendar arithmetic, and Timedelta which respects absolute time arithmetic. DateOffset understands things like daylight savings time, and can work with holidays too.\nPolars just has a Duration type which is like Pandas Timedelta.\n\nPolarsPandas (Timedelta)Pandas (DateOffset)\n\n\n\nohlcv_pl.select(pl.col(\"time\") + pl.duration(days=80))\n\n\nshape: (366, 1)\n\n\n\ntime\n\n\ndate\n\n\n\n\n2021-03-22\n\n\n2021-03-23\n\n\n2021-03-24\n\n\n…\n\n\n2022-03-21\n\n\n2022-03-22\n\n\n\n\n\n\n\n\n\nohlcv_pd.index + pd.Timedelta(80, \"D\")\n\nDatetimeIndex(['2021-03-22', '2021-03-23', '2021-03-24', '2021-03-25',\n               '2021-03-26', '2021-03-27', '2021-03-28', '2021-03-29',\n               '2021-03-30', '2021-03-31',\n               ...\n               '2022-03-13', '2022-03-14', '2022-03-15', '2022-03-16',\n               '2022-03-17', '2022-03-18', '2022-03-19', '2022-03-20',\n               '2022-03-21', '2022-03-22'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None)\n\n\n\n\n\nohlcv_pd.index + pd.DateOffset(months=3, days=-10)\n\nDatetimeIndex(['2021-03-22', '2021-03-23', '2021-03-24', '2021-03-25',\n               '2021-03-26', '2021-03-27', '2021-03-28', '2021-03-29',\n               '2021-03-30', '2021-03-31',\n               ...\n               '2022-03-13', '2022-03-14', '2022-03-15', '2022-03-16',\n               '2022-03-17', '2022-03-18', '2022-03-19', '2022-03-20',\n               '2022-03-21', '2022-03-22'],\n              dtype='datetime64[us]', name='time', length=366, freq=None)\n\n\n\n\n\n\n\n5.5.2 Holiday calendars\nNot many people know this, but Pandas can do some quite powerful stuff with Holiday Calendars. There is an open issue to add this functionality to Polars.\n\n\n5.5.3 Timezones\nSuppose we know that our timestamps are UTC, and we want to see what time it was in US/Eastern:\n\nPolarsPandas\n\n\n\n(\n    ohlcv_pl\n    .with_columns(\n        pl.col(\"time\")\n        .cast(pl.Datetime)\n        .dt.replace_time_zone(\"UTC\")\n        .dt.convert_time_zone(\"US/Eastern\")\n    )\n)\n\n\nshape: (366, 6)\n\n\n\ntime\nopen\nhigh\nlow\nclose\nvolume\n\n\ndatetime[μs, US/Eastern]\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2020-12-31 19:00:00 EST\n28923.63\n29600.0\n28624.57\n29331.69\n54182.925011\n\n\n2021-01-01 19:00:00 EST\n29331.7\n33300.0\n28946.53\n32178.33\n129993.873362\n\n\n2021-01-02 19:00:00 EST\n32176.45\n34778.11\n31962.99\n33000.05\n120957.56675\n\n\n…\n…\n…\n…\n…\n…\n\n\n2021-12-30 19:00:00 EST\n47120.88\n48548.26\n45678.0\n46216.93\n34937.99796\n\n\n2021-12-31 19:00:00 EST\n46216.93\n47954.63\n46208.37\n47722.65\n19604.46325\n\n\n\n\n\n\n\n\n\n(\n    ohlcv_pd\n    .tz_localize('UTC')\n    .tz_convert('US/Eastern')\n)\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nvolume\n\n\ntime\n\n\n\n\n\n\n\n\n\n2020-12-31 19:00:00-05:00\n28923.63\n29600.00\n28624.57\n29331.69\n54182.925011\n\n\n2021-01-01 19:00:00-05:00\n29331.70\n33300.00\n28946.53\n32178.33\n129993.873362\n\n\n...\n...\n...\n...\n...\n...\n\n\n2021-12-30 19:00:00-05:00\n47120.88\n48548.26\n45678.00\n46216.93\n34937.997960\n\n\n2021-12-31 19:00:00-05:00\n46216.93\n47954.63\n46208.37\n47722.65\n19604.463250\n\n\n\n\n366 rows × 5 columns",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Timeseries</span>"
    ]
  },
  {
    "objectID": "timeseries.html#conclusion",
    "href": "timeseries.html#conclusion",
    "title": "5  Timeseries",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\nPolars has really good time series support, though expanding aggregations and holiday calendars are niches in which it is lacking. Pandas DateTimeIndexes are quite cool too, even if they do bring some pain.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Timeseries</span>"
    ]
  },
  {
    "objectID": "scaling.html",
    "href": "scaling.html",
    "title": "6  Scaling",
    "section": "",
    "text": "6.1 Get the data\nWe’ll be using political donation data from the FEC. Warning: this takes a few minutes.\nimport asyncio\nfrom zipfile import ZipFile\nfrom pathlib import Path\nfrom datetime import date\nfrom io import BytesIO\nimport httpx\nimport polars as pl\nimport pandas as pd\n\n\npl.Config.set_tbl_rows(5)\npd.options.display.max_rows = 5\n\nfec_dir = Path(\"../data/fec\")\n\nasync def download_and_save_cm(year: str, client: httpx.AsyncClient):\n    cm_cols = [\"CMTE_ID\", \"CMTE_NM\", \"CMTE_PTY_AFFILIATION\"]\n    dtypes = {\"CMTE_PTY_AFFILIATION\": pl.Categorical}\n    url = f\"https://www.fec.gov/files/bulk-downloads/20{year}/cm{year}.zip\"\n    resp = await client.get(url)\n    with ZipFile(BytesIO(resp.content)) as z:\n        pl.read_csv(\n            z.read(\"cm.txt\"),\n            has_header=False,\n            columns=[0, 1, 10],\n            new_columns=cm_cols,\n            separator=\"|\",\n            dtypes=dtypes,\n        ).write_parquet(fec_dir / f\"cm{year}.pq\")\n\nasync def download_and_save_indiv(year: str, client: httpx.AsyncClient):\n    dtypes = {\n        \"CMTE_ID\": pl.Utf8,\n        \"EMPLOYER\": pl.Categorical,\n        \"OCCUPATION\": pl.Categorical,\n        \"TRANSACTION_DT\": pl.Utf8,\n        \"TRANSACTION_AMT\": pl.Int32,\n    }\n    url = f\"https://www.fec.gov/files/bulk-downloads/20{year}/indiv{year}.zip\"\n    resp = await client.get(url)\n    with ZipFile(BytesIO(resp.content)) as z:\n        pl.read_csv(\n            z.read(\"itcont.txt\"),\n            has_header=False,\n            columns=[0, 11, 12, 13, 14],\n            new_columns=list(dtypes.keys()),\n            separator=\"|\",\n            dtypes=dtypes,\n            encoding=\"cp1252\",\n        ).with_columns(\n            pl.col(\"TRANSACTION_DT\").str.to_date(format=\"%m%d%Y\", strict=False)\n        ).write_parquet(\n            fec_dir / f\"indiv{year}.pq\"\n        )\n\nyears = [\"08\", \"10\", \"12\", \"14\", \"16\"]\nif not fec_dir.exists():\n    fec_dir.mkdir()\n    async with httpx.AsyncClient(follow_redirects=True, timeout=None) as client:\n        cm_tasks = [download_and_save_cm(year, client) for year in years]\n        indiv_tasks = [download_and_save_indiv(year, client) for year in years]\n        tasks = cm_tasks + indiv_tasks\n        await asyncio.gather(*tasks)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "scaling.html#simple-aggregation",
    "href": "scaling.html#simple-aggregation",
    "title": "6  Scaling",
    "section": "6.2 Simple aggregation",
    "text": "6.2 Simple aggregation\nSuppose we want to find the most common occupations among political donors. Let’s assume that this data is too big for your machine’s memory to read it in all at once.\nWe can solve this using Polars streaming, using Dask’s lazy dataframe or simply using Pandas to read the files one by one and keeping a running total:\n\nPolarsDaskPandas\n\n\n\n# otherwise we can't read categoricals from multiple files\npl.enable_string_cache()\noccupation_counts_pl = (\n    pl.scan_parquet(fec_dir / \"indiv*.pq\", cache=False)\n    .select(pl.col(\"OCCUPATION\").value_counts(parallel=True, sort=True))\n    .collect(streaming=True)\n)\noccupation_counts_pl\n\n\nshape: (344_119, 1)\n\n\n\nOCCUPATION\n\n\nstruct[2]\n\n\n\n\n{\"RETIRED\",1643920}\n\n\n{\"ATTORNEY\",826173}\n\n\n{null,620316}\n\n\n…\n\n\n{\"PURNELL MORROW COMPANY\",1}\n\n\n{\"CITY OF BISHOP\",1}\n\n\n\n\n\n\n\n\n\nimport dask.dataframe as dd\nfrom dask import compute\noccupation_counts_dd = dd.read_parquet(\n    fec_dir / \"indiv*.pq\", engine=\"pyarrow\", columns=[\"OCCUPATION\"]\n)[\"OCCUPATION\"].value_counts()\noccupation_counts_dd.compute()\n\nOCCUPATION\nRETIRED                      1643920\nATTORNEY                      826173\n                              ...   \nADUSTON CONSULTING                 1\nSR, IMMIGRATION PARALEGAL          1\nName: count, Length: 344118, dtype: int64\n\n\n\n\n\nfiles = sorted(fec_dir.glob(\"indiv*.pq\"))\n\ntotal_counts_pd = pd.Series(dtype=\"int64\")\n\nfor year in files:\n    occ_pd = pd.read_parquet(year, columns=[\"OCCUPATION\"], engine=\"pyarrow\")\n    counts = occ_pd[\"OCCUPATION\"].value_counts()\n    total_counts_pd = total_counts_pd.add(counts, fill_value=0).astype(\"int64\")\n\ntotal_counts_pd.nlargest(100)\n\nOCCUPATION\nRETIRED         1643920\nATTORNEY         826173\n                 ...   \nECONOMIST          9336\nENTREPRENEUR       9199\nLength: 100, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPolars can handle some larger-than-memory data even without streaming. Thanks to predicate pushdown, we can filter dataframes without reading all the data into memory first. So streaming mode is most useful for cases where we really do need to read in a lot of data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "scaling.html#executing-multiple-queries-in-parallel",
    "href": "scaling.html#executing-multiple-queries-in-parallel",
    "title": "6  Scaling",
    "section": "6.3 Executing multiple queries in parallel",
    "text": "6.3 Executing multiple queries in parallel\nOften we want to generate multiple insights from the same data, and we need them in separate dataframes. In this case, using collect_all is more efficient than calling .collect multiple times, because Polars can avoid repeating common operations like reading the data.\nLet’s compute the average donation size, the total donated by employer and the average donation by occupation:\n\nPolarsDask\n\n\n\n%%time\nindiv_pl = pl.scan_parquet(fec_dir / \"indiv*.pq\")\navg_transaction_lazy_pl = indiv_pl.select(pl.col(\"TRANSACTION_AMT\").mean())\ntotal_by_employer_lazy_pl = (\n    indiv_pl.drop_nulls(\"EMPLOYER\")\n    .group_by(\"EMPLOYER\")\n    .agg([pl.col(\"TRANSACTION_AMT\").sum()])\n    .sort(\"TRANSACTION_AMT\", descending=True)\n    .head(10)\n)\navg_by_occupation_lazy_pl = (\n    indiv_pl.group_by(\"OCCUPATION\")\n    .agg([pl.col(\"TRANSACTION_AMT\").mean()])\n    .sort(\"TRANSACTION_AMT\", descending=True)\n    .head(10)\n)\n\navg_transaction_pl, total_by_employer_pl, avg_by_occupation_pl = pl.collect_all(\n    [avg_transaction_lazy_pl, total_by_employer_lazy_pl, avg_by_occupation_lazy_pl],\n    streaming=True,\n    comm_subplan_elim=False, # cannot use CSE with streaming\n)\n\nCPU times: user 10.3 s, sys: 1.91 s, total: 12.2 s\nWall time: 4.51 s\n\n\n\n\n\n%%time\nindiv_dd = (\n    dd.read_parquet(fec_dir / \"indiv*.pq\", engine=\"pyarrow\")\n    # pandas and dask want datetimes but this is a date col\n    .assign(\n        TRANSACTION_DT=lambda df: dd.to_datetime(df[\"TRANSACTION_DT\"], errors=\"coerce\")\n    )\n)\navg_transaction_lazy_dd = indiv_dd[\"TRANSACTION_AMT\"].mean()\ntotal_by_employer_lazy_dd = (\n    indiv_dd.groupby(\"EMPLOYER\", observed=True)[\"TRANSACTION_AMT\"].sum().nlargest(10)\n)\navg_by_occupation_lazy_dd = (\n    indiv_dd.groupby(\"OCCUPATION\", observed=True)[\"TRANSACTION_AMT\"].mean().nlargest(10)\n)\navg_transaction_dd, total_by_employer_dd, avg_by_occupation_dd = compute(\n    avg_transaction_lazy_dd, total_by_employer_lazy_dd, avg_by_occupation_lazy_dd\n)\n\nCPU times: user 26.5 s, sys: 1.67 s, total: 28.1 s\nWall time: 21.8 s\n\n\n\n\n\nThe Polars code above tends to be ~3.5x faster than Dask on my machine, which if anything is a smaller speedup than I expected.\nWe should also profile memory usage, since it could be the case that Polars is just running faster because it’s reading in bigger chunks. According to the fil profiler, the Dask example’s memory usage peaks at 1450 MiB, while Polars uses ~10% more than that.\nBefore I forget, here are the results of our computations:\n\n6.3.1 avg_transaction\n\nPolarsDask\n\n\n\navg_transaction_pl\n\n\nshape: (1, 1)\n\n\n\nTRANSACTION_AMT\n\n\nf64\n\n\n\n\n1056.45334\n\n\n\n\n\n\n\n\n\navg_transaction_dd\n\n1056.4533404188285\n\n\n\n\n\n\n\n6.3.2 total_by_employer\n\nPolarsDask\n\n\n\ntotal_by_employer_pl\n\n\nshape: (10, 2)\n\n\n\nEMPLOYER\nTRANSACTION_AMT\n\n\ncat\ni32\n\n\n\n\n\"RETIRED\"\n694090644\n\n\n\"SELF-EMPLOYED\"\n561802551\n\n\n\"SELF\"\n403477909\n\n\n…\n…\n\n\n\"FAHR, LLC\"\n76995400\n\n\n\"CANDIDATE\"\n73542276\n\n\n\n\n\n\n\n\n\ntotal_by_employer_dd\n\nEMPLOYER\nRETIRED          694090644\nSELF-EMPLOYED    561802551\n                   ...    \nFAHR, LLC         76995400\nCANDIDATE         73542276\nName: TRANSACTION_AMT, Length: 10, dtype: int32\n\n\n\n\n\n\n\n6.3.3 avg_by_occupation\n\nPolarsDask\n\n\n\navg_by_occupation_pl\n\n\nshape: (10, 2)\n\n\n\nOCCUPATION\nTRANSACTION_AMT\n\n\ncat\nf64\n\n\n\n\n\"PAULSON AND CO., INC.\"\n1e6\n\n\n\"CO-FOUNDING DIRECTOR\"\n875000.0\n\n\n\"CO-FOUNDER, DIRECTOR\"\n550933.333333\n\n\n…\n…\n\n\n\"CO-PRINCIPAL\"\n367000.0\n\n\n\"STEPHEN PATRICK LAFFEY\"\n333692.0\n\n\n\n\n\n\n\n\n\navg_by_occupation_dd\n\nOCCUPATION\nPAULSON AND CO., INC.     1000000.0\nCO-FOUNDING DIRECTOR       875000.0\n                            ...    \nCO-PRINCIPAL               367000.0\nSTEPHEN PATRICK LAFFEY     333692.0\nName: TRANSACTION_AMT, Length: 10, dtype: float64",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "scaling.html#filtering",
    "href": "scaling.html#filtering",
    "title": "6  Scaling",
    "section": "6.4 Filtering",
    "text": "6.4 Filtering\nLet’s filter for only the 10 most common occupations and compute some summary statistics:\n\n6.4.1 avg_by_occupation, filtered\nGetting the most common occupations:\n\nPolarsDask\n\n\n\ntop_occupations_pl = (\n    occupation_counts_pl.select(\n        pl.col(\"OCCUPATION\")\n        .struct.field(\"OCCUPATION\")\n        .drop_nulls()\n        .head(10)\n    )\n    .to_series()\n)\ntop_occupations_pl\n\n\nshape: (10,)\n\n\n\nOCCUPATION\n\n\ncat\n\n\n\n\n\"RETIRED\"\n\n\n\"ATTORNEY\"\n\n\n\"PRESIDENT\"\n\n\n…\n\n\n\"CONSULTANT\"\n\n\n\"CEO\"\n\n\n\n\n\n\n\n\n\ntop_occupations_dd = occupation_counts_dd.head(10).index\ntop_occupations_dd\n\nCategoricalIndex(['RETIRED', 'ATTORNEY', 'PRESIDENT', 'PHYSICIAN', 'HOMEMAKER',\n                  'INFORMATION REQUESTED', 'EXECUTIVE', 'OWNER', 'CONSULTANT',\n                  'CEO'],\n                 categories=['PUBLIC RELATIONS CONSULTANT', 'PRESIDENT', 'PHYSICIAN', 'SENIOR EXECUTIVE', ..., 'PRODUCT DIST', 'EXECUTIVE VICE PRESIDENT, CHIEF COMMUN', 'ACTOR/TEACHER/D', 'SR, IMMIGRATION PARALEGAL'], ordered=False, dtype='category', name='OCCUPATION')\n\n\n\n\n\n\nPolarsDask\n\n\n\ndonations_pl_lazy = (\n    indiv_pl.filter(pl.col(\"OCCUPATION\").is_in(top_occupations_pl.to_list()))\n    .group_by(\"OCCUPATION\")\n    .agg(pl.col(\"TRANSACTION_AMT\").mean())\n)\ntotal_avg_pl, occupation_avg_pl = pl.collect_all(\n    [indiv_pl.select(pl.col(\"TRANSACTION_AMT\").mean()), donations_pl_lazy],\n    streaming=True,\n    comm_subplan_elim=False\n)\n\n\n\n\ndonations_dd_lazy = (\n    indiv_dd[indiv_dd[\"OCCUPATION\"].isin(top_occupations_dd)]\n    .groupby(\"OCCUPATION\", observed=True)[\"TRANSACTION_AMT\"]\n    .mean()\n    .dropna()\n)\ntotal_avg_dd, occupation_avg_dd = compute(\n    indiv_dd[\"TRANSACTION_AMT\"].mean(), donations_dd_lazy\n)\n\n\n\n\n\n\n6.4.2 Plotting\nThese results are small enough to plot:\n\nPolarsDask\n\n\n\nax = (\n    occupation_avg_pl\n    .to_pandas()\n    .set_index(\"OCCUPATION\")\n    .squeeze()\n    .sort_values(ascending=False)\n    .plot.barh(color=\"k\", width=0.9)\n)\nlim = ax.get_ylim()\nax.vlines(total_avg_pl, *lim, color=\"C1\", linewidth=3)\nax.legend([\"Average donation\"])\nax.set(xlabel=\"Donation Amount\", title=\"Average Donation by Occupation\")\n\n[Text(0.5, 0, 'Donation Amount'),\n Text(0.5, 1.0, 'Average Donation by Occupation')]\n\n\n\n\n\n\n\n\n\n\n\n\nax = occupation_avg_dd.sort_values(ascending=False).plot.barh(color=\"k\", width=0.9)\nlim = ax.get_ylim()\nax.vlines(total_avg_dd, *lim, color=\"C1\", linewidth=3)\nax.legend([\"Average donation\"])\nax.set(xlabel=\"Donation Amount\", title=\"Average Donation by Occupation\")\n\n[Text(0.5, 0, 'Donation Amount'),\n Text(0.5, 1.0, 'Average Donation by Occupation')]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "scaling.html#resampling",
    "href": "scaling.html#resampling",
    "title": "6  Scaling",
    "section": "6.5 Resampling",
    "text": "6.5 Resampling\nResampling is another useful way to get our data down to a manageable size:\n\nPolarsDask\n\n\n\ndaily_pl = (\n    indiv_pl.select([\"TRANSACTION_DT\", \"TRANSACTION_AMT\"])\n    .drop_nulls()\n    .sort(\"TRANSACTION_DT\")\n    .group_by_dynamic(\"TRANSACTION_DT\", every=\"1d\")\n    .agg(pl.col(\"TRANSACTION_AMT\").sum())\n    .filter(\n        pl.col(\"TRANSACTION_DT\")\n        .is_between(date(2011, 1, 1), date(2017, 1, 1), closed=\"left\")\n    )\n    .with_columns(pl.col(\"TRANSACTION_AMT\") / 1000)\n    .collect(streaming=True)\n)\nax = (\n    daily_pl.select(\n        [pl.col(\"TRANSACTION_DT\").cast(pl.Datetime), \"TRANSACTION_AMT\"]\n    )\n    .to_pandas()\n    .set_index(\"TRANSACTION_DT\")\n    .squeeze()\n    .plot(figsize=(12, 6))\n)\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\")\n\n[(0.0, 59192.975450000005),\n Text(0.5, 1.0, 'Daily Donations'),\n Text(0, 0.5, '$ (thousands)')]\n\n\n\n\n\n\n\n\n\n\n\n\ndaily_dd = (\n    indiv_dd[[\"TRANSACTION_DT\", \"TRANSACTION_AMT\"]]\n    .dropna()\n    .set_index(\"TRANSACTION_DT\")[\"TRANSACTION_AMT\"]\n    .resample(\"D\")\n    .sum()\n    .loc[\"2011\":\"2016\"]\n    .div(1000)\n    .compute()\n)\n\nax = daily_dd.plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\")\n\n[(0.0, 59192.97545),\n Text(0.5, 1.0, 'Daily Donations'),\n Text(0, 0.5, '$ (thousands)')]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "scaling.html#joining",
    "href": "scaling.html#joining",
    "title": "6  Scaling",
    "section": "6.6 Joining",
    "text": "6.6 Joining\nPolars joins work in streaming mode. Let’s add join the donations data with the committee master data, which contains information about the committees people donate to.\n\nPolarsDask\n\n\n\ncm_pl = (\n    # This data is small so we don't use streaming.\n    # Also, .last isn't available in lazy mode.\n    pl.read_parquet(fec_dir / \"cm*.pq\")\n    # Some committees change their name, but the ID stays the same\n    .group_by(\"CMTE_ID\", maintain_order=True).last()\n)\ncm_pl\n\n\nshape: (28_467, 3)\n\n\n\nCMTE_ID\nCMTE_NM\nCMTE_PTY_AFFILIATION\n\n\nstr\nstr\ncat\n\n\n\n\n\"C00000042\"\n\"ILLINOIS TOOL WORKS INC. FOR B…\nnull\n\n\n\"C00000059\"\n\"HALLMARK CARDS PAC\"\n\"UNK\"\n\n\n\"C00000422\"\n\"AMERICAN MEDICAL ASSOCIATION P…\nnull\n\n\n…\n…\n…\n\n\n\"C90017336\"\n\"LUDWIG, EUGENE\"\nnull\n\n\n\"C90017542\"\n\"CENTER FOR POPULAR DEMOCRACY A…\nnull\n\n\n\n\n\n\n\n\n\ncm_dd = (\n    # This data is small but we use dask here as a \n    # convenient way to read a glob of files.\n    dd.read_parquet(fec_dir / \"cm*.pq\")\n    .compute()\n    # Some committees change their name, but the\n    # ID stays the same.\n    # If we use .last instead of .nth(-1),\n    # we get the last non-null value\n    .groupby(\"CMTE_ID\", as_index=False)\n    .nth(-1)\n)\ncm_dd\n\n\n\n\n\n\n\n\nCMTE_ID\nCMTE_NM\nCMTE_PTY_AFFILIATION\n\n\n\n\n7\nC00000794\nLENT & SCRIVNER PAC\nUNK\n\n\n15\nC00001156\nMICHIGAN LEAGUE OF COMMUNITY BANKS POLITICAL A...\nNaN\n\n\n...\n...\n...\n...\n\n\n17649\nC99002396\nAMERICAN POLITICAL ACTION COMMITTEE\nNaN\n\n\n17650\nC99003428\nTHIRD DISTRICT REPUBLICAN PARTY\nREP\n\n\n\n\n28467 rows × 3 columns\n\n\n\n\n\n\nMerging:\n\nPolarsDask\n\n\n\nindiv_filtered_pl = indiv_pl.filter(\n    pl.col(\"TRANSACTION_DT\").is_between(\n        date(2007, 1, 1), date(2017, 1, 1), closed=\"both\"\n    )\n)\nmerged_pl = indiv_filtered_pl.join(cm_pl.lazy(), on=\"CMTE_ID\")\n\n\n\n\nindiv_filtered_dd = indiv_dd[\n    (indiv_dd[\"TRANSACTION_DT\"] &gt;= pd.Timestamp(\"2007-01-01\"))\n    & (indiv_dd[\"TRANSACTION_DT\"] &lt;= pd.Timestamp(\"2017-01-01\"))\n]\nmerged_dd = dd.merge(indiv_filtered_dd, cm_dd, on=\"CMTE_ID\")\n\n\n\n\nDaily donations by party:\n\nPolarsDask\n\n\n\nparty_donations_pl = (\n    merged_pl.group_by([\"TRANSACTION_DT\", \"CMTE_PTY_AFFILIATION\"])\n    .agg(pl.col(\"TRANSACTION_AMT\").sum())\n    .sort([\"TRANSACTION_DT\", \"CMTE_PTY_AFFILIATION\"])\n    .collect(streaming=True)\n)\n\n\n\n\nparty_donations_dd = (\n    (\n        merged_dd.groupby([\"TRANSACTION_DT\", \"CMTE_PTY_AFFILIATION\"])[\n            \"TRANSACTION_AMT\"\n        ].sum()\n    )\n    .compute()\n    .sort_index()\n)\n\n\n\n\nPlotting daily donations:\n\nPolarsDask\n\n\n\nax = (\n    party_donations_pl\n    .pivot(\n        index=\"TRANSACTION_DT\", on=\"CMTE_PTY_AFFILIATION\", values=\"TRANSACTION_AMT\"\n    )[1:, :]\n    .select(\n        [pl.col(\"TRANSACTION_DT\"), pl.col(pl.Int32).rolling_mean(30, min_periods=0)]\n    )\n    .to_pandas()\n    .set_index(\"TRANSACTION_DT\")\n    [[\"DEM\", \"REP\"]]\n    .plot(color=[\"C0\", \"C3\"], figsize=(12, 6), linewidth=3)\n)\nax.set(title=\"Daily Donations (30-D Moving Average)\", xlabel=\"Date\")\n\n\n\n\n\n\n\n\n\n\n\nax = (\n    party_donations_dd\n    .unstack(\"CMTE_PTY_AFFILIATION\")\n    .iloc[1:]\n    .rolling(\"30D\")\n    .mean()\n    [[\"DEM\", \"REP\"]]\n    .plot(color=[\"C0\", \"C3\"], figsize=(12, 6), linewidth=3)\n)\nax.set(title=\"Daily Donations (30-D Moving Average)\", xlabel=\"Date\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Reasons to use Polars",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "summary.html#reasons-to-use-polars",
    "href": "summary.html#reasons-to-use-polars",
    "title": "Summary",
    "section": "",
    "text": "It’s really fast.\nIt has a nice API.\nIt does most of the things Pandas does. The biggest missing things are plotting and some I/O methods.\nIt’s available in Python, Rust, NodeJS and Ruby. This is partly because most of the code is written in Rust, and calling Rust in other languages works much better than calling Python.\nThe lead dev is very productive and quick to fix bugs.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "summary.html#reasons-not-to-use-polars",
    "href": "summary.html#reasons-not-to-use-polars",
    "title": "Summary",
    "section": "Reasons not to use Polars",
    "text": "Reasons not to use Polars\n\nBugs (maybe)\nBy my count I ran into 11 bugs while writing this book. They were fixed quickly and tests were added to make sure they don’t happen again, but it made me feel somewhat uneasy.\nHowever that was some time ago, and Pandas also has plenty of bugs, so at this point it’s quite difficult to measure which library is buggier.\n\n\nIf it ain’t broke, don’t fix it\nSuppose you have a bunch of important stuff in another library that works fine, even if it’s a bit slow. Maybe it doesn’t have good tests. Switching library is tricky and may not be worth it here. Even if Polars is bug-free, various default behaviours might differ from your expectations.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "summary.html#other-cool-stuff-you-might-like",
    "href": "summary.html#other-cool-stuff-you-might-like",
    "title": "Summary",
    "section": "Other cool stuff you might like",
    "text": "Other cool stuff you might like\n\nr-polars, a work-in-progress project bringing Polars to R.\ntidypolars: an API for py-polars that should be familiar to R Tidyverse users.\nDuckDB: not a dataframe library, but can do a lot of what Polars does and is often mentioned in the same breath.",
    "crumbs": [
      "Summary"
    ]
  }
]
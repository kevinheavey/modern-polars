[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modern Polars",
    "section": "",
    "text": "Preface\nThis is a side-by-side comparison of the Polars and Pandas dataframe libraries, based on Modern Pandas by Tom Augsburger.\n(In case you haven‚Äôt heard, Polars is a very fast and elegant dataframe libary that does the same kinds of things Pandas does.)\nThe bulk of this book is structured examples of idiomatic Polars and Pandas code, with commentary on the API and performance of both.\nFor the most part, I argue that Polars is ‚Äúbetter‚Äù than Pandas, though I do try and make it clear when Polars is lacking a Pandas feature or is otherwise disappointing."
  },
  {
    "objectID": "index.html#who-is-this-for",
    "href": "index.html#who-is-this-for",
    "title": "Modern Polars",
    "section": "Who is this for?",
    "text": "Who is this for?\nThis is not a beginner‚Äôs introduction to data programming, though you certainly don‚Äôt need to be an expert to read it. If you have some familiarity with any dataframe library, most of the examples should make sense, but if you‚Äôre familiar with Pandas they‚Äôll make even more sense because all the Polars code is accompanied by the equivalent Pandas code.\nYou don‚Äôt need to have read Modern Pandas, though I of course think it‚Äôs a great read."
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "Modern Polars",
    "section": "Why?",
    "text": "Why?\nThere‚Äôs this weird phenomenon where people write data programming code as if they hate themselves. Many of them are academic or quant types who seem to have some complex about being ‚Äúbad at coding‚Äù. Armchair psychology aside, lots of clever folk keep doing really dumb stuff with Pandas, and at some point you have to wonder if the Pandas API is too difficult for its users.\nAt the very least, articles like Minimally Sufficient Pandas make a compelling case for Pandas having too much going on.\nHaving used Pandas a lot, I think Polars is more intuitive and does a better job of having One Obvious Way to do stuff. It‚Äôs also much faster at most things, even when you do Pandas the right way.\nHopefully this work shows you how, why and when to prefer Polars."
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "Modern Polars",
    "section": "Credit",
    "text": "Credit\nThe Pandas examples are mostly lifted from Tom‚Äôs articles, with some updates for data that‚Äôs no longer available, and some code changes to reflect how Pandas is written in 2023. This isn‚Äôt just me being lazy - I want to draw on Pandas examples that quite a lot of people are already familiar with.\nSo credit goes to Tom for the Pandas examples, for most of the data fetching code and for the general structure of the articles. Meanwhile the text content and the Polars examples are from me."
  },
  {
    "objectID": "index.html#running-the-code-yourself",
    "href": "index.html#running-the-code-yourself",
    "title": "Modern Polars",
    "section": "Running the code yourself",
    "text": "Running the code yourself\nYou can install the exact packages that the book uses with the env.yml file:\nmamba env create -f env.yml\nIf you‚Äôre not using mamba/conda you can install the following package versions and it should work:\npolars: 0.20.2\npyarrow: 10.0.1\npandas: 2.1.1\nnumpy: 1.23.5\nfsspec: 2022.11.0\nmatplotlib: 3.8.0\nseaborn: 0.13.0\nstatsmodels: 0.14.0\nfilprofiler: 2022.11.0\n\nData\nAll the data fetching code is included, but will eventually break as websites change or shut down. The smaller datasets have been checked in here for posterity."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Modern Polars",
    "section": "Contributing",
    "text": "Contributing\nThis book is free and open source, so please do open an issue if you notice a problem!"
  },
  {
    "objectID": "indexing.html#fetch-data-no-dataframes-here",
    "href": "indexing.html#fetch-data-no-dataframes-here",
    "title": "1¬† Indexing (Or Lack Thereof)",
    "section": "1.1 Fetch Data (No Dataframes Here)",
    "text": "1.1 Fetch Data (No Dataframes Here)\nFirst we fetch some flight delay data. This part isn‚Äôt about dataframes so feel free to skip the code.\n\n\nCode\nfrom pathlib import Path\nfrom zipfile import ZipFile\nimport requests\n\ndata_dir = Path(\"../data\") # replace this with a directory of your choice\ndest = data_dir / \"flights.csv.zip\"\n\nif not dest.exists():\n    r = requests.get(\n        \"https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_1.zip\",\n        verify=False,\n        stream=True,\n    )\n\n    data_dir.mkdir(exist_ok=True)\n    with dest.open(\"wb\") as f:\n        for chunk in r.iter_content(chunk_size=102400):\n            if chunk:\n                f.write(chunk)\n\n    with ZipFile(dest) as zf:\n        zf.extract(zf.filelist[0].filename, path=data_dir)\n\nextracted = data_dir / \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\""
  },
  {
    "objectID": "indexing.html#read-the-data",
    "href": "indexing.html#read-the-data",
    "title": "1¬† Indexing (Or Lack Thereof)",
    "section": "1.2 Read the data",
    "text": "1.2 Read the data\n\n\n\n\n\n\nTip\n\n\n\nThe examples in this book use the lazy evaluation feature of Polars less than you should. It‚Äôs just inconvenient to use the lazy API when displaying dozens of intermediate results for educational purposes.\n\n\n\nPolarsPandas\n\n\n\nimport polars as pl\npl.Config.set_tbl_rows(5) # don't print too many rows in the book\ndf_pl = pl.read_csv(extracted, truncate_ragged_lines=True)\ndf_pl\n\n\n\nshape: (537_902, 109)YearQuarterMonthDayofMonthDayOfWeekFlightDateReporting_AirlineDOT_ID_Reporting_AirlineIATA_CODE_Reporting_AirlineTail_NumberFlight_Number_Reporting_AirlineOriginAirportIDOriginAirportSeqIDOriginCityMarketIDOriginOriginCityNameOriginStateOriginStateFipsOriginStateNameOriginWacDestAirportIDDestAirportSeqIDDestCityMarketIDDestDestCityNameDestStateDestStateFipsDestStateNameDestWacCRSDepTimeDepTimeDepDelayDepDelayMinutesDepDel15DepartureDelayGroupsDepTimeBlkTaxiOut‚Ä¶Div1WheelsOnDiv1TotalGTimeDiv1LongestGTimeDiv1WheelsOffDiv1TailNumDiv2AirportDiv2AirportIDDiv2AirportSeqIDDiv2WheelsOnDiv2TotalGTimeDiv2LongestGTimeDiv2WheelsOffDiv2TailNumDiv3AirportDiv3AirportIDDiv3AirportSeqIDDiv3WheelsOnDiv3TotalGTimeDiv3LongestGTimeDiv3WheelsOffDiv3TailNumDiv4AirportDiv4AirportIDDiv4AirportSeqIDDiv4WheelsOnDiv4TotalGTimeDiv4LongestGTimeDiv4WheelsOffDiv4TailNumDiv5AirportDiv5AirportIDDiv5AirportSeqIDDiv5WheelsOnDiv5TotalGTimeDiv5LongestGTimeDiv5WheelsOffDiv5TailNumi64i64i64i64i64strstri64strstri64i64i64i64strstrstri64stri64i64i64i64strstrstri64stri64i64strf64f64f64i64strf64‚Ä¶strstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstrstr202211145\"2022-01-14\"\"YX\"20452\"YX\"\"N119HQ\"487911066110660631066\"CMH\"\"Columbus, OH\"\"OH\"39\"Ohio\"4411278112780530852\"DCA\"\"Washington, DC‚Ä¶\"VA\"51\"Virginia\"381224\"1221\"-3.00.00.0-1\"1200-1259\"28.0‚Ä¶\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"202211156\"2022-01-15\"\"YX\"20452\"YX\"\"N122HQ\"487911066110660631066\"CMH\"\"Columbus, OH\"\"OH\"39\"Ohio\"4411278112780530852\"DCA\"\"Washington, DC‚Ä¶\"VA\"51\"Virginia\"381224\"1214\"-10.00.00.0-1\"1200-1259\"19.0‚Ä¶\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶20221164\"2022-01-06\"\"DL\"19790\"DL\"\"N989AT\"157911057110570331057\"CLT\"\"Charlotte, NC\"\"NC\"37\"North Carolina‚Ä¶3610397103970730397\"ATL\"\"Atlanta, GA\"\"GA\"13\"Georgia\"341258\"1257\"-1.00.00.0-1\"1200-1259\"15.0‚Ä¶\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"20221164\"2022-01-06\"\"DL\"19790\"DL\"\"N815DN\"158014869148690334614\"SLC\"\"Salt Lake City‚Ä¶\"UT\"49\"Utah\"8714057140570234057\"PDX\"\"Portland, OR\"\"OR\"41\"Oregon\"922240\"2231\"-9.00.00.0-1\"2200-2259\"10.0‚Ä¶\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\"\"nullnull\"\"nullnull\"\"\"\"\n\n\n\n\n\nimport pandas as pd\npd.options.display.max_rows = 5\ndf_pd = pd.read_csv(extracted)\ndf_pd\n\n/tmp/ipykernel_13203/2805799744.py:3: DtypeWarning: Columns (76,77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n  df_pd = pd.read_csv(extracted)\n\n\n\n\n\n\n  \n    \n      \n      Year\n      Quarter\n      Month\n      DayofMonth\n      DayOfWeek\n      FlightDate\n      Reporting_Airline\n      DOT_ID_Reporting_Airline\n      IATA_CODE_Reporting_Airline\n      Tail_Number\n      ...\n      Div4TailNum\n      Div5Airport\n      Div5AirportID\n      Div5AirportSeqID\n      Div5WheelsOn\n      Div5TotalGTime\n      Div5LongestGTime\n      Div5WheelsOff\n      Div5TailNum\n      Unnamed: 109\n    \n  \n  \n    \n      0\n      2022\n      1\n      1\n      14\n      5\n      2022-01-14\n      YX\n      20452\n      YX\n      N119HQ\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      2022\n      1\n      1\n      15\n      6\n      2022-01-15\n      YX\n      20452\n      YX\n      N122HQ\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      537900\n      2022\n      1\n      1\n      6\n      4\n      2022-01-06\n      DL\n      19790\n      DL\n      N989AT\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      537901\n      2022\n      1\n      1\n      6\n      4\n      2022-01-06\n      DL\n      19790\n      DL\n      N815DN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n537902 rows √ó 110 columns"
  },
  {
    "objectID": "indexing.html#indexing",
    "href": "indexing.html#indexing",
    "title": "1¬† Indexing (Or Lack Thereof)",
    "section": "1.3 Indexing",
    "text": "1.3 Indexing\nPandas uses a special index type that is quite powerful for selecting rows and columns but is also very complicated. To quote Modern Pandas:\n\nThe complexity of pandas‚Äô indexing is a microcosm for the complexity of the pandas API in general. There‚Äôs a reason for the complexity (well, most of it), but that‚Äôs not much consolation while you‚Äôre learning. Still, all of these ways of indexing really are useful enough to justify their inclusion in the library.\n\nIt‚Äôs true that Pandas indexing is quite useful, but it‚Äôs also true that everyone always forgets how do to anything non-trivial with it. The benefits of being able to put df.loc[pd.IndexSlice[:, 'B0':'B1'], :]] in your code base are somewhat dubious.\nPolars avoids this complexity by simply not having an index. It just has ordinary methods like .select, .filter and .head for accessing a subset of rows or columns."
  },
  {
    "objectID": "indexing.html#slicing-vs-selecting",
    "href": "indexing.html#slicing-vs-selecting",
    "title": "1¬† Indexing (Or Lack Thereof)",
    "section": "1.4 Slicing vs Selecting",
    "text": "1.4 Slicing vs Selecting\nIn Pandas you can subset a dataframe with .loc[], .iloc[] or just []. In Polars you select rows and columns with expressions as noted above.\nYou can also use square bracket indexing in Polars, but it doesn‚Äôt work in lazy mode so is only to be used when convenience is most important.\nHere are some examples:\n\n1.4.1 Rows by number, columns by name\n\nPolars (recommended)Polars (square bracket)Pandas\n\n\nUsing head and tail:\n\ndf_pl.select([\"Dest\", \"Tail_Number\"]).head(16).tail(4)\n\n\n\nshape: (4, 2)DestTail_Numberstrstr\"DCA\"\"N132HQ\"\"DCA\"\"N109HQ\"\"DCA\"\"N421YX\"\"DCA\"\"N137HQ\"\n\n\nOr using take:\n\ndf_pl.select(pl.col([\"Dest\", \"Tail_Number\"]).gather(list(range(12, 16))))\n\n\n\nshape: (4, 2)DestTail_Numberstrstr\"DCA\"\"N132HQ\"\"DCA\"\"N109HQ\"\"DCA\"\"N421YX\"\"DCA\"\"N137HQ\"\n\n\n\n\n\ndf_pl[12:16, [\"Dest\", \"Tail_Number\"]]\n\n\n\nshape: (4, 2)DestTail_Numberstrstr\"DCA\"\"N132HQ\"\"DCA\"\"N109HQ\"\"DCA\"\"N421YX\"\"DCA\"\"N137HQ\"\n\n\n\n\n\ndf_pd.loc[12:15, [\"Dest\", \"Tail_Number\"]]\n\n\n\n\n\n  \n    \n      \n      Dest\n      Tail_Number\n    \n  \n  \n    \n      12\n      DCA\n      N132HQ\n    \n    \n      13\n      DCA\n      N109HQ\n    \n    \n      14\n      DCA\n      N421YX\n    \n    \n      15\n      DCA\n      N137HQ\n    \n  \n\n\n\n\n\n\n\n\n\n1.4.2 Rows by string index, columns by name\nSince there‚Äôs no such thing as an index in Polars, so we just use .filter:\n\nPolarsPandas\n\n\n\n(\n    df_pl\n    .filter(pl.col(\"IATA_CODE_Reporting_Airline\").is_in(['AA', 'DL']))\n    .select([\"IATA_CODE_Reporting_Airline\", \"Dest\", \"Tail_Number\"])\n)\n\n\n\nshape: (138_363, 3)IATA_CODE_Reporting_AirlineDestTail_Numberstrstrstr\"DL\"\"LGA\"\"N315DN\"\"DL\"\"FLL\"\"N545US\"‚Ä¶‚Ä¶‚Ä¶\"DL\"\"ATL\"\"N989AT\"\"DL\"\"PDX\"\"N815DN\"\n\n\n\n\n\n(\n    df_pd\n    .set_index(\"IATA_CODE_Reporting_Airline\")\n    .loc[['AA', 'DL'], [\"Dest\", \"Tail_Number\"]]\n)\n\n\n\n\n\n  \n    \n      \n      Dest\n      Tail_Number\n    \n    \n      IATA_CODE_Reporting_Airline\n      \n      \n    \n  \n  \n    \n      AA\n      LAX\n      N106NN\n    \n    \n      AA\n      LAX\n      N112AN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      DL\n      ATL\n      N989AT\n    \n    \n      DL\n      PDX\n      N815DN\n    \n  \n\n138363 rows √ó 2 columns\n\n\n\n\n\n\n\n\n1.4.3 Rows by number, columns by number\nThe Polars docs recommend doing this the evil way with square brackets, so make of that what you will. Selecting columns by number isn‚Äôt a very common operation anyway.\n\nPolarsPandas\n\n\n\ndf_pl[[0, 1, 3], [0, 1]]\n\n\n\nshape: (3, 2)YearQuarteri64i64202212022120221\n\n\n\n\n\ndf_pd.iloc[[0, 1, 3], [0, 1]]\n\n\n\n\n\n  \n    \n      \n      Year\n      Quarter\n    \n  \n  \n    \n      0\n      2022\n      1\n    \n    \n      1\n      2022\n      1\n    \n    \n      3\n      2022\n      1"
  },
  {
    "objectID": "indexing.html#settingwithcopy",
    "href": "indexing.html#settingwithcopy",
    "title": "1¬† Indexing (Or Lack Thereof)",
    "section": "1.5 SettingWithCopy",
    "text": "1.5 SettingWithCopy\nPandas has this cute thing where if you assign values to some subset of the dataframe with square bracket indexing, it doesn‚Äôt work and gives the notorious SettingWithCopyWarning. To be fair, this warning also tells you to assign using .loc. Unfortunately many people in the Pandas community can‚Äôt read and instead just ignore the warning.\nPolars is not yet popular enough to attact the same crowd, but when it does it should not run into the same problem, as the only way to add or overwrite columns in Polars is the with_columns method.\n\nPolarsPandas (bad)Pandas (good)Pandas (better)\n\n\n\nf = pl.DataFrame({'a': [1,2,3,4,5], 'b': [10,20,30,40,50]})\nf.with_columns(\n    pl.when(pl.col(\"a\") <= 3)\n    .then(pl.col(\"b\") // 10)\n    .otherwise(pl.col(\"b\"))\n)\n\n\n\nshape: (5, 2)abi64i64112233440550\n\n\n\n\n\nf = pd.DataFrame({'a': [1,2,3,4,5], 'b': [10,20,30,40,50]})\nf[f['a'] <= 3]['b'] = f['b'] // 10\nf\n\n/tmp/ipykernel_13203/1317853993.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  f[f['a'] <= 3]['b'] = f['b'] // 10\n\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      10\n    \n    \n      1\n      2\n      20\n    \n    \n      2\n      3\n      30\n    \n    \n      3\n      4\n      40\n    \n    \n      4\n      5\n      50\n    \n  \n\n\n\n\n\n\n\nf = pd.DataFrame({'a': [1,2,3,4,5], 'b': [10,20,30,40,50]})\nf.loc[f['a'] <= 3, \"b\"] = f['b'] // 10\nf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      1\n    \n    \n      1\n      2\n      2\n    \n    \n      2\n      3\n      3\n    \n    \n      3\n      4\n      40\n    \n    \n      4\n      5\n      50\n    \n  \n\n\n\n\n\n\n\nf = pd.DataFrame({'a': [1,2,3,4,5], 'b': [10,20,30,40,50]})\nf.assign(b=f[\"b\"].mask(f[\"a\"] <=3, f[\"b\"] // 10))\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      1\n    \n    \n      1\n      2\n      2\n    \n    \n      2\n      3\n      3\n    \n    \n      3\n      4\n      40\n    \n    \n      4\n      5\n      50"
  },
  {
    "objectID": "indexing.html#summary",
    "href": "indexing.html#summary",
    "title": "1¬† Indexing (Or Lack Thereof)",
    "section": "1.6 Summary",
    "text": "1.6 Summary\nBasically, there‚Äôs no index in Polars and square brackets are bad most of the time. I think the lack of an index is quite acceptable even if there are cases where it‚Äôs useful. Most Pandas users just call .reset_index() all the time anyway."
  },
  {
    "objectID": "method_chaining.html#setup",
    "href": "method_chaining.html#setup",
    "title": "2¬† Method Chaining",
    "section": "2.1 Setup",
    "text": "2.1 Setup\n\nfrom pathlib import Path\nimport polars as pl\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npl.Config.set_tbl_rows(5)\npd.options.display.max_rows = 5\n\ndata_dir = Path(\"../data\")\nextracted = data_dir / \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\""
  },
  {
    "objectID": "method_chaining.html#extract-city-names",
    "href": "method_chaining.html#extract-city-names",
    "title": "2¬† Method Chaining",
    "section": "2.2 Extract city names",
    "text": "2.2 Extract city names\nThe dataset has two columns that look like $city, $state. Let‚Äôs define a function that removes the state part from these columns. There‚Äôs no method chaining yet but we do have a few things to talk about while we‚Äôre here:\n\nPolarsPandas\n\n\n\ndef extract_city_name_pl() -> pl.Expr:\n    \"\"\"\n    Chicago, IL -> Chicago for OriginCityName and DestCityName\n    \"\"\"\n    cols = [\"OriginCityName\", \"DestCityName\"]\n    return pl.col(cols).str.split(\",\").list.get(0)\n\n\n\n\ndef extract_city_name_pd(df: pd.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Chicago, IL -> Chicago for OriginCityName and DestCityName\n    \"\"\"\n    cols = [\"OriginCityName\", \"DestCityName\"]\n    return df.assign(**{col: df[col].str.split(\",\", regex=False).str[0] for col in cols})\n\n\n\n\nSome items to note:\n\nOur Pandas function adds columns to a dataframe, while our Polars function simply generates a Polars expression. You‚Äôll find it‚Äôs often easier to pass around Exprs than dataframes because:\n\nThey work on both DataFrame and LazyFrame, and they aren‚Äôt bound to any particular data.\nPolars performs better if you put everything in one .select or .with_columns call, rather than calling .select multiple times. If you pass around expressions, this pattern is easy.\n\nPolars is fast and convenient for doing the same thing to multiple columns. We can pass a list of columns to pl.col and then call a method on that pl.col as if it were one column. When the expression gets executed it will be parallelized by Polars.\nMeanwhile in Pandas we have to loop through the columns to create a dictionary of kwargs for .assign. This is not parallelized. (We could use .apply with axis=0 instead but this would still take place sequentially and isn‚Äôt any easier to read, in my opinion).\nCalling .str.split in Polars creates a column where every element is a list. This kind of data is annoying in Pandas because it‚Äôs slow and awkward to work with - notice how the most convenient way to get the first element of a list column in Pandas is to call .str[0], even though this is a list, not a string ü§î\nI‚Äôm not sure if that‚Äôs even supposed to work. In contrast, Polars actually has first class support for list columns, and they are fast as long as they don‚Äôt have mixed types.\n\n\nPolarsPandas\n\n\n\ndef time_col_pl(col: str) -> pl.Expr:\n    col_expr = pl.col(col)\n    return (\n        pl.when(col_expr == \"2400\")\n        .then(pl.lit(\"0000\"))\n        .otherwise(col_expr)\n        .str.strptime(pl.Time, \"%H%M\", strict=True)\n        .alias(col)\n    )\n\n\ndef time_to_datetime_pl(columns: list[str]) -> list[pl.Expr]:\n    \"\"\"\n    Combine all time items into datetimes.\n\n    2014-01-01,0914 -> 2014-01-01 09:14:00\n    \"\"\"\n    date_col = pl.col(\"FlightDate\")\n    return [\n        date_col\n        .dt.combine(time_col_pl(col))\n        .alias(col)\n        for col in columns\n    ]\n\n\n\n\ndef time_col_pd(col: str, df: pd.DataFrame) -> pd.Series:\n    timepart = df[col].replace(\"2400\", \"0000\")\n    return pd.to_datetime(df[\"FlightDate\"] + ' ' +\n                            timepart.str.slice(0, 2) + ':' +\n                            timepart.str.slice(2, 4),\n                            errors='coerce')\n\ndef time_to_datetime_pd(df: pd.DataFrame, columns: list[str]) -> pd.DataFrame:\n    '''\n    Combine all time items into datetimes.\n\n    2014-01-01,0914 -> 2014-01-01 09:14:00\n    '''\n    return df.assign(**{col: time_col_pd(col, df) for col in columns})\n\n\n\n\nThe Pandas version concatenates the date and time strings then parses the datetime string. We could do the same in Polars but I wanted to show you the pl.Date and pl.Time dtypes, which Pandas lacks. We can then combine pl.Date and pl.Time with .dt.combine()."
  },
  {
    "objectID": "method_chaining.html#bringing-it-all-back-home",
    "href": "method_chaining.html#bringing-it-all-back-home",
    "title": "2¬† Method Chaining",
    "section": "2.3 Bringing It All Back Home",
    "text": "2.3 Bringing It All Back Home\nIt‚Äôs time for some method chaining. First, some common variables for both the Polars and Pandas code:\n\ncategory_cols = [\n    \"Dest\",\n    \"Tail_Number\",\n    \"IATA_CODE_Reporting_Airline\",\n    \"CancellationCode\",\n]\ntime_cols = [\"DepTime\", \"ArrTime\", \"CRSArrTime\", \"CRSDepTime\"]\ncols = (\n    category_cols\n    + time_cols\n    + [\n        \"FlightDate\",\n        \"Flight_Number_Reporting_Airline\",\n        \"OriginCityName\",\n        \"DestCityName\",\n        \"Origin\",\n        \"DepDelay\",\n    ]\n)\n\nNow to read the CSVs and use the functions we defined above:\n\nPolarsPandas\n\n\n\ndtypes_pl = (\n    {col: pl.Categorical for col in category_cols}\n    | {\"FlightDate\": pl.Date}\n    | {col: pl.Utf8 for col in time_cols}\n)\ndf_pl = (\n    pl.scan_csv(extracted, dtypes=dtypes_pl, null_values=\"\")\n    .select(cols)\n    .with_columns([extract_city_name_pl(), *time_to_datetime_pl(time_cols)])\n    .collect()\n)\ndf_pl.head()\n\n\n\nshape: (5, 14)DestTail_NumberIATA_CODE_Reporting_AirlineCancellationCodeDepTimeArrTimeCRSArrTimeCRSDepTimeFlightDateFlight_Number_Reporting_AirlineOriginCityNameDestCityNameOriginDepDelaycatcatcatcatdatetime[Œºs]datetime[Œºs]datetime[Œºs]datetime[Œºs]datei64strstrstrf64\"DCA\"\"N119HQ\"\"YX\"null2022-01-14 12:21:002022-01-14 13:56:002022-01-14 13:52:002022-01-14 12:24:002022-01-144879\"Columbus\"\"Washington\"\"CMH\"-3.0\"DCA\"\"N122HQ\"\"YX\"null2022-01-15 12:14:002022-01-15 13:28:002022-01-15 13:52:002022-01-15 12:24:002022-01-154879\"Columbus\"\"Washington\"\"CMH\"-10.0\"DCA\"\"N412YX\"\"YX\"null2022-01-16 12:18:002022-01-16 13:39:002022-01-16 13:52:002022-01-16 12:24:002022-01-164879\"Columbus\"\"Washington\"\"CMH\"-6.0\"DCA\"\"N405YX\"\"YX\"null2022-01-17 12:17:002022-01-17 14:01:002022-01-17 13:52:002022-01-17 12:24:002022-01-174879\"Columbus\"\"Washington\"\"CMH\"-7.0\"DCA\"\"N420YX\"\"YX\"null2022-01-18 12:18:002022-01-18 13:23:002022-01-18 13:52:002022-01-18 12:24:002022-01-184879\"Columbus\"\"Washington\"\"CMH\"-6.0\n\n\n\n\n\ndtypes_pd = (\n    {col: pd.CategoricalDtype() for col in category_cols}\n    | {col: pd.StringDtype() for col in time_cols}\n)\ndf_pd = (\n    pd.read_csv(extracted, dtype=dtypes_pd, usecols=cols, na_values=\"\")\n    .pipe(extract_city_name_pd)\n    .pipe(time_to_datetime_pd, time_cols)\n    .assign(FlightDate=lambda df: pd.to_datetime(df[\"FlightDate\"]))\n)\ndf_pd[cols].head()\n\n\n\n\n\n  \n    \n      \n      Dest\n      Tail_Number\n      IATA_CODE_Reporting_Airline\n      CancellationCode\n      DepTime\n      ArrTime\n      CRSArrTime\n      CRSDepTime\n      FlightDate\n      Flight_Number_Reporting_Airline\n      OriginCityName\n      DestCityName\n      Origin\n      DepDelay\n    \n  \n  \n    \n      0\n      DCA\n      N119HQ\n      YX\n      NaN\n      2022-01-14 12:21:00\n      2022-01-14 13:56:00\n      2022-01-14 13:52:00\n      2022-01-14 12:24:00\n      2022-01-14\n      4879\n      Columbus\n      Washington\n      CMH\n      -3.0\n    \n    \n      1\n      DCA\n      N122HQ\n      YX\n      NaN\n      2022-01-15 12:14:00\n      2022-01-15 13:28:00\n      2022-01-15 13:52:00\n      2022-01-15 12:24:00\n      2022-01-15\n      4879\n      Columbus\n      Washington\n      CMH\n      -10.0\n    \n    \n      2\n      DCA\n      N412YX\n      YX\n      NaN\n      2022-01-16 12:18:00\n      2022-01-16 13:39:00\n      2022-01-16 13:52:00\n      2022-01-16 12:24:00\n      2022-01-16\n      4879\n      Columbus\n      Washington\n      CMH\n      -6.0\n    \n    \n      3\n      DCA\n      N405YX\n      YX\n      NaN\n      2022-01-17 12:17:00\n      2022-01-17 14:01:00\n      2022-01-17 13:52:00\n      2022-01-17 12:24:00\n      2022-01-17\n      4879\n      Columbus\n      Washington\n      CMH\n      -7.0\n    \n    \n      4\n      DCA\n      N420YX\n      YX\n      NaN\n      2022-01-18 12:18:00\n      2022-01-18 13:23:00\n      2022-01-18 13:52:00\n      2022-01-18 12:24:00\n      2022-01-18\n      4879\n      Columbus\n      Washington\n      CMH\n      -6.0\n    \n  \n\n\n\n\n\n\n\nDifferences between the two approaches:\n\nSince scan_csv is lazy, using scan_csv followed by .selecting a subset of columns is equivalent to usecols in pd.read_csv. This is why pl.scan_csv itself doesn‚Äôt have a parameter for choosing a subset of columns to read.\nPolars does have a .pipe method, but we don‚Äôt use it in this case since it‚Äôs easier to work with expressions.\n\n\n\n\n\n\n\nTip\n\n\n\nThe .with_columns method is for adding new columns or overwriting existing ones. But .select can also do those things, so you may be wondering: what‚Äôs the difference?\nBasically the .with_columns method is just convenient for when you don‚Äôt want to reselect all the columns you‚Äôre not modifying."
  },
  {
    "objectID": "method_chaining.html#example-plots",
    "href": "method_chaining.html#example-plots",
    "title": "2¬† Method Chaining",
    "section": "2.4 Example plots",
    "text": "2.4 Example plots\n\n2.4.1 Daily Flights\nHere‚Äôs how plotting the number of daily flights looks in Polars and Pandas:\n\n\n\n\n\n\nNote\n\n\n\nPolars works fine with Matplotlib, Plotly, Seaborn, and Altair, but it doesn‚Äôt have its own .plot method. The .plot method in Pandas is pretty convenient for exploratory data analysis, so for some these examples we‚Äôll just use .to_pandas() followed by .plot() function after doing all the data manipulation in Polars.\n\n\n\nPolarsPandas\n\n\n\n# filter for the busiest airlines\nfilter_expr = pl.col(\"IATA_CODE_Reporting_Airline\").is_in(\n    pl.col(\"IATA_CODE_Reporting_Airline\")\n    .value_counts(sort=True)\n    .struct.field(\"IATA_CODE_Reporting_Airline\")\n    .head(5)\n)\n(\n    df_pl\n    .drop_nulls(subset=[\"DepTime\", \"IATA_CODE_Reporting_Airline\"])\n    .filter(filter_expr)\n    .sort(\"DepTime\")\n    .group_by_dynamic(\n        \"DepTime\",\n        every=\"1h\",\n        by=\"IATA_CODE_Reporting_Airline\")\n    .agg(pl.col(\"Flight_Number_Reporting_Airline\").count())\n    .pivot(\n        index=\"DepTime\",\n        columns=\"IATA_CODE_Reporting_Airline\",\n        values=\"Flight_Number_Reporting_Airline\",\n    )\n    .sort(\"DepTime\")\n    # fill every missing hour with 0 so the plot looks better\n    .upsample(time_column=\"DepTime\", every=\"1h\")\n    .fill_null(0)\n    .select([pl.col(\"DepTime\"), pl.col(pl.UInt32).rolling_sum(24)])\n    .to_pandas()\n    .set_index(\"DepTime\")\n    .rename_axis(\"Flights per Day\", axis=1)\n    .plot()\n)\n\n<Axes: xlabel='DepTime'>\n\n\n\n\n\n\n\n\n(\n    df_pd\n    .dropna(subset=[\"DepTime\", \"IATA_CODE_Reporting_Airline\"])\n    # filter for the busiest airlines\n    .loc[\n        lambda x: x[\"IATA_CODE_Reporting_Airline\"].isin(\n            x[\"IATA_CODE_Reporting_Airline\"].value_counts().index[:5]\n        )\n    ]\n    .assign(\n        IATA_CODE_Reporting_Airline=lambda x: x[\n            \"IATA_CODE_Reporting_Airline\"\n        ].cat.remove_unused_categories()  #  annoying pandas behaviour\n    )\n    .set_index(\"DepTime\")\n    # TimeGrouper to resample & groupby at once\n    .groupby([\"IATA_CODE_Reporting_Airline\", pd.Grouper(freq=\"H\")])[\n        \"Flight_Number_Reporting_Airline\"\n    ]\n    .count()\n    # the .pivot takes care of this in the Polars code.\n    .unstack(0)\n    .fillna(0)\n    .rolling(24)\n    .sum()\n    .rename_axis(\"Flights per Day\", axis=1)\n    .plot()\n)\n\n/tmp/ipykernel_13274/4223446110.py:17: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  .groupby([\"IATA_CODE_Reporting_Airline\", pd.Grouper(freq=\"H\")])[\n\n\n<Axes: xlabel='DepTime'>\n\n\n\n\n\n\n\n\nDifferences between Polars and Pandas:\n\nTo group by a time window and another value, we use .groupby_dynamic. In Pandas we use .groupby with the pd.Grouper helper.\nInstead of .rolling(n).sum(), Polars has .rolling_sum(n).\nIf you see Pandas code using .unstack, the corresponding Polars code probably needs .pivot.\nIn Polars, .value_counts returns a pl.Struct column containing the value and the value count. In Pandas it returns a series where the elements are the value counts and the index contains the values themselves.\nIn Polars we need to select all the UInt32 cols at one point using pl.col(pl.UInt32). In Pandas, the way .rolling works means we don‚Äôt need to select these cols explicitly, but if we did it would look like df.select_dtypes(\"uint32\").\n\n\n\n2.4.2 Planes With Multiple Daily Flights\nNow let‚Äôs see if planes with multiple flights per day tend to get delayed as the day goes on:\n\nPolarsPandas\n\n\n\nflights_pl = (\n    df_pl.select(\n        pl.col([\n            \"FlightDate\",\n            \"Tail_Number\",\n            \"DepTime\",\n            \"DepDelay\"\n        ])\n    )\n    .drop_nulls()\n    .sort(\"DepTime\")\n    .filter(pl.col(\"DepDelay\") < 500)\n    .with_columns(\n        pl.col(\"DepTime\")\n        .rank()\n        .over([\"FlightDate\", \"Tail_Number\"])\n        .alias(\"turn\")\n    )\n)\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.boxplot(x=\"turn\", y=\"DepDelay\", data=flights_pl, ax=ax)\nax.set_ylim(-50, 50)\n\n(-50.0, 50.0)\n\n\n\n\n\n\n\n\nflights_pd = (\n    df_pd[[\n        \"FlightDate\",\n        \"Tail_Number\",\n        \"DepTime\",\n        \"DepDelay\"\n    ]]\n    .dropna()\n    .sort_values('DepTime')\n    .loc[lambda x: x[\"DepDelay\"] < 500]\n    .assign(turn = lambda x:\n        x.groupby([\"FlightDate\", \"Tail_Number\"])\n        [\"DepTime\"].transform('rank')\n        .astype(int)\n    )\n)\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.boxplot(x=\"turn\", y=\"DepDelay\", data=flights_pd, ax=ax)\nax.set_ylim(-50, 50)\n\n/tmp/ipykernel_13274/2848021590.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  x.groupby([\"FlightDate\", \"Tail_Number\"])\n\n\n(-50.0, 50.0)\n\n\n\n\n\n\n\n\nOne new thing here: window functions. When Pandas code looks like:\n.groupby(\"country\")[\"population\"].transform(\"sum\")\nthe equivalent Polars code will look like:\npl.col(\"population\").sum().over(\"country\")\n\n\n2.4.3 Delay by hour of the day\nMaybe later flights have longer delays:\n\nPolarsPandas\n\n\n\nplt.figure(figsize=(10, 5))\n(\n    df_pl.select(\n        pl.col(\n            [\"FlightDate\", \"Tail_Number\", \"DepTime\", \"DepDelay\"],\n        )\n    )\n    .drop_nulls()\n    .filter(pl.col(\"DepDelay\").is_between(5, 600, closed=\"none\"))\n    .with_columns(pl.col(\"DepTime\").dt.hour().alias(\"hour\"))\n    .to_pandas()\n    .pipe((sns.boxplot, \"data\"), x=\"hour\", y=\"DepDelay\")\n)\n\n<Axes: xlabel='hour', ylabel='DepDelay'>\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\n(\n    df_pd[[\"FlightDate\", \"Tail_Number\", \"DepTime\", \"DepDelay\"]]\n    .dropna()\n    .loc[lambda df: df[\"DepDelay\"].between(5, 600, inclusive=\"neither\")]\n    .assign(hour=lambda df: df[\"DepTime\"].dt.hour)\n    .pipe((sns.boxplot, \"data\"), x=\"hour\", y=\"DepDelay\")\n)\n\n<Axes: xlabel='hour', ylabel='DepDelay'>"
  },
  {
    "objectID": "method_chaining.html#how-much-is-too-much",
    "href": "method_chaining.html#how-much-is-too-much",
    "title": "2¬† Method Chaining",
    "section": "2.5 How much is too much?",
    "text": "2.5 How much is too much?\nThe above examples have some fairly long method chains that could perhaps be split up. That said, if I saw them in a PR I probably wouldn‚Äôt mind. Here‚Äôs a chain that‚Äôs too long:\n\nI would argue this code is not hard to follow, but empirically it provokes disgust in people. I think it‚Äôs just visually shocking, which is often a hallmark of terrible code. That‚Äôs enough reason to avoid doing this. You don‚Äôt want folks assuming you‚Äôve lost your mind.\nIn the above example, it would probably be enough to just move those long lists to their own variables. That would make it more obvious that the code is really doing simple, menial things."
  },
  {
    "objectID": "method_chaining.html#summary",
    "href": "method_chaining.html#summary",
    "title": "2¬† Method Chaining",
    "section": "2.6 Summary",
    "text": "2.6 Summary\n\nMethod chaining is great but use it with care.\nThe Polars code looks pretty good here next to Pandas. It‚Äôs less clunky and more conducive to fluent programming. I‚Äôve noticed some people are under the impression that you should only use Polars when you need the performance, but I would argue the above examples show that it‚Äôs better for small data too.\nIf you want to translate Pandas code to Polars, pay attention to the examples above and the accompanying notes on the API differences."
  },
  {
    "objectID": "performance.html#six-fairly-obvious-performance-rules",
    "href": "performance.html#six-fairly-obvious-performance-rules",
    "title": "3¬† Performance",
    "section": "3.1 Six fairly obvious performance rules",
    "text": "3.1 Six fairly obvious performance rules\nHere are some tips that are almost always a good idea:\n\nUse the lazy API.\nUse Exprs, and don‚Äôt use .apply unless you really have to.\nUse the smallest necessary numeric types (so if you have an integer between 0 and 255, use pl.UInt8, not pl.Int64). This will save both time and space.\nUse efficient storage (if you‚Äôre dumping stuff in files, Parquet is a good choice).\nUse categoricals for recurring strings (but note that it may not be worth it if there‚Äôs not much repetition).\nOnly select the columns you need.\n\n\n\n\n\n\n\nTip\n\n\n\nIf your colleagues are happy with CSVs and can‚Äôt be convinced to use something else, tell them that the Modern Polars book says they should feel bad.\n\n\nThese are basically the same rules you‚Äôd follow when using Pandas, except for the one about the lazy API. Now for some comparisons between the performance of idiomatic Pandas and Polars."
  },
  {
    "objectID": "performance.html#polars-is-faster-at-the-boring-stuff",
    "href": "performance.html#polars-is-faster-at-the-boring-stuff",
    "title": "3¬† Performance",
    "section": "3.2 Polars is faster at the boring stuff",
    "text": "3.2 Polars is faster at the boring stuff\nHere we‚Äôll clean up a messy dataset, kindly provided by Kaggle user Rachit Toshniwal as a deliberate example of a really crap CSV. Most of the cleanup involves extracting numeric data from awkward strings.\nAlso, the data is too small so I‚Äôve concatenated it to itself 20 times. We‚Äôre not doing anything that will care about the duplication. Here‚Äôs how the raw table looks:\n\n\nCode\nimport pandas as pd\npd.read_csv(\"../data/fifa21_raw_big.csv\", dtype=\"string\", nrows=2)\n\n\n\n\n\n\n  \n    \n      \n      ID\n      Name\n      LongName\n      photoUrl\n      playerUrl\n      Nationality\n      Age\n      ‚ÜìOVA\n      POT\n      Club\n      ...\n      A/W\n      D/W\n      IR\n      PAC\n      SHO\n      PAS\n      DRI\n      DEF\n      PHY\n      Hits\n    \n  \n  \n    \n      0\n      158023\n      L. Messi\n      Lionel Messi\n      https://cdn.sofifa.com/players/158/023/21_60.png\n      http://sofifa.com/player/158023/lionel-messi/2...\n      Argentina\n      33\n      93\n      93\n      FC Barcelona\n      ...\n      Medium\n      Low\n      5 ‚òÖ\n      85\n      92\n      91\n      95\n      38\n      65\n      771\n    \n    \n      1\n      20801\n      Cristiano Ronaldo\n      C. Ronaldo dos Santos Aveiro\n      https://cdn.sofifa.com/players/020/801/21_60.png\n      http://sofifa.com/player/20801/c-ronaldo-dos-s...\n      Portugal\n      35\n      92\n      92\n      Juventus\n      ...\n      High\n      Low\n      5 ‚òÖ\n      89\n      93\n      81\n      89\n      35\n      77\n      562\n    \n  \n\n2 rows √ó 77 columns\n\n\n\nFor this exercise we‚Äôll assume we want to make use of all the columns. First some boilerplate where we map out the different data types:\n\n\nCode\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nimport math\nstr_cols = [\n    \"Name\",\n    \"LongName\",\n    \"playerUrl\",\n    \"photoUrl\",\n]\ninitial_category_cols_pl = [\n    \"Nationality\",\n    \"Preferred Foot\",\n    \"Best Position\",\n    \"A/W\",\n    \"D/W\"\n]\ncategory_cols = [*initial_category_cols_pl, \"Club\"]\ndate_cols = [\n    \"Joined\",\n    \"Loan Date End\"\n]\n# these all start with the euro symbol and end with 0, M or K\nmoney_cols = [\n    \"Value\",\n    \"Wage\",\n    \"Release Clause\"\n]\nstar_cols = [\n    \"W/F\",\n    \"SM\",\n    \"IR\",\n]\n# Contract col is a range of years\n# Positions is a list of positions\n# Height is in cm\n# Weight is in kg\n# Hits is numbers with K and M \nmessy_cols = [\n    \"Contract\",\n    \"Positions\",\n    \"Height\",\n    \"Weight\",\n    \"Hits\"\n]\ninitially_str_cols = str_cols + date_cols + money_cols + star_cols + messy_cols\ninitially_str_cols_pl = [*initially_str_cols, \"Club\"]\nu32_cols = [\n    \"ID\",\n    \"Total Stats\"\n]\nu8_cols = [\n    'Age',\n    '‚ÜìOVA',\n    'POT',\n    'BOV',\n    'Crossing',\n    'Finishing',\n    'Heading Accuracy',\n    'Short Passing',\n    'Volleys',\n    'Dribbling',\n    'Curve',\n    'FK Accuracy',\n    'Long Passing',\n    'Ball Control',\n    'Acceleration',\n    'Sprint Speed',\n    'Agility',\n    'Reactions',\n    'Balance',\n    'Shot Power',\n    'Jumping',\n    'Stamina',\n    'Strength',\n    'Long Shots',\n    'Aggression',\n    'Interceptions',\n    'Positioning',\n    'Vision',\n    'Penalties',\n    'Composure',\n    'Marking',\n    'Standing Tackle',\n    'Sliding Tackle',\n    'GK Diving',\n    'GK Handling',\n    'GK Kicking',\n    'GK Positioning',\n    'GK Reflexes',\n    'PAC',\n    'SHO',\n    'PAS',\n    'DRI',\n    'DEF',\n    'PHY'\n]\n\nu16_cols = [\n    'Attacking',\n    'Skill',\n    'Movement',\n    'Power',\n    'Mentality',\n    'Defending',\n    'Goalkeeping',\n    'Total Stats',\n    'Base Stats'\n]\n\n\n\n3.2.1 Dtypes\nHere are the initial dtypes for the two dataframes:\n\nPolarsPandas\n\n\n\n# can't use UInt8/16 in scan_csv\ndtypes_pl = (\n    {col: pl.Utf8 for col in initially_str_cols_pl}\n    | {col: pl.Categorical for col in initial_category_cols_pl}\n    | {col: pl.UInt32 for col in [*u32_cols, *u16_cols, *u8_cols]}\n)\n\n\n\n\ndtypes_pd = (\n    {col: pd.StringDtype() for col in initially_str_cols}\n    | {col: pd.CategoricalDtype() for col in category_cols}\n    | {col: \"uint32\" for col in u32_cols}\n    | {col: \"uint8\" for col in u8_cols}\n    | {col: \"uint16\" for col in u16_cols}\n)\n\n\n\n\nOne thing I‚Äôll note here is that Pandas numeric types are somewhat confusing: \"uint32\" means np.uint32 which is not the same thing as pd.UInt32Dtype(). Only the latter is nullable. On the other hand, Polars has just one unsigned 32-bit integer type, and it‚Äôs nullable.\n\n\n\n\n\n\nTip\n\n\n\nPolars expressions have a shrink_dtype method that can be more convenient than manually specifying the dtypes yourself. It‚Äôs not magic though, and it has to spend time finding the min and max of the column.\n\n\n\n\n3.2.2 Data cleaning\nThere‚Äôs not much that you haven‚Äôt seen here already, so we won‚Äôt explain the code line by line. The main new thing here is pl.when for ternary expressions.\n\nPolarsPandas\n\n\n\ndef parse_date_pl(col: pl.Expr) -> pl.Expr:\n    return col.str.strptime(pl.Date, format=\"%b %d, %Y\")\n\ndef parse_suffixed_num_pl(col: pl.Expr) -> pl.Expr:\n    suffix = col.str.slice(-1, 1)\n    suffix_value = (\n        pl.when(suffix == \"K\")\n        .then(1_000)\n        .when(suffix == \"M\")\n        .then(1_000_000)\n        .otherwise(1)\n        .cast(pl.UInt32)\n    )\n    without_suffix = (\n        col\n        .str.replace(\"K\", \"\", literal=True)\n        .str.replace(\"M\", \"\", literal=True)\n        .cast(pl.Float32)\n    )\n    original_name = col.meta.output_name()\n    return (suffix_value * without_suffix).alias(original_name)\n\ndef parse_money_pl(col: pl.Expr) -> pl.Expr:\n    return parse_suffixed_num_pl(col.str.slice(1)).cast(pl.UInt32)\n\ndef parse_star_pl(col: pl.Expr) -> pl.Expr:\n    return col.str.slice(0, 1).cast(pl.UInt8)\n\ndef feet_to_cm_pl(col: pl.Expr) -> pl.Expr:\n    feet_inches_split = col.str.split_exact(\"'\", 1)\n    total_inches = (\n        (feet_inches_split.struct.field(\"field_0\").cast(pl.UInt8, strict=False) * 12)\n        + feet_inches_split.struct.field(\"field_1\").str.strip_chars_end('\"').cast(pl.UInt8, strict=False)\n    )\n    return (total_inches * 2.54).round(0).cast(pl.UInt8)\n\ndef parse_height_pl(col: pl.Expr) -> pl.Expr:\n    is_cm = col.str.ends_with(\"cm\")\n    return (\n        pl.when(is_cm)\n        .then(col.str.slice(0, 3).cast(pl.UInt8, strict=False))\n        .otherwise(feet_to_cm_pl(col))\n    )\n\ndef parse_weight_pl(col: pl.Expr) -> pl.Expr:\n    is_kg = col.str.ends_with(\"kg\")\n    without_unit = col.str.extract(r\"(\\d+)\").cast(pl.UInt8)\n    return (\n        pl.when(is_kg)\n        .then(without_unit)\n        .otherwise((without_unit * 0.453592).round(0).cast(pl.UInt8))\n    )\n\ndef parse_contract_pl(col: pl.Expr) -> list[pl.Expr]:\n    contains_tilde = col.str.contains(\" ~ \", literal=True)\n    loan_str = \" On Loan\"\n    loan_col = col.str.ends_with(loan_str)\n    split = (\n        pl.when(contains_tilde)\n        .then(col)\n        .otherwise(None)\n        .str.split_exact(\" ~ \", 1)\n    )\n    start = split.struct.field(\"field_0\").cast(pl.UInt16).alias(\"contract_start\")\n    end = split.struct.field(\"field_1\").cast(pl.UInt16).alias(\"contract_end\")\n    free_agent = (col == \"Free\").alias(\"free_agent\").fill_null(False)\n    loan_date = (\n        pl.when(loan_col)\n        .then(col)\n        .otherwise(None)\n        .str.split_exact(\" On Loan\", 1)\n        .struct.field(\"field_0\")\n        .alias(\"loan_date_start\")\n    )\n    return [start, end, free_agent, parse_date_pl(loan_date)]\n\n\n\n\ndef parse_date_pd(col: pd.Series) -> pd.Series:\n    return pd.to_datetime(col, format=\"%b %d, %Y\")\n\ndef parse_suffixed_num_pd(col: pd.Series) -> pd.Series:\n    suffix_value = (\n        col\n        .str[-1]\n        .map({\"K\": 1_000, \"M\": 1_000_000})\n        .fillna(1)\n        .astype(\"uint32\")\n    )\n    without_suffix = (\n        col\n        .str.replace(\"K\", \"\", regex=False)\n        .str.replace(\"M\", \"\", regex=False)\n        .astype(\"float\")\n    )\n    return suffix_value * without_suffix\n\ndef parse_money_pd(col: pd.Series) -> pd.Series:\n    return parse_suffixed_num_pd(col.str[1:]).astype(\"uint32\")\n\ndef parse_star_pd(col: pd.Series) -> pd.Series:\n    return col.str[0].astype(\"uint8\")\n\ndef feet_to_cm_pd(col: pd.Series) -> pd.Series:\n    feet_inches_split = col.str.split(\"'\", expand=True)\n    total_inches = (\n        feet_inches_split[0].astype(\"uint8\").mul(12)\n        + feet_inches_split[1].str[:-1].astype(\"uint8\")\n    )\n    return total_inches.mul(2.54).round().astype(\"uint8\")\n\ndef parse_height_pd(col: pd.Series) -> pd.Series:\n    is_cm = col.str.endswith(\"cm\")\n    cm_values = col.loc[is_cm].str[:-2].astype(\"uint8\")\n    inches_as_cm = feet_to_cm_pd(col.loc[~is_cm])\n    return pd.concat([cm_values, inches_as_cm])\n\ndef parse_weight_pd(col: pd.Series) -> pd.Series:\n    is_kg = col.str.endswith(\"kg\")\n    without_unit = col.where(is_kg, col.str[:-3]).mask(is_kg, col.str[:-2]).astype(\"uint8\")\n    return without_unit.where(is_kg, without_unit.mul(0.453592).round().astype(\"uint8\"))\n\ndef parse_contract_pd(df: pd.DataFrame) -> pd.DataFrame:\n    contract_col = df[\"Contract\"]\n    contains_tilde = contract_col.str.contains(\" ~ \", regex=False)\n    split = (\n        contract_col.loc[contains_tilde].str.split(\" ~ \", expand=True).astype(pd.UInt16Dtype())\n    )\n    split.columns = [\"contract_start\", \"contract_end\"]\n    not_tilde = contract_col.loc[~contains_tilde]\n    free_agent = (contract_col == \"Free\").rename(\"free_agent\").fillna(False)\n    loan_date = parse_date_pd(not_tilde.loc[~free_agent].str[:-8]).rename(\"loan_date_start\")\n    return pd.concat([df.drop(\"Contract\", axis=1), split, free_agent, loan_date], axis=1)\n\n\n\n\n\n\n3.2.3 Performance comparison\nIn this example, Polars is ~150x faster than Pandas:\n\nPolarsPandas\n\n\n\n%%time\nnew_cols_pl = ([\n    pl.col(\"Club\").str.strip_chars().cast(pl.Categorical),\n    parse_suffixed_num_pl(pl.col(\"Hits\")).cast(pl.UInt32),\n    pl.col(\"Positions\").str.split(\",\"),\n    parse_height_pl(pl.col(\"Height\")),\n    parse_weight_pl(pl.col(\"Weight\")),\n]\n+ [parse_date_pl(pl.col(col)) for col in date_cols]\n+ [parse_money_pl(pl.col(col)) for col in money_cols]\n+ [parse_star_pl(pl.col(col)) for col in star_cols]\n+ parse_contract_pl(pl.col(\"Contract\"))\n+ [pl.col(col).cast(pl.UInt16) for col in u16_cols]\n+ [pl.col(col).cast(pl.UInt8) for col in u8_cols]\n)\nfifa_pl = (\n    pl.scan_csv(\"../data/fifa21_raw_v2.csv\", dtypes=dtypes_pl)\n    .with_columns(new_cols_pl)\n    .drop(\"Contract\")\n    .rename({\"‚ÜìOVA\": \"OVA\"})\n    .collect()\n)\n\nCPU times: user 120 ms, sys: 30.3 ms, total: 151 ms\nWall time: 50.3 ms\n\n\n\n\n\n%%time\nfifa_pd = (\n    pd.read_csv(\"../data/fifa21_raw_big.csv\", dtype=dtypes_pd)\n    .assign(Club=lambda df: df[\"Club\"].cat.rename_categories(lambda c: c.strip()),\n        **{col: lambda df: parse_date_pd(df[col]) for col in date_cols},\n        **{col: lambda df: parse_money_pd(df[col]) for col in money_cols},\n        **{col: lambda df: parse_star_pd(df[col]) for col in star_cols},\n        Hits=lambda df: parse_suffixed_num_pd(df[\"Hits\"]).astype(pd.UInt32Dtype()),\n        Positions=lambda df: df[\"Positions\"].str.split(\",\"),\n        Height=lambda df: parse_height_pd(df[\"Height\"]),\n        Weight=lambda df: parse_weight_pd(df[\"Weight\"])\n    )\n    .pipe(parse_contract_pd)\n    .rename(columns={\"‚ÜìOVA\": \"OVA\"})\n)\n\nCPU times: user 6.1 s, sys: 318 ms, total: 6.42 s\nWall time: 6.59 s\n\n\n\n\n\nOutput:\n\nPolarsPandas\n\n\n\nfifa_pl.head()\n\n\n\nshape: (5, 80)IDNameLongNamephotoUrlplayerUrlNationalityAgeOVAPOTClubPositionsHeightWeightPreferred FootBOVBest PositionJoinedLoan Date EndValueWageRelease ClauseAttackingCrossingFinishingHeading AccuracyShort PassingVolleysSkillDribblingCurveFK AccuracyLong PassingBall ControlMovementAccelerationSprint SpeedAgility‚Ä¶StrengthLong ShotsMentalityAggressionInterceptionsPositioningVisionPenaltiesComposureDefendingMarkingStanding TackleSliding TackleGoalkeepingGK DivingGK HandlingGK KickingGK PositioningGK ReflexesTotal StatsBase StatsW/FSMA/WD/WIRPACSHOPASDRIDEFPHYHitscontract_startcontract_endfree_agentloan_date_startu32strstrstrstrcatu8u8u8catlist[str]u8u8catu8catdatedateu32u32u32u16u8u8u8u8u8u16u8u8u8u8u8u16u8u8u8‚Ä¶u8u8u16u8u8u8u8u8u8u16u8u8u8u16u8u8u8u8u8u16u16u8u8catcatu8u8u8u8u8u8u8u32u16u16booldate158023\"L. Messi\"\"Lionel Messi\"\"https://cdn.so‚Ä¶\"http://sofifa.‚Ä¶\"Argentina\"339393\"FC Barcelona\"[\"RW\", \" ST\", \" CF\"]17072\"Left\"93\"RW\"2004-07-01null10350000056000013839999342985957091884709693949196451918091‚Ä¶6994347444093957596913235245461115148223146644\"Medium\"\"Low\"585929195386577120042021falsenull20801\"Cristiano Rona‚Ä¶\"C. Ronaldo dos‚Ä¶\"https://cdn.so‚Ä¶\"http://sofifa.‚Ä¶\"Portugal\"359292\"Juventus\"[\"ST\", \" LW\"]18783\"Right\"92\"ST\"2018-07-10null630000002200007590000143784959082864148881767792431879187‚Ä¶78933536329958284958428322458711151411222146445\"High\"\"Low\"589938189357756220182022falsenull200389\"J. Oblak\"\"Jan Oblak\"\"https://cdn.so‚Ä¶\"http://sofifa.‚Ä¶\"Slovenia\"279193\"Atl√©tico Madri‚Ä¶[\"GK\"]18887\"Right\"91\"GK\"2014-07-16null1200000001250001593999939513111543131091213144030307436067‚Ä¶7812140341911651168572712184378792789090141348931\"Medium\"\"Medium\"387927890529015020142023falsenull192985\"K. De Bruyne\"\"Kevin De Bruyn‚Ä¶\"https://cdn.so‚Ä¶\"http://sofifa.‚Ä¶\"Belgium\"299191\"Manchester Cit‚Ä¶[\"CAM\", \" CM\"]18170\"Right\"91\"CAM\"2015-08-30null12900000037000016100000040794825594824418885839392398777678‚Ä¶749140876668894849118668655356151351013230448554\"High\"\"High\"476869388647820720152023falsenull190871\"Neymar Jr\"\"Neymar da Silv‚Ä¶\"https://cdn.so‚Ä¶\"http://sofifa.‚Ä¶\"Brazil\"289191\"Paris Saint-Ge‚Ä¶[\"LW\", \" CAM\"]17568\"Right\"91\"LW\"2017-08-03null13200000027000016650000040885876287874489588898195453948996‚Ä¶5084356513687909293943530295999151511217545155\"High\"\"Medium\"591858694365959520172022falsenull\n\n\n\n\n\nfifa_pd.head()\n\n\n\n\n\n  \n    \n      \n      ID\n      Name\n      LongName\n      photoUrl\n      playerUrl\n      Nationality\n      Age\n      OVA\n      POT\n      Club\n      ...\n      SHO\n      PAS\n      DRI\n      DEF\n      PHY\n      Hits\n      contract_start\n      contract_end\n      free_agent\n      loan_date_start\n    \n  \n  \n    \n      0\n      158023\n      L. Messi\n      Lionel Messi\n      https://cdn.sofifa.com/players/158/023/21_60.png\n      http://sofifa.com/player/158023/lionel-messi/2...\n      Argentina\n      33\n      93\n      93\n      FC Barcelona\n      ...\n      92\n      91\n      95\n      38\n      65\n      771\n      2004\n      2021\n      False\n      NaT\n    \n    \n      1\n      20801\n      Cristiano Ronaldo\n      C. Ronaldo dos Santos Aveiro\n      https://cdn.sofifa.com/players/020/801/21_60.png\n      http://sofifa.com/player/20801/c-ronaldo-dos-s...\n      Portugal\n      35\n      92\n      92\n      Juventus\n      ...\n      93\n      81\n      89\n      35\n      77\n      562\n      2018\n      2022\n      False\n      NaT\n    \n    \n      2\n      200389\n      J. Oblak\n      Jan Oblak\n      https://cdn.sofifa.com/players/200/389/21_60.png\n      http://sofifa.com/player/200389/jan-oblak/210006/\n      Slovenia\n      27\n      91\n      93\n      Atl√©tico Madrid\n      ...\n      92\n      78\n      90\n      52\n      90\n      150\n      2014\n      2023\n      False\n      NaT\n    \n    \n      3\n      192985\n      K. De Bruyne\n      Kevin De Bruyne\n      https://cdn.sofifa.com/players/192/985/21_60.png\n      http://sofifa.com/player/192985/kevin-de-bruyn...\n      Belgium\n      29\n      91\n      91\n      Manchester City\n      ...\n      86\n      93\n      88\n      64\n      78\n      207\n      2015\n      2023\n      False\n      NaT\n    \n    \n      4\n      190871\n      Neymar Jr\n      Neymar da Silva Santos Jr.\n      https://cdn.sofifa.com/players/190/871/21_60.png\n      http://sofifa.com/player/190871/neymar-da-silv...\n      Brazil\n      28\n      91\n      91\n      Paris Saint-Germain\n      ...\n      85\n      86\n      94\n      36\n      59\n      595\n      2017\n      2022\n      False\n      NaT\n    \n  \n\n5 rows √ó 80 columns\n\n\n\n\n\n\nYou could play around with the timings here and even try the .profile method to see what Polars spends its time on. In this scenario the speed advantage of Polars likely comes down to three things:\n\nIt is much faster at reading CSVs.\nIt is much faster at processing strings.\nIt can select/assign columns in parallel."
  },
  {
    "objectID": "performance.html#numpy-can-make-polars-faster",
    "href": "performance.html#numpy-can-make-polars-faster",
    "title": "3¬† Performance",
    "section": "3.3 NumPy can make Polars faster",
    "text": "3.3 NumPy can make Polars faster\nPolars gets along well with NumPy ufuncs, even in lazy mode (which is interesting because NumPy has no lazy API). Let‚Äôs see how this looks by calculating the great-circle distance between a bunch of coordinates.\n\n3.3.1 Get the data\nWe create a lazy dataframe containing pairs of airports and their coordinates:\n\nairports = pl.scan_csv(\"../data/airports.csv\").drop_nulls().unique(subset=[\"AIRPORT\"])\npairs = airports.join(airports, on=\"AIRPORT\", how=\"cross\").filter(\n    (pl.col(\"AIRPORT\") != pl.col(\"AIRPORT_right\"))\n    & (pl.col(\"LATITUDE\") != pl.col(\"LATITUDE_right\"))\n    & (pl.col(\"LONGITUDE\") != pl.col(\"LONGITUDE_right\"))\n)\n\n\n\n3.3.2 Calculate great-circle distance\nOne use case for NumPy ufuncs is doing computations that Polars expressions don‚Äôt support. In this example Polars can do everything we need, though the ufunc version ends up being slightly faster:\n\nPolarsNumPy\n\n\n\ndef deg2rad_pl(degrees: pl.Expr) -> pl.Expr:\n    return degrees * math.pi / 180\n\ndef gcd_pl(lat1: pl.Expr, lng1: pl.Expr, lat2: pl.Expr, lng2: pl.Expr):\n    œï1 = deg2rad_pl(90 - lat1)\n    œï2 = deg2rad_pl(90 - lat2)\n\n    Œ∏1 = deg2rad_pl(lng1)\n    Œ∏2 = deg2rad_pl(lng2)\n\n    cos = œï1.sin() * œï2.sin() * (Œ∏1 - Œ∏2).cos() + œï1.cos() * œï2.cos()\n    arc = cos.arccos()\n    return arc * 6373\n\n\n\n\ndef gcd_np(lat1, lng1, lat2, lng2):\n    œï1 = np.deg2rad(90 - lat1)\n    œï2 = np.deg2rad(90 - lat2)\n\n    Œ∏1 = np.deg2rad(lng1)\n    Œ∏2 = np.deg2rad(lng2)\n\n    cos = np.sin(œï1) * np.sin(œï2) * np.cos(Œ∏1 - Œ∏2) + np.cos(œï1) * np.cos(œï2)\n    arc = np.arccos(cos)\n    return arc * 6373\n\n\n\n\nWe can pass Polars expressions directly to our gcd_np function, which is pretty nice since these things don‚Äôt even store the data themselves:\n\n%%timeit\npairs.select(\n    gcd_np(\n        pl.col(\"LATITUDE\"),\n        pl.col(\"LONGITUDE\"),\n        pl.col(\"LATITUDE_right\"),\n        pl.col(\"LONGITUDE_right\")\n    )\n).collect()\n\n6.31 s ¬± 198 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\nOn my machine the NumPy version tends to be 5-20% faster than the pure Polars version:\n\n%%timeit\npairs.select(\n    gcd_pl(\n        pl.col(\"LATITUDE\"),\n        pl.col(\"LONGITUDE\"),\n        pl.col(\"LATITUDE_right\"),\n        pl.col(\"LONGITUDE_right\")\n    )\n).collect()\n\n5.09 s ¬± 77.5 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\nThis may not be a huge performance difference, but it at least means you don‚Äôt sacrifice speed when relying on NumPy. There are some gotchas though so watch out for those.\nAlso watch out for .to_numpy() - you don‚Äôt always need to call this and it can slow things down:\n\n%%timeit\ncollected = pairs.collect()\ngcd_np(\n    collected[\"LATITUDE\"].to_numpy(),\n    collected[\"LONGITUDE\"].to_numpy(),\n    collected[\"LATITUDE_right\"].to_numpy(),\n    collected[\"LONGITUDE_right\"].to_numpy()\n)\n\n6 s ¬± 375 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "performance.html#polars-can-be-slower-than-pandas-sometimes-maybe",
    "href": "performance.html#polars-can-be-slower-than-pandas-sometimes-maybe",
    "title": "3¬† Performance",
    "section": "3.4 Polars can be slower than Pandas sometimes, maybe",
    "text": "3.4 Polars can be slower than Pandas sometimes, maybe\nHere‚Äôs an example where we calculate z-scores, using window functions in Polars and using groupby-transform in Pandas:\n\ndef create_frame(n, n_groups):\n    return pl.DataFrame(\n        {\"name\": np.random.randint(0, n_groups, size=n), \"value2\": np.random.randn(n)}\n    )\n\ndef pandas_transform(df: pd.DataFrame) -> pd.DataFrame:\n    g = df.groupby(\"name\")[\"value2\"]\n    v = df[\"value2\"]\n    return (v - g.transform(\"mean\")) / g.transform(\"std\")\n\n\ndef polars_transform() -> pl.Expr:\n    v = pl.col(\"value2\")\n    return (v - v.mean().over(\"name\")) / v.std().over(\"name\")\n\nrand_df_pl = create_frame(50_000_000, 50_000)\nrand_df_pd = rand_df_pl.to_pandas()\n\nThe Polars version tends to be 10-100% slower on my machine:\n\nPolarsPandas\n\n\n\n%timeit rand_df_pl.select(polars_transform())\n\n2.49 s ¬± 167 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\n\n\n%timeit pandas_transform(rand_df_pd)\n\n1.83 s ¬± 7.6 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\n\n\nThis example isn‚Äôt telling you to use Pandas in this specific situation. Once you add in the time spent reading a file, Polars likely wins.\nAnd even here, if you sort by the name col, Polars wins again. It has fast-track algorithms for sorted data."
  },
  {
    "objectID": "performance.html#summary",
    "href": "performance.html#summary",
    "title": "3¬† Performance",
    "section": "3.5 Summary",
    "text": "3.5 Summary\n\nPolars is really fast. Pandas was already respectably fast and Polars wipes the floor with it.\nYou can still make Polars slow if you do silly things with it, but compared to Pandas it‚Äôs easier to do the right thing in the first place.\nPolars works well with NumPy ufuncs.\nThere are still some situations where Pandas can be faster. They are probably not compelling, but we shouldn‚Äôt pretend they don‚Äôt exist."
  },
  {
    "objectID": "tidy.html#get-the-data",
    "href": "tidy.html#get-the-data",
    "title": "4¬† Reshaping and Tidy Data",
    "section": "4.1 Get the data",
    "text": "4.1 Get the data\n\nfrom pathlib import Path\nimport polars as pl\nimport pandas as pd\n\npl.Config.set_tbl_rows(5)\npd.options.display.max_rows = 5\n\nnba_dir = Path(\"../data/nba/\")\n\ncolumn_names = {\n    \"Date\": \"date\",\n    \"Visitor/Neutral\": \"away_team\",\n    \"PTS\": \"away_points\",\n    \"Home/Neutral\": \"home_team\",\n    \"PTS.1\": \"home_points\",\n}\n\nif not nba_dir.exists():\n    nba_dir.mkdir()\n    for month in (\n        \"october\",\n        \"november\",\n        \"december\",\n        \"january\",\n        \"february\",\n        \"march\",\n        \"april\",\n        \"may\",\n        \"june\",\n    ):\n        # In practice we would do more data cleaning here, and save to parquet not CSV.\n        # But we save messy data here so we can clean it later for pedagogical purposes.\n        url = f\"http://www.basketball-reference.com/leagues/NBA_2016_games-{month}.html\"\n        tables = pd.read_html(url)\n        raw = (\n            pl.from_pandas(tables[0].query(\"Date != 'Playoffs'\"))\n            .rename(column_names)\n            .select(column_names.values())\n        )\n        raw.write_csv(nba_dir / f\"{month}.csv\")\n\nnba_glob = nba_dir / \"*.csv\"\npl.scan_csv(nba_glob).head().collect()\n\n\n\nshape: (5, 5)dateaway_teamaway_pointshome_teamhome_pointsstrstri64stri64\"Fri, Apr 1, 20‚Ä¶\"Philadelphia 7‚Ä¶91\"Charlotte Horn‚Ä¶100\"Fri, Apr 1, 20‚Ä¶\"Dallas Maveric‚Ä¶98\"Detroit Piston‚Ä¶89\"Fri, Apr 1, 20‚Ä¶\"Brooklyn Nets\"91\"New York Knick‚Ä¶105\"Fri, Apr 1, 20‚Ä¶\"Cleveland Cava‚Ä¶110\"Atlanta Hawks\"108\"Fri, Apr 1, 20‚Ä¶\"Toronto Raptor‚Ä¶99\"Memphis Grizzl‚Ä¶95"
  },
  {
    "objectID": "tidy.html#cleaning",
    "href": "tidy.html#cleaning",
    "title": "4¬† Reshaping and Tidy Data",
    "section": "4.2 Cleaning üßπ",
    "text": "4.2 Cleaning üßπ\nNothing super interesting here:\n\nPolarsPandas\n\n\n\ngames_pl = (\n    pl.scan_csv(nba_glob)\n    .with_columns(\n        pl.col(\"date\").str.strptime(pl.Date, \"%a, %b %d, %Y\"),\n    )\n    .sort(\"date\")\n    .with_row_count(\"game_id\")\n)\ngames_pl.head().collect()\n\n\n\nshape: (5, 6)game_iddateaway_teamaway_pointshome_teamhome_pointsu32datestri64stri6402015-10-27\"Cleveland Cava‚Ä¶95\"Chicago Bulls\"9712015-10-27\"Detroit Piston‚Ä¶106\"Atlanta Hawks\"9422015-10-27\"New Orleans Pe‚Ä¶95\"Golden State W‚Ä¶11132015-10-28\"Washington Wiz‚Ä¶88\"Orlando Magic\"8742015-10-28\"Philadelphia 7‚Ä¶95\"Boston Celtics‚Ä¶112\n\n\n\n\n\ngames_pd = (\n    pl.read_csv(nba_glob)\n    .to_pandas()\n    .dropna(how=\"all\")\n    .assign(date=lambda x: pd.to_datetime(x[\"date\"], format=\"%a, %b %d, %Y\"))\n    .sort_values(\"date\")\n    .reset_index(drop=True)\n    .set_index(\"date\", append=True)\n    .rename_axis([\"game_id\", \"date\"])\n    .sort_index()\n)\ngames_pd.head()\n\n\n\n\n\n  \n    \n      \n      \n      away_team\n      away_points\n      home_team\n      home_points\n    \n    \n      game_id\n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      2015-10-27\n      Cleveland Cavaliers\n      95\n      Chicago Bulls\n      97\n    \n    \n      1\n      2015-10-27\n      Detroit Pistons\n      106\n      Atlanta Hawks\n      94\n    \n    \n      2\n      2015-10-27\n      New Orleans Pelicans\n      95\n      Golden State Warriors\n      111\n    \n    \n      3\n      2015-10-28\n      Philadelphia 76ers\n      95\n      Boston Celtics\n      112\n    \n    \n      4\n      2015-10-28\n      Washington Wizards\n      88\n      Orlando Magic\n      87\n    \n  \n\n\n\n\n\n\n\nPolars does have a drop_nulls method but the only parameter it takes is subset, which ‚Äî like in Pandas ‚Äî lets you consider null values just for a subset of the columns. Pandas additionally lets you specify how=\"all\" to drop a row only if every value is null, but Polars drop_nulls has no such parameter and will drop the row if any values are null. If you only want to drop when all values are null, the docs recommend .filter(~pl.all(pl.all().is_null())).\n\n\n\n\n\n\nNote\n\n\n\nA previous version of the Polars example used pl.fold, which is for fast horizontal operations. It doesn‚Äôt come up anywhere else in this book, so consider this your warning that it exists."
  },
  {
    "objectID": "tidy.html#pivot-and-melt",
    "href": "tidy.html#pivot-and-melt",
    "title": "4¬† Reshaping and Tidy Data",
    "section": "4.3 Pivot and Melt",
    "text": "4.3 Pivot and Melt\nI recently came across someone who was doing advanced quantitative research in Python but had never heard of the Pandas .pivot method. I shudder to imagine the code he must have written in the absence of this knowledge, so here‚Äôs a simple explanation of pivoting and melting, lest anyone else suffer in ignorance. If you already know what pivot and melt are, feel free to scroll past this bit.\n\n4.3.1 Pivot\nSuppose you have a dataframe that looks like this:\n\n\nCode\nfrom datetime import date\nprices = pl.DataFrame({\n    \"date\": [*[date(2020, 1, 1)]*4, *[date(2020, 1, 2)]*4, *[date(2020, 1, 3)]*4],\n    \"ticker\": [*[\"AAPL\", \"TSLA\", \"MSFT\", \"NFLX\"]*3],\n    \"price\": [100, 200, 300, 400, 110, 220, 330, 420, 105, 210, 315, 440],\n})\nprices\n\n\n\n\nshape: (12, 3)datetickerpricedatestri642020-01-01\"AAPL\"1002020-01-01\"TSLA\"200‚Ä¶‚Ä¶‚Ä¶2020-01-03\"MSFT\"3152020-01-03\"NFLX\"440\n\n\nIn both Polars and Pandas you can call df.pivot(index=\"date\", values=\"price\", columns=\"ticker\") to get a dataframe that looks like this:\n\n\nCode\npivoted = prices.pivot(index=\"date\", values=\"price\", columns=\"ticker\")\npivoted\n\n\n\n\nshape: (3, 5)dateAAPLTSLAMSFTNFLXdatei64i64i64i642020-01-011002003004002020-01-021102203304202020-01-03105210315440\n\n\nAs you can see, .pivot creates a dataframe where the columns are the unique labels from one column (‚Äúticker‚Äù), alongside the index column (‚Äúdate‚Äù). The values for the non-index columns are taken from the corresponding rows of the values column (‚Äúprice‚Äù).\nIf our dataframe had multiple prices for the same ticker on the same date, we would use the aggregate_fn parameter of the .pivot method, e.g.: prices.pivot(..., aggregate_fn=\"mean\"). Pivoting with an aggregate function gives us similar behaviour to what Excel calls ‚Äúpivot tables‚Äù.\n\n\n4.3.2 Melt\nMelt is the inverse of pivot. While pivot takes us from long data to wide data, melt goes from wide to long. If we call .melt(id_vars=\"date\", value_name=\"price\") on our pivoted dataframe we get our original dataframe back:\n\n\nCode\npivoted.melt(id_vars=\"date\", value_name=\"price\")\n\n\n\n\nshape: (12, 3)datevariablepricedatestri642020-01-01\"AAPL\"1002020-01-02\"AAPL\"110‚Ä¶‚Ä¶‚Ä¶2020-01-02\"NFLX\"4202020-01-03\"NFLX\"440"
  },
  {
    "objectID": "tidy.html#tidy-nba-data",
    "href": "tidy.html#tidy-nba-data",
    "title": "4¬† Reshaping and Tidy Data",
    "section": "4.4 Tidy NBA data",
    "text": "4.4 Tidy NBA data\nSuppose we want to calculate the days of rest each team had before each game. In the current structure this is difficult because we need to track both the home_team and away_team columns. We‚Äôll use .melt so that there‚Äôs a single team column. This makes it easier to add a rest column with the per-team rest days between games.\n\nPolarsPandas\n\n\n\ntidy_pl = (\n    games_pl\n    .melt(\n        id_vars=[\"game_id\", \"date\"],\n        value_vars=[\"away_team\", \"home_team\"],\n        value_name=\"team\",\n    )\n    .sort(\"game_id\")\n    .with_columns((\n        pl.col(\"date\")\n        .alias(\"rest\")\n        .diff().over(\"team\")\n        .dt.total_days() - 1).cast(pl.Int8))\n    .drop_nulls(\"rest\")\n    .collect()\n)\ntidy_pl\n\n\n\nshape: (2_602, 5)game_iddatevariableteamrestu32datestrstri852015-10-28\"away_team\"\"Chicago Bulls\"062015-10-28\"home_team\"\"Detroit Piston‚Ä¶0‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶13152016-06-19\"away_team\"\"Cleveland Cava‚Ä¶213152016-06-19\"home_team\"\"Golden State W‚Ä¶2\n\n\n\n\n\ntidy_pd = (\n    games_pd.reset_index()\n    .melt(\n        id_vars=[\"game_id\", \"date\"],\n        value_vars=[\"away_team\", \"home_team\"],\n        value_name=\"team\",\n    )\n    .sort_values(\"game_id\")\n    .assign(\n        rest=lambda df: (\n            df\n            .sort_values(\"date\")\n            .groupby(\"team\")\n            [\"date\"]\n            .diff()\n            .dt.days\n            .sub(1)\n        )\n    )\n    .dropna(subset=[\"rest\"])\n    .astype({\"rest\": pd.Int8Dtype()})\n)\ntidy_pd\n\n\n\n\n\n  \n    \n      \n      game_id\n      date\n      variable\n      team\n      rest\n    \n  \n  \n    \n      7\n      7\n      2015-10-28\n      away_team\n      New Orleans Pelicans\n      0\n    \n    \n      11\n      11\n      2015-10-28\n      away_team\n      Chicago Bulls\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1315\n      1315\n      2016-06-19\n      away_team\n      Cleveland Cavaliers\n      2\n    \n    \n      2631\n      1315\n      2016-06-19\n      home_team\n      Golden State Warriors\n      2\n    \n  \n\n2602 rows √ó 5 columns\n\n\n\n\n\n\nNow we use .pivot so that this days-of-rest data can be added back to the original dataframe. We‚Äôll also add columns for the spread between the home team‚Äôs rest and away team‚Äôs rest, and a flag for whether the home team won.\n\nPolarsPandas\n\n\n\nby_game_pl = (\n    tidy_pl\n    .pivot(\n        values=\"rest\",\n        index=[\"game_id\", \"date\"],\n        columns=\"variable\"\n    )\n    .rename({\"away_team\": \"away_rest\", \"home_team\": \"home_rest\"})\n)\njoined_pl = (\n    by_game_pl\n    .join(games_pl.collect(), on=[\"game_id\", \"date\"])\n    .with_columns([\n        pl.col(\"home_points\").alias(\"home_win\") > pl.col(\"away_points\"),\n        pl.col(\"home_rest\").alias(\"rest_spread\") - pl.col(\"away_rest\"),\n    ])\n)\njoined_pl\n\n\n\nshape: (1_303, 10)game_iddateaway_resthome_restaway_teamaway_pointshome_teamhome_pointshome_winrest_spreadu32datei8i8stri64stri64booli852015-10-280null\"Chicago Bulls\"115\"Brooklyn Nets\"100falsenull62015-10-28null0\"Utah Jazz\"87\"Detroit Piston‚Ä¶92truenull‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶13142016-06-1622\"Golden State W‚Ä¶101\"Cleveland Cava‚Ä¶115true013152016-06-1922\"Cleveland Cava‚Ä¶93\"Golden State W‚Ä¶89false0\n\n\n\n\n\nby_game_pd = (\n    tidy_pd\n    .pivot(\n        values=\"rest\",\n        index=[\"game_id\", \"date\"],\n        columns=\"variable\"\n    )\n    .rename(\n        columns={\"away_team\": \"away_rest\", \"home_team\": \"home_rest\"}\n    )\n)\njoined_pd = by_game_pd.join(games_pd).assign(\n    home_win=lambda df: df[\"home_points\"] > df[\"away_points\"],\n    rest_spread=lambda df: df[\"home_rest\"] - df[\"away_rest\"],\n)\njoined_pd\n\n\n\n\n\n  \n    \n      \n      \n      away_rest\n      home_rest\n      away_team\n      away_points\n      home_team\n      home_points\n      home_win\n      rest_spread\n    \n    \n      game_id\n      date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      7\n      2015-10-28\n      0\n      <NA>\n      New Orleans Pelicans\n      94\n      Portland Trail Blazers\n      112\n      True\n      <NA>\n    \n    \n      11\n      2015-10-28\n      0\n      <NA>\n      Chicago Bulls\n      115\n      Brooklyn Nets\n      100\n      False\n      <NA>\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1314\n      2016-06-16\n      2\n      2\n      Golden State Warriors\n      101\n      Cleveland Cavaliers\n      115\n      True\n      0\n    \n    \n      1315\n      2016-06-19\n      2\n      2\n      Cleveland Cavaliers\n      93\n      Golden State Warriors\n      89\n      False\n      0\n    \n  \n\n1303 rows √ó 8 columns\n\n\n\n\n\n\nHere‚Äôs a lightly edited quote from Modern Pandas:\n\nOne somewhat subtle point: an ‚Äúobservation‚Äù depends on the question being asked. So really, we have two tidy datasets, tidy for answering team-level questions, and joined for answering game-level questions.\n\nLet‚Äôs use the team-level dataframe to see each team‚Äôs average days of rest, both at home and away:\n\nimport seaborn as sns\nsns.set_theme(font_scale=0.6)\nsns.catplot(\n    tidy_pl,\n    x=\"variable\",\n    y=\"rest\",\n    col=\"team\",\n    col_wrap=5,\n    kind=\"bar\",\n    height=1.5,\n)\n\n<seaborn.axisgrid.FacetGrid at 0x7f1490a76b50>\n\n\n\n\n\nPlotting the distribution of rest_spread:\n\nPolarsPandas\n\n\n\nimport numpy as np\ndelta_pl = joined_pl[\"rest_spread\"]\nax = (\n    delta_pl\n    .value_counts()\n    .drop_nulls()\n    .to_pandas()\n    .set_index(\"rest_spread\")\n    [\"count\"]\n    .reindex(np.arange(delta_pl.min(), delta_pl.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind=\"bar\", color=\"k\", width=0.9, rot=0, figsize=(9, 6))\n)\nax.set(xlabel=\"Difference in Rest (Home - Away)\", ylabel=\"Games\")\n\n[Text(0.5, 0, 'Difference in Rest (Home - Away)'), Text(0, 0.5, 'Games')]\n\n\n\n\n\n\n\n\ndelta_pd = joined_pd[\"rest_spread\"]\nax = (\n    delta_pd\n    .value_counts()\n    .reindex(np.arange(delta_pd.min(), delta_pd.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind=\"bar\", color=\"k\", width=0.9, rot=0, figsize=(9, 6))\n)\nax.set(xlabel=\"Difference in Rest (Home - Away)\", ylabel=\"Games\")\n\n[Text(0.5, 0, 'Difference in Rest (Home - Away)'), Text(0, 0.5, 'Games')]\n\n\n\n\n\n\n\n\nPlotting the win percent by rest_spread:\n\nPolarsPandas\n\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(9, 6))\nsns.barplot(\n    x=\"rest_spread\",\n    y=\"home_win\",\n    data=joined_pl.filter(pl.col(\"rest_spread\").is_between(-3, 3, closed=\"both\")),\n    color=\"#4c72b0\",\n    ax=ax,\n)\n\n<Axes: xlabel='rest_spread', ylabel='home_win'>\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(9, 6))\nsns.barplot(\n    x=\"rest_spread\",\n    y=\"home_win\",\n    data=joined_pd.query('-3 <= rest_spread <= 3'),\n    color=\"#4c72b0\",\n    ax=ax,\n)\n\n<Axes: xlabel='rest_spread', ylabel='home_win'>"
  },
  {
    "objectID": "tidy.html#stack-unstack-vs-melt-pivot",
    "href": "tidy.html#stack-unstack-vs-melt-pivot",
    "title": "4¬† Reshaping and Tidy Data",
    "section": "4.5 Stack / Unstack vs Melt / Pivot",
    "text": "4.5 Stack / Unstack vs Melt / Pivot\nPandas has special methods stack and unstack for reshaping data with a MultiIndex. Polars doesn‚Äôt have an index, so anywhere you see stack / unstack in Pandas, the equivalent Polars code will use melt / pivot.\n\nPolarsPandas\n\n\n\nrest_pl = (\n    tidy_pl\n    .group_by([\"date\", \"variable\"], maintain_order=True)\n    .agg(pl.col(\"rest\").mean())\n)\nrest_pl\n\n\n\nshape: (418, 3)datevariablerestdatestrf642015-10-28\"away_team\"0.02015-10-28\"home_team\"0.0‚Ä¶‚Ä¶‚Ä¶2016-06-19\"away_team\"2.02016-06-19\"home_team\"2.0\n\n\n\n\n\nrest_pd = (\n    tidy_pd\n    .groupby([\"date\", \"variable\"])\n    [\"rest\"]\n    .mean()\n)\nrest_pd\n\ndate        variable \n2015-10-28  away_team    0.0\n            home_team    0.0\n                        ... \n2016-06-19  away_team    2.0\n            home_team    2.0\nName: rest, Length: 418, dtype: Float64\n\n\n\n\n\nIn Polars we use .pivot to do what in Pandas would require .unstack:\n\nPolarsPandas\n\n\n\nrest_pl.pivot(index=\"date\", columns=\"variable\", values=\"rest\")\n\n\n\nshape: (209, 3)dateaway_teamhome_teamdatef64f642015-10-280.00.02015-10-290.3333330.0‚Ä¶‚Ä¶‚Ä¶2016-06-162.02.02016-06-192.02.0\n\n\n\n\n\nrest_pd.unstack()\n\n\n\n\n\n  \n    \n      variable\n      away_team\n      home_team\n    \n    \n      date\n      \n      \n    \n  \n  \n    \n      2015-10-28\n      0.0\n      0.0\n    \n    \n      2015-10-29\n      0.333333\n      0.0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2016-06-16\n      2.0\n      2.0\n    \n    \n      2016-06-19\n      2.0\n      2.0\n    \n  \n\n209 rows √ó 2 columns\n\n\n\n\n\n\nPlotting the moving average of rest days:\n\nPolarsPandas\n\n\n\nax = (\n    rest_pl.pivot(index=\"date\", values=\"rest\", columns=\"variable\")\n    .filter(pl.col(\"away_team\") < 7)\n    .sort(\"date\")\n    .select([pl.col(\"date\"), pl.col(pl.Float64).rolling_mean(7)])\n    .to_pandas()\n    .set_index(\"date\")\n    .plot(figsize=(9, 6), linewidth=3)\n)\nax.set(ylabel=\"Rest (7 day MA)\")\n\n[Text(0, 0.5, 'Rest (7 day MA)')]\n\n\n\n\n\n\n\n\nax = (\n    rest_pd.unstack()\n    .query('away_team < 7')\n    .sort_index()\n    .rolling(7)\n    .mean()\n    .plot(figsize=(9, 6), linewidth=3)\n)\nax.set(ylabel=\"Rest (7 day MA)\")\n\n[Text(0, 0.5, 'Rest (7 day MA)')]"
  },
  {
    "objectID": "tidy.html#mini-project-home-court-advantage",
    "href": "tidy.html#mini-project-home-court-advantage",
    "title": "4¬† Reshaping and Tidy Data",
    "section": "4.6 Mini Project: Home Court Advantage?",
    "text": "4.6 Mini Project: Home Court Advantage?\nWe may as well do some (not very rigorous) analysis: let‚Äôs see if home advantage is a real thing.\n\n4.6.1 Find the win percent for each team\nWe want to control for the strength of the teams playing. The team‚Äôs victory percentage is probably not a good control but it‚Äôs what we‚Äôll use:\n\nPolarsPandas\n\n\n\nwin_col = pl.col(\"win\")\nwins_pl = (\n    joined_pl.melt(\n        id_vars=[\"game_id\", \"date\", \"home_win\"],\n        value_name=\"team\",\n        variable_name=\"is_home\",\n        value_vars=[\"home_team\", \"away_team\"],\n    )\n    .with_columns(pl.col(\"home_win\").alias(\"win\") == (pl.col(\"is_home\") == \"home_team\"))\n    .group_by([\"team\", \"is_home\"])\n    .agg(\n        [\n            win_col.sum().alias(\"n_wins\"),\n            win_col.count().alias(\"n_games\"),\n            win_col.mean().alias(\"win_pct\"),\n        ]\n    )\n    .sort([\"team\", \"is_home\"])\n)\nwins_pl\n\n\n\nshape: (60, 5)teamis_homen_winsn_gameswin_pctstrstru32u32f64\"Atlanta Hawks\"\"away_team\"22460.478261\"Atlanta Hawks\"\"home_team\"30450.666667‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶\"Washington Wiz‚Ä¶\"away_team\"18400.45\"Washington Wiz‚Ä¶\"home_team\"22410.536585\n\n\n\n\n\nwins_pd = (\n    joined_pd\n    .reset_index()\n    .melt(\n        id_vars=[\"game_id\", \"date\", \"home_win\"],\n        value_name=\"team\",\n        var_name=\"is_home\",\n        value_vars=[\"home_team\", \"away_team\"],\n    )\n    .assign(win=lambda df: df[\"home_win\"] == (df[\"is_home\"] == \"home_team\"))\n    .groupby([\"team\", \"is_home\"])[\"win\"]\n    .agg(['sum', 'count', 'mean'])\n    .rename(columns={\n        \"sum\": 'n_wins',\n        \"count\": 'n_games',\n        \"mean\": 'win_pct'\n    })\n)\nwins_pd\n\n\n\n\n\n  \n    \n      \n      \n      n_wins\n      n_games\n      win_pct\n    \n    \n      team\n      is_home\n      \n      \n      \n    \n  \n  \n    \n      Atlanta Hawks\n      away_team\n      22\n      46\n      0.478261\n    \n    \n      home_team\n      30\n      45\n      0.666667\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Washington Wizards\n      away_team\n      18\n      40\n      0.450000\n    \n    \n      home_team\n      22\n      41\n      0.536585\n    \n  \n\n60 rows √ó 3 columns"
  },
  {
    "objectID": "tidy.html#some-visualisations",
    "href": "tidy.html#some-visualisations",
    "title": "4¬† Reshaping and Tidy Data",
    "section": "4.7 Some visualisations",
    "text": "4.7 Some visualisations\n\ng = sns.FacetGrid(wins_pl, hue=\"team\", aspect=0.8, palette=[\"k\"], height=5)\ng.map(\n    sns.pointplot,\n    \"is_home\",\n    \"win_pct\",\n    order=[\"away_team\", \"home_team\"]).set(ylim=(0, 1))\n\n<seaborn.axisgrid.FacetGrid at 0x7f146ba2d710>\n\n\n\n\n\n\nsns.catplot(\n    wins_pl,\n    x=\"is_home\",\n    y=\"win_pct\",\n    col=\"team\",\n    col_wrap=5,\n    hue=\"team\",\n    kind=\"point\",\n    height=1.5,\n)\n\n<seaborn.axisgrid.FacetGrid at 0x7f146a638450>\n\n\n\n\n\nNow we calculate the win percent by team, regardless of whether they‚Äôre home or away:\n\nPolarsPandas\n\n\n\nwin_percent_pl = (\n    wins_pl.group_by(\"team\", maintain_order=True).agg(\n        pl.col(\"n_wins\").sum().alias(\"win_pct\") / pl.col(\"n_games\").sum()\n    )\n)\nwin_percent_pl\n\n\n\nshape: (30, 2)teamwin_pctstrf64\"Atlanta Hawks\"0.571429\"Boston Celtics‚Ä¶0.563218‚Ä¶‚Ä¶\"Utah Jazz\"0.487805\"Washington Wiz‚Ä¶0.493827\n\n\n\n\n\nwin_percent_pd = (\n    wins_pd\n    .groupby(level=\"team\", as_index=True)\n    .apply(lambda x: x[\"n_wins\"].sum() / x[\"n_games\"].sum())\n)\nwin_percent_pd\n\nteam\nAtlanta Hawks         0.571429\nBoston Celtics        0.563218\n                        ...   \nUtah Jazz             0.487805\nWashington Wizards    0.493827\nLength: 30, dtype: float64\n\n\n\n\n\n\n(\n    win_percent_pl\n    .sort(\"win_pct\")\n    .to_pandas()\n    .set_index(\"team\")\n    .plot.barh(figsize=(6, 12), width=0.85, color=\"k\")\n)\nplt.xlabel(\"Win Percent\")\n\nText(0.5, 0, 'Win Percent')\n\n\n\n\n\nHere‚Äôs a plot of team home court advantage against team overall win percentage:\n\nPolarsPandas\n\n\n\nwins_to_plot_pl = (\n    wins_pl.pivot(index=\"team\", columns=\"is_home\", values=\"win_pct\")\n    .with_columns(\n        [\n            pl.col(\"home_team\").alias(\"Home Win % - Away %\") - pl.col(\"away_team\"),\n            (pl.col(\"home_team\").alias(\"Overall %\") + pl.col(\"away_team\")) / 2,\n        ]\n    )\n)\nsns.regplot(data=wins_to_plot_pl, x='Overall %', y='Home Win % - Away %')\n\n<Axes: >\n\n\n\n\n\n\n\n\nwins_to_plot_pd = (\n    wins_pd\n    [\"win_pct\"]\n    .unstack()\n    .assign(**{'Home Win % - Away %': lambda x: x[\"home_team\"] - x[\"away_team\"],\n               'Overall %': lambda x: (x[\"home_team\"] + x[\"away_team\"]) / 2})\n)\nsns.regplot(data=wins_to_plot_pd, x='Overall %', y='Home Win % - Away %')\n\n<Axes: xlabel='Overall %', ylabel='Home Win % - Away %'>\n\n\n\n\n\n\n\n\nLet‚Äôs add the win percent back to the dataframe and run a regression:\n\nPolarsPandas\n\n\n\nreg_df_pl = (\n    joined_pl.join(win_percent_pl, left_on=\"home_team\", right_on=\"team\")\n    .rename({\"win_pct\": \"home_strength\"})\n    .join(win_percent_pl, left_on=\"away_team\", right_on=\"team\")\n    .rename({\"win_pct\": \"away_strength\"})\n    .with_columns(\n        [\n            pl.col(\"home_points\").alias(\"point_diff\") - pl.col(\"away_points\"),\n            pl.col(\"home_rest\").alias(\"rest_diff\") - pl.col(\"away_rest\"),\n            pl.col(\"home_win\").cast(pl.UInt8),  # for statsmodels\n        ]\n    )\n)\nreg_df_pl.head()\n\n\n\nshape: (5, 14)game_iddateaway_resthome_restaway_teamaway_pointshome_teamhome_pointshome_winrest_spreadhome_strengthaway_strengthpoint_diffrest_diffu32datei8i8stri64stri64u8i8f64f64i64i852015-10-280null\"Chicago Bulls\"115\"Brooklyn Nets\"1000null0.2560980.506173-15null62015-10-28null0\"Utah Jazz\"87\"Detroit Piston‚Ä¶921null0.5058820.4878055null112015-10-280null\"Cleveland Cava‚Ä¶106\"Memphis Grizzl‚Ä¶760null0.4883720.715686-30null142015-10-280null\"New Orleans Pe‚Ä¶94\"Portland Trail‚Ä¶1121null0.5268820.3703718null172015-10-2900\"Memphis Grizzl‚Ä¶112\"Indiana Pacers‚Ä¶103000.5454550.488372-90\n\n\n\n\n\nreg_df_pd = (\n    joined_pd.assign(\n        away_strength=joined_pd['away_team'].map(win_percent_pd),\n        home_strength=joined_pd['home_team'].map(win_percent_pd),\n        point_diff=joined_pd['home_points'] - joined_pd['away_points'],\n        rest_diff=joined_pd['home_rest'] - joined_pd['away_rest'])\n)\nreg_df_pd.head()\n\n\n\n\n\n  \n    \n      \n      \n      away_rest\n      home_rest\n      away_team\n      away_points\n      home_team\n      home_points\n      home_win\n      rest_spread\n      away_strength\n      home_strength\n      point_diff\n      rest_diff\n    \n    \n      game_id\n      date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      7\n      2015-10-28\n      0\n      <NA>\n      New Orleans Pelicans\n      94\n      Portland Trail Blazers\n      112\n      True\n      <NA>\n      0.370370\n      0.526882\n      18\n      <NA>\n    \n    \n      11\n      2015-10-28\n      0\n      <NA>\n      Chicago Bulls\n      115\n      Brooklyn Nets\n      100\n      False\n      <NA>\n      0.506173\n      0.256098\n      -15\n      <NA>\n    \n    \n      15\n      2015-10-28\n      <NA>\n      0\n      Utah Jazz\n      87\n      Detroit Pistons\n      92\n      True\n      <NA>\n      0.487805\n      0.505882\n      5\n      <NA>\n    \n    \n      16\n      2015-10-28\n      0\n      <NA>\n      Cleveland Cavaliers\n      106\n      Memphis Grizzlies\n      76\n      False\n      <NA>\n      0.715686\n      0.488372\n      -30\n      <NA>\n    \n    \n      17\n      2015-10-29\n      1\n      0\n      Atlanta Hawks\n      112\n      New York Knicks\n      101\n      False\n      -1\n      0.571429\n      0.382716\n      -11\n      -1\n    \n  \n\n\n\n\n\n\n\n\nimport statsmodels.formula.api as sm\n\nmod = sm.logit(\n    \"home_win ~ home_strength + away_strength + home_rest + away_rest\",\n    reg_df_pl.to_pandas(),\n)\nres = mod.fit()\nres.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.554797\n         Iterations 6\n\n\n\n\nLogit Regression Results\n\n  Dep. Variable:       home_win       No. Observations:      1299  \n\n\n  Model:                 Logit        Df Residuals:          1294  \n\n\n  Method:                 MLE         Df Model:                 4  \n\n\n  Date:            Fri, 29 Dec 2023   Pseudo R-squ.:       0.1777  \n\n\n  Time:                16:57:44       Log-Likelihood:      -720.68 \n\n\n  converged:             True         LL-Null:             -876.38 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        3.748e-66\n\n\n\n\n                   coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept        -0.0019     0.304    -0.006  0.995    -0.597     0.593\n\n\n  home_strength     5.7161     0.466    12.272  0.000     4.803     6.629\n\n\n  away_strength    -4.9133     0.456   -10.786  0.000    -5.806    -4.020\n\n\n  home_rest         0.1045     0.076     1.381  0.167    -0.044     0.253\n\n\n  away_rest        -0.0347     0.066    -0.526  0.599    -0.164     0.095\n\n\n\n\nYou can play around with the regressions yourself but we‚Äôll end them here."
  },
  {
    "objectID": "tidy.html#summary",
    "href": "tidy.html#summary",
    "title": "4¬† Reshaping and Tidy Data",
    "section": "4.8 Summary",
    "text": "4.8 Summary\nThis was mostly a demonstration of .pivot and .melt, with several different examples of reshaping data in Polars and Pandas."
  },
  {
    "objectID": "timeseries.html#get-the-data",
    "href": "timeseries.html#get-the-data",
    "title": "5¬† Timeseries",
    "section": "5.1 Get the data",
    "text": "5.1 Get the data\nWe‚Äôll download a year‚Äôs worth of daily price and volume data for Bitcoin:\n\nfrom pathlib import Path\nfrom io import StringIO\nfrom datetime import datetime, date\nimport requests\nimport polars as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npl.Config.set_tbl_rows(5)\npd.options.display.max_rows = 5\ndata_path = Path(\"../data/ohlcv.pq\")\n\n\ndef epoch_ms(dt: datetime) -> int:\n    return int(dt.timestamp()) * 1000\n\n\nif data_path.exists():\n    ohlcv_pl = pl.read_parquet(data_path).set_sorted(\"time\")\n\nelse:\n    start = epoch_ms(datetime(2021, 1, 1))\n    end = epoch_ms(datetime(2022, 1, 1))\n    url = (\n        \"https://api.binance.com/api/v3/klines?symbol=BTCUSDT&\"\n        f\"interval=1d&startTime={start}&endTime={end}\"\n    )\n    resp = requests.get(url)\n    time_col = \"time\"\n    ohlcv_cols = [\n        \"open\",\n        \"high\",\n        \"low\",\n        \"close\",\n        \"volume\",\n    ]\n    cols_to_use = [time_col, *ohlcv_cols] \n    cols = cols_to_use + [f\"ignore_{i}\" for i in range(6)]\n    ohlcv_pl = pl.from_records(resp.json(), orient=\"row\", schema=cols).select(\n        [\n            pl.col(time_col).cast(pl.Datetime).dt.with_time_unit(\"ms\").cast(pl.Date),\n            pl.col(ohlcv_cols).cast(pl.Float64),\n        ]\n    ).set_sorted(\"time\")\n    ohlcv_pl.write_parquet(data_path)\n\nohlcv_pd = ohlcv_pl.with_columns(pl.col(\"time\").cast(pl.Datetime)).to_pandas().set_index(\"time\")"
  },
  {
    "objectID": "timeseries.html#filtering",
    "href": "timeseries.html#filtering",
    "title": "5¬† Timeseries",
    "section": "5.2 Filtering",
    "text": "5.2 Filtering\nPandas has special methods for filtering data with a DatetimeIndex. Since Polars doesn‚Äôt have an index, we just use .filter. I will admit the Pandas code is more convenient for things like filtering for a specific month:\n\nPolarsPandas\n\n\n\nohlcv_pl.filter(\n    pl.col(\"time\").is_between(\n        date(2021, 2, 1),\n        date(2021, 3, 1),\n        closed=\"left\"\n    )\n)\n\n\n\nshape: (28, 6)timeopenhighlowclosevolumedatef64f64f64f64f642021-02-0133092.9734717.2732296.1633526.3782718.2768822021-02-0233517.0935984.3333418.035466.2478056.65988‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶2021-02-2746276.8848394.045000.046106.4366060.8342922021-02-2846103.6746638.4643000.045135.6683055.369042\n\n\n\n\n\nohlcv_pd.loc[\"2021-02\"]\n\n\n\n\n\n  \n    \n      \n      open\n      high\n      low\n      close\n      volume\n    \n    \n      time\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-02-01\n      33092.97\n      34717.27\n      32296.16\n      33526.37\n      82718.276882\n    \n    \n      2021-02-02\n      33517.09\n      35984.33\n      33418.00\n      35466.24\n      78056.659880\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-02-27\n      46276.88\n      48394.00\n      45000.00\n      46106.43\n      66060.834292\n    \n    \n      2021-02-28\n      46103.67\n      46638.46\n      43000.00\n      45135.66\n      83055.369042\n    \n  \n\n28 rows √ó 5 columns"
  },
  {
    "objectID": "timeseries.html#resampling",
    "href": "timeseries.html#resampling",
    "title": "5¬† Timeseries",
    "section": "5.3 Resampling",
    "text": "5.3 Resampling\nResampling is like a special case of groupby for a time column. You can of course use regular .groupby with a time column, but it won‚Äôt be as powerful because it doesn‚Äôt understand time like resampling methods do.\nThere are two kinds of resampling: downsampling and upsampling.\n\n5.3.1 Downsampling\nDownsampling moves from a higher time frequency to a lower time frequency. This requires some aggregation or subsetting, since we‚Äôre reducing the number of rows in our data.\nIn Polars we use the .groupby_dynamic method for downsampling (we also use groupby_dynamic when we want to combine resampling with regular groupby logic).\n\nPolarsPandas\n\n\n\n(\n    ohlcv_pl\n    .group_by_dynamic(\"time\", every=\"5d\")\n    .agg(pl.col(pl.Float64).mean())\n)\n\n\n\nshape: (74, 6)timeopenhighlowclosevolumedatef64f64f64f64f642020-12-2929127.66531450.028785.5530755.0192088.3991862021-01-0333577.02836008.46431916.19835027.986127574.470245‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶2021-12-2450707.0851407.65649540.15250048.06629607.1605722021-12-2946836.552548135.492545970.8446881.277531098.406725\n\n\n\n\n\nohlcv_pd.resample(\"5d\").mean()\n\n\n\n\n\n  \n    \n      \n      open\n      high\n      low\n      close\n      volume\n    \n    \n      time\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-01-01\n      31084.316\n      33127.622\n      29512.818\n      32089.662\n      112416.849570\n    \n    \n      2021-01-06\n      38165.310\n      40396.842\n      35983.822\n      39004.538\n      118750.076685\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-12-27\n      48521.240\n      49475.878\n      47087.400\n      47609.528\n      35886.943710\n    \n    \n      2022-01-01\n      46216.930\n      47954.630\n      46208.370\n      47722.650\n      19604.463250\n    \n  \n\n74 rows √ó 5 columns\n\n\n\n\n\n\nResampling and performing multiple aggregations to each column:\n\nPolarsPandas\n\n\n\n(\n    ohlcv_pl\n    .group_by_dynamic(\"time\", every=\"1w\", start_by=\"friday\")\n    .agg([\n        pl.col(pl.Float64).mean().name.suffix(\"_mean\"),\n        pl.col(pl.Float64).sum().name.suffix(\"_sum\")\n    ])\n)\n\n\n\nshape: (53, 11)timeopen_meanhigh_meanlow_meanclose_meanvolume_meanopen_sumhigh_sumlow_sumclose_sumvolume_sumdatef64f64f64f64f64f64f64f64f64f642021-01-0132305.78142934706.04571431021.72714333807.135714117435.5928226140.47242942.32217152.09236649.95822049.1495982021-01-0837869.79714339646.10571434623.33428637827.52135188.296617265088.58277522.74242363.34264792.64946318.076319‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶2021-12-2449649.11428650439.62285748528.2549117.9831126.709793347543.8353077.36339697.75343825.86217886.968552021-12-3146668.90548251.44545943.18546969.7927271.23060593337.8196502.8991886.3793939.5854542.46121\n\n\n\n\n\nohlcv_pd.resample(\"W-Fri\", closed=\"left\", label=\"left\").agg(['mean', 'sum'])\n\n\n\n\n\n  \n    \n      \n      open\n      high\n      low\n      close\n      volume\n    \n    \n      \n      mean\n      sum\n      mean\n      sum\n      mean\n      sum\n      mean\n      sum\n      mean\n      sum\n    \n    \n      time\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-01-01\n      32305.781429\n      226140.47\n      34706.045714\n      242942.32\n      31021.727143\n      217152.09\n      33807.135714\n      236649.95\n      117435.592800\n      822049.149598\n    \n    \n      2021-01-08\n      37869.797143\n      265088.58\n      39646.105714\n      277522.74\n      34623.334286\n      242363.34\n      37827.520000\n      264792.64\n      135188.296617\n      946318.076319\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-12-24\n      49649.114286\n      347543.80\n      50439.622857\n      353077.36\n      48528.250000\n      339697.75\n      49117.980000\n      343825.86\n      31126.709793\n      217886.968550\n    \n    \n      2021-12-31\n      46668.905000\n      93337.81\n      48251.445000\n      96502.89\n      45943.185000\n      91886.37\n      46969.790000\n      93939.58\n      27271.230605\n      54542.461210\n    \n  \n\n53 rows √ó 10 columns\n\n\n\n\n\n\n\n\n5.3.2 Upsampling\nUpsampling moves in the opposite direction, from low-frequency data to high frequency data. Since we can‚Äôt create new data by magic, upsampling defaults to filling the new rows with nulls (which we could then interpolate, perhaps). In Polars we have a special upsample method for this, while Pandas reuses its resample method.\n\nPolarsPandas\n\n\n\nohlcv_pl.upsample(\"time\", every=\"6h\")\n\n\n\nshape: (1_461, 6)timeopenhighlowclosevolumedatef64f64f64f64f642021-01-0128923.6329600.028624.5729331.6954182.9250112021-01-01nullnullnullnullnull‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶2021-12-31nullnullnullnullnull2022-01-0146216.9347954.6346208.3747722.6519604.46325\n\n\n\n\n\nohlcv_pd.resample(\"6H\").mean()\n\n\n\n\n\n  \n    \n      \n      open\n      high\n      low\n      close\n      volume\n    \n    \n      time\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-01-01 00:00:00\n      28923.63\n      29600.00\n      28624.57\n      29331.69\n      54182.925011\n    \n    \n      2021-01-01 06:00:00\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-12-31 18:00:00\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2022-01-01 00:00:00\n      46216.93\n      47954.63\n      46208.37\n      47722.65\n      19604.463250\n    \n  \n\n1461 rows √ó 5 columns"
  },
  {
    "objectID": "timeseries.html#rolling-expanding-ew",
    "href": "timeseries.html#rolling-expanding-ew",
    "title": "5¬† Timeseries",
    "section": "5.4 Rolling / Expanding / EW",
    "text": "5.4 Rolling / Expanding / EW\nPolars supports all three of these but they‚Äôre not quite as powerful as in Pandas, since they don‚Äôt have as many different methods. The expanding support is more limited again, though there are workarounds for this (see below):\n\nPolarsPandas\n\n\n\nclose = pl.col(\"close\")\nohlcv_pl.select(\n    [\n        pl.col(\"time\"),\n        close.alias(\"Raw\"),\n        close.rolling_mean(28).alias(\"28D MA\"),\n        close.alias(\"Expanding Average\").cum_sum() / (close.cum_count() + 1),\n        close.ewm_mean(alpha=0.03).alias(\"EWMA($\\\\alpha=.03$)\"),\n    ]\n).to_pandas().set_index(\"time\").plot()\n\nplt.ylabel(\"Close ($)\")\n\nText(0, 0.5, 'Close ($)')\n\n\n\n\n\n\n\n\nohlcv_pd[\"close\"].plot(label=\"Raw\")\nohlcv_pd[\"close\"].rolling(28).mean().plot(label=\"28D MA\")\nohlcv_pd[\"close\"].expanding().mean().plot(label=\"Expanding Average\")\nohlcv_pd[\"close\"].ewm(alpha=0.03).mean().plot(label=\"EWMA($\\\\alpha=.03$)\")\n\nplt.legend(bbox_to_anchor=(0.63, 0.27))\nplt.ylabel(\"Close ($)\")\n\nText(0, 0.5, 'Close ($)')\n\n\n\n\n\n\n\n\nPolars doesn‚Äôt have an expanding_mean yet so we make do by combining cumsum and cumcount.\n\n5.4.1 Combining rolling aggregations\n\nPolarsPandas\n\n\n\nmean_std_pl = ohlcv_pl.select(\n    [\n        \"time\",\n        pl.col(\"close\").rolling_mean(30, center=True).alias(\"mean\"),\n        pl.col(\"close\").rolling_std(30, center=True).alias(\"std\"),\n    ]\n)\nax = mean_std_pl.to_pandas().set_index(\"time\")[\"mean\"].plot()\nax.fill_between(\n    mean_std_pl[\"time\"].to_numpy(),\n    mean_std_pl[\"mean\"] - mean_std_pl[\"std\"],\n    mean_std_pl[\"mean\"] + mean_std_pl[\"std\"],\n    alpha=0.25,\n)\nplt.tight_layout()\nplt.ylabel(\"Close ($)\")\n\nText(26.83333333333334, 0.5, 'Close ($)')\n\n\n\n\n\n\n\n\nroll_pd = ohlcv_pd[\"close\"].rolling(30, center=True)\nmean_std_pd = roll_pd.agg([\"mean\", \"std\"])\nax = mean_std_pd[\"mean\"].plot()\nax.fill_between(\n    mean_std_pd.index,\n    mean_std_pd[\"mean\"] - mean_std_pd[\"std\"],\n    mean_std_pd[\"mean\"] + mean_std_pd[\"std\"],\n    alpha=0.25,\n)\nplt.tight_layout()\nplt.ylabel(\"Close ($)\")\n\nText(26.83333333333334, 0.5, 'Close ($)')"
  },
  {
    "objectID": "timeseries.html#grab-bag",
    "href": "timeseries.html#grab-bag",
    "title": "5¬† Timeseries",
    "section": "5.5 Grab Bag",
    "text": "5.5 Grab Bag\n\n5.5.1 Offsets\nPandas has two similar objects for datetime arithmetic: DateOffset which respects calendar arithmetic, and Timedelta which respects absolute time arithmetic. DateOffset understands things like daylight savings time, and can work with holidays too.\nPolars just has a Duration type which is like Pandas Timedelta.\n\nPolarsPandas (Timedelta)Pandas (DateOffset)\n\n\n\nohlcv_pl.select(pl.col(\"time\") + pl.duration(days=80))\n\n\n\nshape: (366, 1)timedate2021-03-222021-03-23‚Ä¶2022-03-212022-03-22\n\n\n\n\n\nohlcv_pd.index + pd.Timedelta(80, \"D\")\n\nDatetimeIndex(['2021-03-22', '2021-03-23', '2021-03-24', '2021-03-25',\n               '2021-03-26', '2021-03-27', '2021-03-28', '2021-03-29',\n               '2021-03-30', '2021-03-31',\n               ...\n               '2022-03-13', '2022-03-14', '2022-03-15', '2022-03-16',\n               '2022-03-17', '2022-03-18', '2022-03-19', '2022-03-20',\n               '2022-03-21', '2022-03-22'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None)\n\n\n\n\n\nohlcv_pd.index + pd.DateOffset(months=3, days=-10)\n\nDatetimeIndex(['2021-03-22', '2021-03-23', '2021-03-24', '2021-03-25',\n               '2021-03-26', '2021-03-27', '2021-03-28', '2021-03-29',\n               '2021-03-30', '2021-03-31',\n               ...\n               '2022-03-13', '2022-03-14', '2022-03-15', '2022-03-16',\n               '2022-03-17', '2022-03-18', '2022-03-19', '2022-03-20',\n               '2022-03-21', '2022-03-22'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None)\n\n\n\n\n\n\n\n5.5.2 Holiday calendars\nNot many people know this, but Pandas can do some quite powerful stuff with Holiday Calendars. There is an open issue to add this functionality to Polars.\n\n\n5.5.3 Timezones\nSuppose we know that our timestamps are UTC, and we want to see what time it was in US/Eastern:\n\nPolarsPandas\n\n\n\n(\n    ohlcv_pl\n    .with_columns(\n        pl.col(\"time\")\n        .cast(pl.Datetime)\n        .dt.replace_time_zone(\"UTC\")\n        .dt.convert_time_zone(\"US/Eastern\")\n    )\n)\n\n\n\nshape: (366, 6)timeopenhighlowclosevolumedatetime[Œºs, US/Eastern]f64f64f64f64f642020-12-31 19:00:00 EST28923.6329600.028624.5729331.6954182.9250112021-01-01 19:00:00 EST29331.733300.028946.5332178.33129993.873362‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶2021-12-30 19:00:00 EST47120.8848548.2645678.046216.9334937.997962021-12-31 19:00:00 EST46216.9347954.6346208.3747722.6519604.46325\n\n\n\n\n\n(\n    ohlcv_pd\n    .tz_localize('UTC')\n    .tz_convert('US/Eastern')\n)\n\n\n\n\n\n  \n    \n      \n      open\n      high\n      low\n      close\n      volume\n    \n    \n      time\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-12-31 19:00:00-05:00\n      28923.63\n      29600.00\n      28624.57\n      29331.69\n      54182.925011\n    \n    \n      2021-01-01 19:00:00-05:00\n      29331.70\n      33300.00\n      28946.53\n      32178.33\n      129993.873362\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-12-30 19:00:00-05:00\n      47120.88\n      48548.26\n      45678.00\n      46216.93\n      34937.997960\n    \n    \n      2021-12-31 19:00:00-05:00\n      46216.93\n      47954.63\n      46208.37\n      47722.65\n      19604.463250\n    \n  \n\n366 rows √ó 5 columns"
  },
  {
    "objectID": "timeseries.html#conclusion",
    "href": "timeseries.html#conclusion",
    "title": "5¬† Timeseries",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\nPolars has really good time series support, though expanding aggregations and holiday calendars are niches in which it is lacking. Pandas DateTimeIndexes are quite cool too, even if they do bring some pain."
  },
  {
    "objectID": "scaling.html#get-the-data",
    "href": "scaling.html#get-the-data",
    "title": "6¬† Scaling",
    "section": "6.1 Get the data",
    "text": "6.1 Get the data\nWe‚Äôll be using political donation data from the FEC. Warning: this takes a few minutes.\n\nimport asyncio\nfrom zipfile import ZipFile\nfrom pathlib import Path\nfrom datetime import date\nfrom io import BytesIO\nimport httpx\nimport polars as pl\nimport pandas as pd\n\n\npl.Config.set_tbl_rows(5)\npd.options.display.max_rows = 5\n\nfec_dir = Path(\"../data/fec\")\n\nasync def download_and_save_cm(year: str, client: httpx.AsyncClient):\n    cm_cols = [\"CMTE_ID\", \"CMTE_NM\", \"CMTE_PTY_AFFILIATION\"]\n    dtypes = {\"CMTE_PTY_AFFILIATION\": pl.Categorical}\n    url = f\"https://www.fec.gov/files/bulk-downloads/20{year}/cm{year}.zip\"\n    resp = await client.get(url)\n    with ZipFile(BytesIO(resp.content)) as z:\n        pl.read_csv(\n            z.read(\"cm.txt\"),\n            has_header=False,\n            columns=[0, 1, 10],\n            new_columns=cm_cols,\n            separator=\"|\",\n            dtypes=dtypes,\n        ).write_parquet(fec_dir / f\"cm{year}.pq\")\n\nasync def download_and_save_indiv(year: str, client: httpx.AsyncClient):\n    dtypes = {\n        \"CMTE_ID\": pl.Utf8,\n        \"EMPLOYER\": pl.Categorical,\n        \"OCCUPATION\": pl.Categorical,\n        \"TRANSACTION_DT\": pl.Utf8,\n        \"TRANSACTION_AMT\": pl.Int32,\n    }\n    url = f\"https://www.fec.gov/files/bulk-downloads/20{year}/indiv{year}.zip\"\n    resp = await client.get(url)\n    with ZipFile(BytesIO(resp.content)) as z:\n        pl.read_csv(\n            z.read(\"itcont.txt\"),\n            has_header=False,\n            columns=[0, 11, 12, 13, 14],\n            new_columns=list(dtypes.keys()),\n            separator=\"|\",\n            dtypes=dtypes,\n            encoding=\"cp1252\",\n        ).with_columns(\n            pl.col(\"TRANSACTION_DT\").str.to_date(format=\"%m%d%Y\", strict=False)\n        ).write_parquet(\n            fec_dir / f\"indiv{year}.pq\"\n        )\n\nyears = [\"08\", \"10\", \"12\", \"14\", \"16\"]\nif not fec_dir.exists():\n    fec_dir.mkdir()\n    async with httpx.AsyncClient(follow_redirects=True, timeout=None) as client:\n        cm_tasks = [download_and_save_cm(year, client) for year in years]\n        indiv_tasks = [download_and_save_indiv(year, client) for year in years]\n        tasks = cm_tasks + indiv_tasks\n        await asyncio.gather(*tasks)"
  },
  {
    "objectID": "scaling.html#simple-aggregation",
    "href": "scaling.html#simple-aggregation",
    "title": "6¬† Scaling",
    "section": "6.2 Simple aggregation",
    "text": "6.2 Simple aggregation\nSuppose we want to find the most common occupations among political donors. Let‚Äôs assume that this data is too big for your machine‚Äôs memory to read it in all at once.\nWe can solve this using Polars streaming, using Dask‚Äôs lazy dataframe or simply using Pandas to read the files one by one and keeping a running total:\n\nPolarsDaskPandas\n\n\n\n# otherwise we can't read categoricals from multiple files\npl.enable_string_cache()\noccupation_counts_pl = (\n    pl.scan_parquet(fec_dir / \"indiv*.pq\", cache=False)\n    .select(pl.col(\"OCCUPATION\").value_counts(parallel=True, sort=True))\n    .collect(streaming=True)\n)\noccupation_counts_pl\n\n\n\nshape: (579_159, 1)OCCUPATIONstruct[2]{\"RETIRED\",4773715}{\"NOT EMPLOYED\",2715939}‚Ä¶{\"PROFESSOR OF PYSICS\",1}{\"ARTIST/SINGER-SONGWRITER\",1}\n\n\n\n\n\nimport dask.dataframe as dd\nfrom dask import compute\noccupation_counts_dd = dd.read_parquet(\n    fec_dir / \"indiv*.pq\", engine=\"pyarrow\", columns=[\"OCCUPATION\"]\n)[\"OCCUPATION\"].value_counts()\noccupation_counts_dd.compute()\n\nOCCUPATION\nRETIRED                               4773715\nNOT EMPLOYED                          2715939\n                                       ...   \nTRANSPORTATION COMPLIANCE DIRECTOR          1\nPROASSURANCE                                1\nName: count, Length: 579158, dtype: int64\n\n\n\n\n\nfiles = sorted(fec_dir.glob(\"indiv*.pq\"))\n\ntotal_counts_pd = pd.Series(dtype=\"int64\")\n\nfor year in files:\n    occ_pd = pd.read_parquet(year, columns=[\"OCCUPATION\"], engine=\"pyarrow\")\n    counts = occ_pd[\"OCCUPATION\"].value_counts()\n    total_counts_pd = total_counts_pd.add(counts, fill_value=0).astype(\"int64\")\n\ntotal_counts_pd.nlargest(100)\n\nOCCUPATION\nRETIRED         4773715\nNOT EMPLOYED    2715939\n                 ...   \nSURGEON           25545\nOPERATOR          25161\nLength: 100, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPolars can handle some larger-than-memory data even without streaming. Thanks to predicate pushdown, we can filter dataframes without reading all the data into memory first. So streaming mode is most useful for cases where we really do need to read in a lot of data."
  },
  {
    "objectID": "scaling.html#executing-multiple-queries-in-parallel",
    "href": "scaling.html#executing-multiple-queries-in-parallel",
    "title": "6¬† Scaling",
    "section": "6.3 Executing multiple queries in parallel",
    "text": "6.3 Executing multiple queries in parallel\nOften we want to generate multiple insights from the same data, and we need them in separate dataframes. In this case, using collect_all is more efficient than calling .collect multiple times, because Polars can avoid repeating common operations like reading the data.\nLet‚Äôs compute the average donation size, the total donated by employer and the average donation by occupation:\n\nPolarsDask\n\n\n\n%%time\nindiv_pl = pl.scan_parquet(fec_dir / \"indiv*.pq\")\navg_transaction_lazy_pl = indiv_pl.select(pl.col(\"TRANSACTION_AMT\").mean())\ntotal_by_employer_lazy_pl = (\n    indiv_pl.drop_nulls(\"EMPLOYER\")\n    .group_by(\"EMPLOYER\")\n    .agg([pl.col(\"TRANSACTION_AMT\").sum()])\n    .sort(\"TRANSACTION_AMT\", descending=True)\n    .head(10)\n)\navg_by_occupation_lazy_pl = (\n    indiv_pl.group_by(\"OCCUPATION\")\n    .agg([pl.col(\"TRANSACTION_AMT\").mean()])\n    .sort(\"TRANSACTION_AMT\", descending=True)\n    .head(10)\n)\n\navg_transaction_pl, total_by_employer_pl, avg_by_occupation_pl = pl.collect_all(\n    [avg_transaction_lazy_pl, total_by_employer_lazy_pl, avg_by_occupation_lazy_pl],\n    streaming=True,\n    comm_subplan_elim=False, # cannot use CSE with streaming\n)\n\nCPU times: user 11.8 s, sys: 2.24 s, total: 14 s\nWall time: 5.66 s\n\n\n\n\n\n%%time\nindiv_dd = (\n    dd.read_parquet(fec_dir / \"indiv*.pq\", engine=\"pyarrow\")\n    # pandas and dask want datetimes but this is a date col\n    .assign(\n        TRANSACTION_DT=lambda df: dd.to_datetime(df[\"TRANSACTION_DT\"], errors=\"coerce\")\n    )\n)\navg_transaction_lazy_dd = indiv_dd[\"TRANSACTION_AMT\"].mean()\ntotal_by_employer_lazy_dd = (\n    indiv_dd.groupby(\"EMPLOYER\", observed=True)[\"TRANSACTION_AMT\"].sum().nlargest(10)\n)\navg_by_occupation_lazy_dd = (\n    indiv_dd.groupby(\"OCCUPATION\", observed=True)[\"TRANSACTION_AMT\"].mean().nlargest(10)\n)\navg_transaction_dd, total_by_employer_dd, avg_by_occupation_dd = compute(\n    avg_transaction_lazy_dd, total_by_employer_lazy_dd, avg_by_occupation_lazy_dd\n)\n\nCPU times: user 25.1 s, sys: 3.54 s, total: 28.7 s\nWall time: 17.8 s\n\n\n\n\n\nThe Polars code above tends to be ~3.5x faster than Dask on my machine, which if anything is a smaller speedup than I expected.\nWe should also profile memory usage, since it could be the case that Polars is just running faster because it‚Äôs reading in bigger chunks. According to the fil profiler, the Dask example‚Äôs memory usage peaks at 1450 MiB, while Polars uses ~10% more than that.\nBefore I forget, here are the results of our computations:\n\n6.3.1 avg_transaction\n\nPolarsDask\n\n\n\navg_transaction_pl\n\n\n\nshape: (1, 1)TRANSACTION_AMTf64563.97184\n\n\n\n\n\navg_transaction_dd\n\n563.9718398183915\n\n\n\n\n\n\n\n6.3.2 total_by_employer\n\nPolarsDask\n\n\n\ntotal_by_employer_pl\n\n\n\nshape: (10, 2)EMPLOYERTRANSACTION_AMTcati32\"RETIRED\"1023306104\"SELF-EMPLOYED\"834757599‚Ä¶‚Ä¶\"FAHR, LLC\"166679844\"CANDIDATE\"75187243\n\n\n\n\n\ntotal_by_employer_dd\n\nEMPLOYER\nRETIRED          1023306104\nSELF-EMPLOYED     834757599\n                    ...    \nFAHR, LLC         166679844\nCANDIDATE          75187243\nName: TRANSACTION_AMT, Length: 10, dtype: int32\n\n\n\n\n\n\n\n6.3.3 avg_by_occupation\n\nPolarsDask\n\n\n\navg_by_occupation_pl\n\n\n\nshape: (10, 2)OCCUPATIONTRANSACTION_AMTcatf64\"CHAIRMAN CEO &‚Ä¶1.0233e6\"PAULSON AND CO‚Ä¶1e6‚Ä¶‚Ä¶\"CHIEF EXECUTIV‚Ä¶500000.0\"MOORE CAPITAL ‚Ä¶500000.0\n\n\n\n\n\navg_by_occupation_dd\n\nOCCUPATION\nCHAIRMAN CEO & FOUNDER              1.023333e+06\nPAULSON AND CO., INC.               1.000000e+06\n                                        ...     \nOWNER, FOUNDER AND CEO              5.000000e+05\nCHIEF EXECUTIVE OFFICER/PRODUCER    5.000000e+05\nName: TRANSACTION_AMT, Length: 10, dtype: float64"
  },
  {
    "objectID": "scaling.html#filtering",
    "href": "scaling.html#filtering",
    "title": "6¬† Scaling",
    "section": "6.4 Filtering",
    "text": "6.4 Filtering\nLet‚Äôs filter for only the 10 most common occupations and compute some summary statistics:\n\n6.4.1 avg_by_occupation, filtered\nGetting the most common occupations:\n\nPolarsDask\n\n\n\ntop_occupations_pl = (\n    occupation_counts_pl.select(\n        pl.col(\"OCCUPATION\")\n        .struct.field(\"OCCUPATION\")\n        .drop_nulls()\n        .head(10)\n    )\n    .to_series()\n)\ntop_occupations_pl\n\n\n\nshape: (10,)OCCUPATIONcat\"RETIRED\"\"NOT EMPLOYED\"‚Ä¶\"EXECUTIVE\"\"ENGINEER\"\n\n\n\n\n\ntop_occupations_dd = occupation_counts_dd.head(10).index\ntop_occupations_dd\n\nCategoricalIndex(['RETIRED', 'NOT EMPLOYED', 'ATTORNEY', 'PHYSICIAN',\n                  'HOMEMAKER', 'PRESIDENT', 'PROFESSOR', 'CONSULTANT',\n                  'EXECUTIVE', 'ENGINEER'],\n                 categories=['PUBLIC RELATIONS CONSULTANT', 'PRESIDENT', 'PHYSICIAN', 'SENIOR EXECUTIVE', ..., 'VICE PRESIDENT - FUEL PROCUREMENT', 'INFORMATION TECHNOLOGY SPECI', 'SR MANAGER, PROJECT PLANNING', 'PROASSURANCE'], ordered=False, dtype='category', name='OCCUPATION')\n\n\n\n\n\n\nPolarsDask\n\n\n\ndonations_pl_lazy = (\n    indiv_pl.filter(pl.col(\"OCCUPATION\").is_in(top_occupations_pl.to_list()))\n    .group_by(\"OCCUPATION\")\n    .agg(pl.col(\"TRANSACTION_AMT\").mean())\n)\ntotal_avg_pl, occupation_avg_pl = pl.collect_all(\n    [indiv_pl.select(pl.col(\"TRANSACTION_AMT\").mean()), donations_pl_lazy],\n    streaming=True,\n    comm_subplan_elim=False\n)\n\n\n\n\ndonations_dd_lazy = (\n    indiv_dd[indiv_dd[\"OCCUPATION\"].isin(top_occupations_dd)]\n    .groupby(\"OCCUPATION\", observed=True)[\"TRANSACTION_AMT\"]\n    .mean()\n    .dropna()\n)\ntotal_avg_dd, occupation_avg_dd = compute(\n    indiv_dd[\"TRANSACTION_AMT\"].mean(), donations_dd_lazy\n)\n\n\n\n\n\n\n6.4.2 Plotting\nThese results are small enough to plot:\n\nPolarsDask\n\n\n\nax = (\n    occupation_avg_pl\n    .to_pandas()\n    .set_index(\"OCCUPATION\")\n    .squeeze()\n    .sort_values(ascending=False)\n    .plot.barh(color=\"k\", width=0.9)\n)\nlim = ax.get_ylim()\nax.vlines(total_avg_pl, *lim, color=\"C1\", linewidth=3)\nax.legend([\"Average donation\"])\nax.set(xlabel=\"Donation Amount\", title=\"Average Donation by Occupation\")\n\n[Text(0.5, 0, 'Donation Amount'),\n Text(0.5, 1.0, 'Average Donation by Occupation')]\n\n\n\n\n\n\n\n\nax = occupation_avg_dd.sort_values(ascending=False).plot.barh(color=\"k\", width=0.9)\nlim = ax.get_ylim()\nax.vlines(total_avg_dd, *lim, color=\"C1\", linewidth=3)\nax.legend([\"Average donation\"])\nax.set(xlabel=\"Donation Amount\", title=\"Average Donation by Occupation\")\n\n[Text(0.5, 0, 'Donation Amount'),\n Text(0.5, 1.0, 'Average Donation by Occupation')]"
  },
  {
    "objectID": "scaling.html#resampling",
    "href": "scaling.html#resampling",
    "title": "6¬† Scaling",
    "section": "6.5 Resampling",
    "text": "6.5 Resampling\nResampling is another useful way to get our data down to a manageable size:\n\nPolarsDask\n\n\n\ndaily_pl = (\n    indiv_pl.select([\"TRANSACTION_DT\", \"TRANSACTION_AMT\"])\n    .drop_nulls()\n    .sort(\"TRANSACTION_DT\")\n    .group_by_dynamic(\"TRANSACTION_DT\", every=\"1d\")\n    .agg(pl.col(\"TRANSACTION_AMT\").sum())\n    .filter(\n        pl.col(\"TRANSACTION_DT\")\n        .is_between(date(2011, 1, 1), date(2017, 1, 1), closed=\"left\")\n    )\n    .with_columns(pl.col(\"TRANSACTION_AMT\") / 1000)\n    .collect(streaming=True)\n)\nax = (\n    daily_pl.select(\n        [pl.col(\"TRANSACTION_DT\").cast(pl.Datetime), \"TRANSACTION_AMT\"]\n    )\n    .to_pandas()\n    .set_index(\"TRANSACTION_DT\")\n    .squeeze()\n    .plot(figsize=(12, 6))\n)\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\")\n\n[(0.0, 83407.5242),\n Text(0.5, 1.0, 'Daily Donations'),\n Text(0, 0.5, '$ (thousands)')]\n\n\n\n\n\n\n\n\ndaily_dd = (\n    indiv_dd[[\"TRANSACTION_DT\", \"TRANSACTION_AMT\"]]\n    .dropna()\n    .set_index(\"TRANSACTION_DT\")[\"TRANSACTION_AMT\"]\n    .resample(\"D\")\n    .sum()\n    .loc[\"2011\":\"2016\"]\n    .div(1000)\n    .compute()\n)\n\nax = daily_dd.plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\")\n\n/home/user/mambaforge/envs/modern-polars/lib/python3.11/site-packages/partd/pandas.py:138: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n  elif is_datetime64tz_dtype(block):\n/home/user/mambaforge/envs/modern-polars/lib/python3.11/site-packages/partd/pandas.py:138: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n  elif is_datetime64tz_dtype(block):\n\n\n/home/user/mambaforge/envs/modern-polars/lib/python3.11/site-packages/partd/pandas.py:138: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n  elif is_datetime64tz_dtype(block):\n\n\n[(0.0, 83407.5242),\n Text(0.5, 1.0, 'Daily Donations'),\n Text(0, 0.5, '$ (thousands)')]"
  },
  {
    "objectID": "scaling.html#joining",
    "href": "scaling.html#joining",
    "title": "6¬† Scaling",
    "section": "6.6 Joining",
    "text": "6.6 Joining\nPolars joins work in streaming mode. Let‚Äôs add join the donations data with the committee master data, which contains information about the committees people donate to.\n\nPolarsDask\n\n\n\ncm_pl = (\n    # This data is small so we don't use streaming.\n    # Also, .last isn't available in lazy mode.\n    pl.read_parquet(fec_dir / \"cm*.pq\")\n    # Some committees change their name, but the ID stays the same\n    .group_by(\"CMTE_ID\", maintain_order=True).last()\n)\ncm_pl\n\n\n\nshape: (28_467, 3)CMTE_IDCMTE_NMCMTE_PTY_AFFILIATIONstrstrcat\"C00000042\"\"ILLINOIS TOOL ‚Ä¶null\"C00000059\"\"HALLMARK CARDS‚Ä¶\"UNK\"‚Ä¶‚Ä¶‚Ä¶\"C90017336\"\"LUDWIG, EUGENE‚Ä¶null\"C90017542\"\"CENTER FOR POP‚Ä¶null\n\n\n\n\n\ncm_dd = (\n    # This data is small but we use dask here as a \n    # convenient way to read a glob of files.\n    dd.read_parquet(fec_dir / \"cm*.pq\")\n    .compute()\n    # Some committees change their name, but the\n    # ID stays the same.\n    # If we use .last instead of .nth(-1),\n    # we get the last non-null value\n    .groupby(\"CMTE_ID\", as_index=False)\n    .nth(-1)\n)\ncm_dd\n\n\n\n\n\n  \n    \n      \n      CMTE_ID\n      CMTE_NM\n      CMTE_PTY_AFFILIATION\n    \n  \n  \n    \n      7\n      C00000794\n      LENT  & SCRIVNER PAC\n      UNK\n    \n    \n      15\n      C00001156\n      MICHIGAN LEAGUE OF COMMUNITY BANKS POLITICAL A...\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      17649\n      C99002396\n      AMERICAN POLITICAL ACTION COMMITTEE\n      NaN\n    \n    \n      17650\n      C99003428\n      THIRD DISTRICT REPUBLICAN PARTY\n      REP\n    \n  \n\n28467 rows √ó 3 columns\n\n\n\n\n\n\nMerging:\n\nPolarsDask\n\n\n\nindiv_filtered_pl = indiv_pl.filter(\n    pl.col(\"TRANSACTION_DT\").is_between(\n        date(2007, 1, 1), date(2017, 1, 1), closed=\"both\"\n    )\n)\nmerged_pl = indiv_filtered_pl.join(cm_pl.lazy(), on=\"CMTE_ID\")\n\n\n\n\nindiv_filtered_dd = indiv_dd[\n    (indiv_dd[\"TRANSACTION_DT\"] >= pd.Timestamp(\"2007-01-01\"))\n    & (indiv_dd[\"TRANSACTION_DT\"] <= pd.Timestamp(\"2017-01-01\"))\n]\nmerged_dd = dd.merge(indiv_filtered_dd, cm_dd, on=\"CMTE_ID\")\n\n\n\n\nDaily donations by party:\n\nPolarsDask\n\n\n\nparty_donations_pl = (\n    merged_pl.group_by([\"TRANSACTION_DT\", \"CMTE_PTY_AFFILIATION\"])\n    .agg(pl.col(\"TRANSACTION_AMT\").sum())\n    .sort([\"TRANSACTION_DT\", \"CMTE_PTY_AFFILIATION\"])\n    .collect(streaming=True)\n)\n\n\n\n\nparty_donations_dd = (\n    (\n        merged_dd.groupby([\"TRANSACTION_DT\", \"CMTE_PTY_AFFILIATION\"])[\n            \"TRANSACTION_AMT\"\n        ].sum()\n    )\n    .compute()\n    .sort_index()\n)\n\n\n\n\nPlotting daily donations:\n\nPolarsDask\n\n\n\nax = (\n    party_donations_pl\n    .pivot(\n        index=\"TRANSACTION_DT\", columns=\"CMTE_PTY_AFFILIATION\", values=\"TRANSACTION_AMT\"\n    )[1:, :]\n    .select(\n        [pl.col(\"TRANSACTION_DT\"), pl.col(pl.Int32).rolling_mean(30, min_periods=0)]\n    )\n    .to_pandas()\n    .set_index(\"TRANSACTION_DT\")\n    [[\"DEM\", \"REP\"]]\n    .plot(color=[\"C0\", \"C3\"], figsize=(12, 6), linewidth=3)\n)\nax.set(title=\"Daily Donations (30-D Moving Average)\", xlabel=\"Date\")\n\n[Text(0.5, 1.0, 'Daily Donations (30-D Moving Average)'), Text(0.5, 0, 'Date')]\n\n\n\n\n\n\n\n\nax = (\n    party_donations_dd\n    .unstack(\"CMTE_PTY_AFFILIATION\")\n    .iloc[1:]\n    .rolling(\"30D\")\n    .mean()\n    [[\"DEM\", \"REP\"]]\n    .plot(color=[\"C0\", \"C3\"], figsize=(12, 6), linewidth=3)\n)\nax.set(title=\"Daily Donations (30-D Moving Average)\", xlabel=\"Date\")\n\n[Text(0.5, 1.0, 'Daily Donations (30-D Moving Average)'), Text(0.5, 0, 'Date')]"
  },
  {
    "objectID": "summary.html#reasons-to-use-polars",
    "href": "summary.html#reasons-to-use-polars",
    "title": "Summary",
    "section": "Reasons to use Polars",
    "text": "Reasons to use Polars\n\nIt‚Äôs really fast.\nIt has a nice API.\nIt does most of the things Pandas does. The biggest missing things are plotting and some I/O methods.\nIt‚Äôs available in Python, Rust, NodeJS and Ruby. This is partly because most of the code is written in Rust, and calling Rust in other languages works much better than calling Python.\nThe lead dev is very productive and quick to fix bugs."
  },
  {
    "objectID": "summary.html#reasons-not-to-use-polars",
    "href": "summary.html#reasons-not-to-use-polars",
    "title": "Summary",
    "section": "Reasons not to use Polars",
    "text": "Reasons not to use Polars\n\nBugs (maybe)\nBy my count I ran into 11 bugs while writing this book. They were fixed quickly and tests were added to make sure they don‚Äôt happen again, but it made me feel somewhat uneasy.\nHowever that was some time ago, and Pandas also has plenty of bugs, so at this point it‚Äôs quite difficult to measure which library is buggier.\n\n\nIf it ain‚Äôt broke, don‚Äôt fix it\nSuppose you have a bunch of important stuff in another library that works fine, even if it‚Äôs a bit slow. Maybe it doesn‚Äôt have good tests. Switching library is tricky and may not be worth it here. Even if Polars is bug-free, various default behaviours might differ from your expectations.\n\n\nAPI stability\nSince Polars is young and has not made a 1.0 release yet, the API is subject to breaking changes. Indeed, it has quite a few features that are explicitly labelled as experimental."
  },
  {
    "objectID": "summary.html#other-cool-stuff-you-might-like",
    "href": "summary.html#other-cool-stuff-you-might-like",
    "title": "Summary",
    "section": "Other cool stuff you might like",
    "text": "Other cool stuff you might like\n\nr-polars, a work-in-progress project bringing Polars to R.\ntidypolars: an API for py-polars that should be familiar to R Tidyverse users.\nDuckDB: not a dataframe library, but can do a lot of what Polars does and is often mentioned in the same breath."
  }
]